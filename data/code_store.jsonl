{"file_name": "main.nf", "file_path": "/main.nf", "language": "nextflow", "id": "main", "content": "nextflow.enable.dsl=2\n\ninclude { pipeline_ngsmanager } from './pipelines/pipeline_ngsmanager'\ninclude { logHeader } from './functions/common.nf'\n\nlog.info logHeader('NGSMANAGER')\n\ndef helpMessage() {\n    log.info\"\"\"\n    Usage:\n\n    # single-sample\n\n    nextflow run http://gtc-devsrv:3000/bioinfo/ngsmanager --cmp 2021.TE.8600.1.96 --riscd 210108-11234826-2AS_mapping-bowtie\n\n    Mandatory arguments\n      --cmp                         sample id\n      --riscd                       analysis result to be used as input\n      --sample_type                 one of: 'bacterium', 'virus', 'sarscov2'\n\n    # multi-sample\n\n    nextflow run http://gtc-devsrv:3000/bioinfo/ngsmanager --samplesheet samples.csv \n    \n    Mandatory arguments      \n      --samplesheet                 samplesheet containing samples metadata\n\n    \"\"\".stripIndent()\n}\n\nworkflow {\n    if (!params.containsKey('cmp') && !params.containsKey('samplesheet')){\n        helpMessage()\n        exit 1\n    }\n    pipeline_ngsmanager()\n}"}
{"file_name": "multi_clustering__reportree_vcf.nf", "file_path": "/multi/multi_clustering__reportree_vcf.nf", "language": "nextflow", "id": "multi_clustering__reportree_vcf", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\ninclude { _getAlleles;param;optional;optionalOrDefault;getVCFs } from '../functions/parameters.nf'\n\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')  \ndef SUMMARY_COLUMNS = param('multi_clustering__reportree__summary_columns')  \ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')  \ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')  \n\nprocess prepare_metadata {\n    container \"ubuntu:20.04\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(metadata)\n    output:\n      path '*'\n      path 'reportree_metadata.tsv', emit: metadata\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'reportree_metadata.tsv' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"prepare_metadata.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"prepare_metadata.cfg\" }\n    script:\n      \"\"\"\n         sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' ${metadata} > reportree_metadata.tsv\n      \"\"\"\n}\n\nprocess reportree_gt {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(vcfs)\n      path(metadata)\n      path(nomenclature_path), stageAs: 'prev_nomenclature.tsv'\n    output:\n      path '*'\n      path 'gt_dist_hamming.tsv', emit: matrix\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'gt_zooms.txt', saveAs: { \"zooms.txt\" }\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*_*[0-9]'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_gt.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_gt.cfg\" }\n    script:\n      thr = param('multi_clustering__reportree__thr')      \n      soi = optional('multi_clustering__reportree__sample_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      soi_par = soi ? \" --sample_of_interest ${soi}\" : ''\n      zoom = optional('multi_clustering__reportree__zoom_cluster_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_par = zoom ? \" --zoom-cluster-of-interest  ${zoom}\" : ''\n      zoom_subtree = optional('multi_clustering__reportree__subtree-of-interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_subtree_par = zoom_subtree ? \" --subtree-of-interest  ${zoom_subtree}\" : ''\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      extra = optional('multi_clustering__reportree__extra')\n      nomenclature = !nomenclature_path.empty() ? \"--nomenclature-file prev_nomenclature.tsv\" : \"\"\n      \"\"\"\n      #!/bin/bash -euo pipefail\n\n      for f in ${vcfs}; do \n        cmp=`echo -n \\$f | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([[:digit:]]+\\\\.[[:alnum:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+).+/\\\\1/'`        \n        ln -s \\$f \\$cmp\n        echo \"\\$cmp\" >> vcf_files.txt\n      done \n     \n      reportree.py \\\n        -m ${metadata} \\\n        -vcf vcf_files.txt \\\n        -out 'gt' \\\n        --analysis grapetree \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --matrix-4-grapetree \\\n        --mx-transpose \\\n        --n_proc ${task.cpus} \\\n        --thr ${thr}  \\\n        --loci-called ${lociCalled} \\\n        --unzip \\\n        --site-inclusion ${siteInclusion} ${soi_par} ${zoom_par} ${zoom_subtree_par} ${extra} ${nomenclature} \n      touch gt_zooms.txt        \n      \"\"\"      \n}\n\nprocess reportree_hc {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(vcfs)\n      path(metadata)\n    output:\n      path '*'\n      path 'hc_*.nwk', emit: nwk_hc\n      path 'hc_metadata_w_partitions.tsv', emit: metadata_hc\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{hc_*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_hc.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_hc.cfg\" }\n    script:\n      hcMethod = param('multi_clustering__reportree__HC_threshold')\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      \"\"\"\n      for f in ${vcfs}; do \n        cmp=`echo -n \\$f | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`\n        ln -s \\$f \\$cmp\n        echo \"\\$cmp\" >> vcf_files.txt\n      done \n\n      reportree.py \\\n        -m ${metadata} \\\n        -vcf vcf_files.txt \\\n        -out hc \\\n        --analysis HC \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --mx-transpose \\\n        --loci-called ${lociCalled} \\\n        --site-inclusion ${siteInclusion} \\\n        --HC-threshold ${hcMethod}\n      \"\"\"\n}\n\nprocess find_closest {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/multi_clustering__reportree:/scripts:ro\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'sample_of_interest_summary.txt'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"find_closest.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"find_closest.cfg\" }\n    script:\n      samples =  param('multi_clustering__reportree__sample_of_interest').split(',').collect { it.trim() }.join(',')\n      threshold = param('multi_clustering__reportree__report_threshold')\n      \"\"\"\n        /scripts/filter-matrix-distance.py ${matrix} ${threshold} ${samples} sample_of_interest_summary.txt\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' | sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\n\nworkflow multi_clustering__reportree {\n    take: \n        input\n        raw_metadata\n        geodata\n        nomenclature\n    main:\n        metadata = prepare_metadata(raw_metadata).metadata\n        vcfs = input.flatMap { it[1] }.collect()\n        matrix = reportree_gt(vcfs, metadata, nomenclature).matrix\n\n        if (optional('multi_clustering__reportree__sample_of_interest') && optional('multi_clustering__reportree__report_threshold')) {\n            find_closest(matrix)\n        }\n        if (optional('multi_clustering__reportree__HC_threshold')) {\n          reportree_hc(vcfs, metadata)\n          augur(reportree_hc.out.nwk_hc, reportree_hc.out.metadata_hc, geodata)\n        }\n}\n\nworkflow {\n  multi_clustering__reportree(getVCFs(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}"}
{"file_name": "multi_clustering__cfsan.nf", "file_path": "/multi/multi_clustering__cfsan.nf", "language": "nextflow", "id": "multi_clustering__cfsan", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory;flattenPath } from '../functions/common.nf'\ninclude { getReferenceUnkeyed;getResult;getInput;param } from '../functions/parameters.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nIMAGES = [\n  '2.2.1': 'staphb/cfsan-snp-pipeline:2.2.1',\n  '2.0.2': 'cfsanbiostatistics/snp-pipeline@sha256:448787923371ade95217982814db25efb1e01287a8180b523d76a9f093f97d01'\n]\n\ndef DOCKER_IMAGE = IMAGES[param('multi_clustering__cfsan__version')] ?: (exit 2, \"params (multi_clustering__cfsan__version) not valid\");\n\nprocess cfsan_snp_pipeline {\n    container DOCKER_IMAGE\n    containerOptions = \"-v ${workflow.projectDir}/scripts/multi_clustering__cfsan:/scripts:ro\"\n    input:\n      path(samples)\n      tuple val(_), val(reference), path(refPath)\n    output:\n      path '**'\n      path 'results/snpma.fasta', emit: snpma\n      path '{*.sh,*.log}', hidden: true\n    stageInMode 'symlink'  \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*.tsv', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*.vcf', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'results/*.fasta', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cfsan_snp_pipeline.cfg\" }\n    script:\n      version = param('multi_clustering__cfsan__version')\n      \"\"\"\n        trap \"find  \\\\( -name '*.bam' -o -name '*.sam' -o -name '*.pileup' \\\\) -delete\" EXIT\n        for FILE in ${samples}; do CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`; mkdir -p samples/\\${CMP} && mv \\$FILE samples/\\${CMP}/ ; done\n        cfsan_snp_pipeline run -c /scripts/snppipeline_${version}.conf -m soft -o results --samples_dir samples ${refPath} >> cfsan_snp_pipeline.log\n      \"\"\"\n}\n\nprocess iqtree {\n    container \"quay.io/biocontainers/iqtree:1.6.12--he513fc3_0\"\n    stageInMode 'copy'\n    input:\n      path(snpma)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n      path(\"snpma.fasta.treefile\"), emit: nwk\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'snpma.fasta.treefile', saveAs: { \"snpma.fasta.nwk\" } \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"iqtree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"iqtree.cfg\" }\n    script:\n      \"\"\"\n        iqtree -s ${snpma} -nt AUTO\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__cfsan {\n    take: \n        input\n        reference\n        metadata\n        geodata\n    main:\n        snpma = cfsan_snp_pipeline(input, reference).snpma\n        nwk = iqtree(snpma).nwk\n        augur(nwk, metadata, geodata)\n}\n\nworkflow {\n    reads = getInput()\n        .map { it[1] }\n        .toSortedList( { a, b -> a[0] <=> b[0] } )\n        .flatten()     \n        .collect()  \n    multi_clustering__cfsan(reads, getReferenceUnkeyed('fa'), param('metadata'), param('geodata'));\n}"}
{"file_name": "multi_clustering__reportree_alignment.nf", "file_path": "/multi/multi_clustering__reportree_alignment.nf", "language": "nextflow", "id": "multi_clustering__reportree_alignment", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\ninclude { _getAlleles;param;optional;optionalOrDefault;getVCFs } from '../functions/parameters.nf'\n\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')  \ndef SUMMARY_COLUMNS = param('multi_clustering__reportree__summary_columns')  \ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')  \ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')  \n\nprocess prepare_metadata {\n    container \"ubuntu:20.04\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(metadata)\n    output:\n      path '*'\n      path 'reportree_metadata.tsv', emit: metadata\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'reportree_metadata.tsv' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"prepare_metadata.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"prepare_metadata.cfg\" }\n    script:\n      \"\"\"\n         sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' ${metadata} > reportree_metadata.tsv\n      \"\"\"\n}\n\nprocess maaft {\n    container \"quay.io/biocontainers/mafft:7.520--h031d066_3\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(sequences)\n    output:\n      path '*'\n      path 'alignment.fasta', emit: alignment\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.fasta}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"maaft.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"maaft.cfg\" }\n    script:     \n      \"\"\"\n        for f in ${sequences}; do \n          cmp=`echo -n \\$f | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`\n          awk \"NR==1 {\\\\\\$0=\\\\\">\\$cmp\\\\\"}1\" \\$f >> input.fasta\n        done   \n        mafft \\\n        --thread ${task.cpus} \\\n        --auto \\\n        input.fasta > alignment.fasta\n      \"\"\"\n}\n\nprocess reportree_gt {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(alignment)\n      path(metadata)\n      path(nomenclature_path), stageAs: 'prev_nomenclature.tsv'\n    output:\n      path '*'\n      path 'gt_dist_hamming.tsv', emit: matrix\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'gt_zooms.txt', saveAs: { \"zooms.txt\" }\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*_*[0-9]'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_gt.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_gt.cfg\" }\n    script:\n      thr = param('multi_clustering__reportree__thr')      \n      soi = optional('multi_clustering__reportree__sample_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      soi_par = soi ? \" --sample_of_interest ${soi}\" : ''\n      zoom = optional('multi_clustering__reportree__zoom_cluster_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_par = zoom ? \" --zoom-cluster-of-interest  ${zoom}\" : ''\n      zoom_subtree = optional('multi_clustering__reportree__subtree-of-interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_subtree_par = zoom_subtree ? \" --subtree-of-interest  ${zoom_subtree}\" : ''\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      extra = optional('multi_clustering__reportree__extra')\n      nomenclature = !nomenclature_path.empty() ? \"--nomenclature-file prev_nomenclature.tsv\" : \"\"\n      \"\"\"\n      reportree.py \\\n        -m ${metadata} \\\n        -align ${alignment} \\\n        -out 'gt' \\\n        --analysis grapetree \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --matrix-4-grapetree \\\n        --mx-transpose \\\n        --n_proc ${task.cpus} \\\n        --thr ${thr}  \\\n        --loci-called ${lociCalled} \\\n        --unzip \\\n        --site-inclusion ${siteInclusion} ${soi_par} ${zoom_par} ${zoom_subtree_par} ${extra} ${nomenclature} \n      touch gt_zooms.txt\n      \"\"\"\n}\n\nprocess reportree_hc {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(alignment)\n      path(metadata)\n    output:\n      path '*'\n      path 'hc_*.nwk', emit: nwk_hc\n      path 'hc_metadata_w_partitions.tsv', emit: metadata_hc\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{hc_*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_hc.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_hc.cfg\" }\n    script:\n      hcMethod = param('multi_clustering__reportree__HC_threshold')\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      \"\"\"\n        reportree.py \\\n        -m ${metadata} \\\n        -align ${alignment} \\\n        -out hc \\\n        --analysis HC \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --mx-transpose \\\n        --loci-called ${lociCalled} \\\n        --site-inclusion ${siteInclusion} \\\n        --HC-threshold ${hcMethod}\n      \"\"\"\n}\n\nprocess find_closest {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/multi_clustering__reportree:/scripts:ro\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'sample_of_interest_summary.txt'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"find_closest.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"find_closest.cfg\" }\n    script:\n      samples =  param('multi_clustering__reportree__sample_of_interest').split(',').collect { it.trim() }.join(',')\n      threshold = param('multi_clustering__reportree__report_threshold')\n      \"\"\"\n        /scripts/filter-matrix-distance.py ${matrix} ${threshold} ${samples} sample_of_interest_summary.txt\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\n\nworkflow multi_clustering__reportree {\n    take: \n        input\n        raw_metadata\n        geodata\n        nomenclature\n    main:\n        metadata = prepare_metadata(raw_metadata).metadata\n        fastas = input.flatMap { it[1] }.collect()  \n        alignment = maaft(fastas).alignment\n        matrix = reportree_gt(alignment, metadata, nomenclature).matrix\n\n        if (optional('multi_clustering__reportree__sample_of_interest') && optional('multi_clustering__reportree__report_threshold')) {\n            find_closest(matrix)\n        }\n        if (optional('multi_clustering__reportree__HC_threshold')) {\n          reportree_hc(alignment, metadata)\n          augur(reportree_hc.out.nwk_hc, reportree_hc.out.metadata_hc, geodata)\n        }\n}\n\nworkflow {\n  multi_clustering__reportree(getInput(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}"}
{"file_name": "multi_clustering__augur.nf", "file_path": "/multi/multi_clustering__augur.nf", "language": "nextflow", "id": "multi_clustering__augur", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory } from '../functions/common.nf'\ninclude { getInput;getReferenceUnkeyed;param;optional } from '../functions/parameters.nf'\n\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')  \ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')  \ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')  \ndef TRAITS_COLUMNS = param('multi_clustering__augur__traits_columns') \n\nprocess align {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(sequences)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'alignment.fasta', emit: alignment\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'alignment.fasta' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"align.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"align.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__align_extra')  \n      \"\"\"\n        for f in ${sequences}; do \n          cmp=`echo -n \\$f | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`\n          awk \"NR==1 {\\\\\\$0=\\\\\">\\$cmp\\\\\"}1\" \\$f > seq_\\$f\n        done     \n        augur align \\\n        --nthreads auto \\\n        --sequences seq_* \\\n        --reference-sequence ${ref_path} \\\n        --remove-reference \\\n        --output alignment.fasta \\\n        --fill-gaps ${extra}  \n      \"\"\"\n}\n\nprocess tree {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    input:\n      path(alignment)\n    output:\n      path '*'\n      path 'tree_raw.nwk', emit: tree_raw\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'tree_raw.nwk' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"tree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"tree.cfg\" }\n    script:\n      method = param('multi_clustering__augur__tree_method')  \n      extra = optional('multi_clustering__augur__tree_extra')  \n      \"\"\"\n        augur tree \\\n          --alignment ${alignment} \\\n          --output tree_raw.nwk \\\n          --method ${method} \\\n          --nthreads auto ${extra}\n      \"\"\"\n}\n\nprocess refine {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(alignment)\n      path(metadata)\n    output:\n      path '*'\n      path 'tree.nwk', emit: tree\n      path 'branch_lengths.json', emit: branch_lengths\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'tree.nwk' \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'branch_lengths.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"refine.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"refine.cfg\" }\n    script:\n      coalescent = param('multi_clustering__augur__refine_coalescent')  \n      extra = optional('multi_clustering__augur__refine_extra')  \n      \"\"\"\n         augur refine \\\n            --tree ${tree} \\\n            --alignment ${alignment} \\\n            --metadata ${metadata} \\\n            --output-tree tree.nwk \\\n            --output-node-data branch_lengths.json \\\n            --timetree \\\n            --coalescent ${coalescent} \\\n            --date-confidence ${extra}\n      \"\"\"\n}\n\nprocess ancestral {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(alignment)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'nt_muts.json', emit: nt_muts\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'nt_muts.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"ancestral.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"ancestral.cfg\" }\n    script:\n      inference = param('multi_clustering__augur__ancestral_inference')  \n      extra = optional('multi_clustering__augur__ancestral_extra')      \n      \"\"\"\n        augur ancestral \\\n            --tree ${tree} \\\n            --alignment ${alignment} \\\n            --root-sequence ${ref_path} \\\n            --output-node-data nt_muts.json \\\n            --inference ${inference} ${extra}\n      \"\"\"\n}\n\nprocess translate {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(nt_muts)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'aa_muts.json', emit: aa_muts\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'aa_muts.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"translate.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"translate.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__translate_extra')      \n      \"\"\"\n        augur translate \\\n            --tree ${tree} \\\n            --ancestral-sequences ${nt_muts} \\\n            --reference-sequence ${ref_path} \\\n            --output aa_muts.json ${extra}\n      \"\"\"\n}\n\nprocess traits {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(metadata)\n    output:\n      path '*'\n      path 'traits.json', emit: traits\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'traits.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"traits.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"traits.cfg\" }\n    script:\n      columns = TRAITS_COLUMNS   \n      extra = optional('multi_clustering__augur__traits_extra')      \n      \"\"\"\n        augur traits \\\n            --tree ${tree} \\\n            --metadata ${metadata} \\\n            --output-node-data traits.json \\\n            --columns ${columns} \\\n            --confidence ${extra}\n      \"\"\"\n}\n\nprocess prepare_metadata {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(metadata)\n    output:\n      path '*'\n      path 'augur_metadata.tsv', emit: metadata\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'augur_metadata.tsv' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"prepare_metadata.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"prepare_metadata.cfg\" }\n    script:\n      \"\"\"\n         cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' | sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv \n      \"\"\"\n}\n\nprocess export {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(metadata)\n      path(branch_lengths)\n      path(traits)\n      path(nt_muts)\n      path(aa_muts)\n      path(lat_longs)\n    output:\n      path '*'\n      path 'auspice.json', emit: auspice\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"export.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"export.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__export_extra')      \n      \"\"\"\n        METADATA_LIST=\\$(head -n 1 ${metadata} | tr \\$'\\t' ' ')\n        augur export v2 \\\n            --tree ${tree} \\\n            --metadata ${metadata} \\\n            --node-data ${branch_lengths} ${traits} ${nt_muts} ${aa_muts} \\\n            --lat-longs ${lat_longs} \\\n            --color-by-metadata \\${METADATA_LIST} \\\n            --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n            --output auspice.json ${extra}\n      \"\"\"\n}\n\nworkflow multi_clustering__augur {\n    take: \n      reference\n      raw_metadata\n      geodata\n      ref2\n    main:  \n      metadata = prepare_metadata(raw_metadata).metadata\n      fastas = getInput().flatMap { it[1] }.collect()\n      alignment = align(fastas, reference).alignment\n      tree_raw = tree(alignment).tree_raw\n      tree = refine(tree_raw, alignment, metadata).tree\n      nt_muts = ancestral(tree, alignment, reference).nt_muts\n      aa_muts = translate(tree, nt_muts, reference).aa_muts\n      traits = traits(tree, metadata).traits\n      export(tree, metadata, refine.out.branch_lengths, traits, nt_muts, aa_muts, geodata)\n  }\n\nworkflow {\n    reference = getReferenceUnkeyed('gb')      \n    multi_clustering__augur(reference, param('metadata'), param('geodata'), reference)\n}\n"}
{"file_name": "multi_clustering__reportree_alleles.nf", "file_path": "/multi/multi_clustering__reportree_alleles.nf", "language": "nextflow", "id": "multi_clustering__reportree_alleles", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\ninclude { _getAlleles;param;optional;optionalOrDefault } from '../functions/parameters.nf'\n\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')  \ndef SUMMARY_COLUMNS = param('multi_clustering__reportree__summary_columns')  \ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')  \ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')  \n\nprocess merge_profiles {\n    container \"ghcr.io/genpat-it/chewbbaca-w-chewie-schemas:2.8.5--16b816c96d\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      path(profiles)\n    output:\n      path '**'\n      path 'cgMLST.tsv', emit: all_profiles\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'cgMLST.tsv', saveAs: { \"result_alleles_all.tsv\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"merge_profiles.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"merge_profiles.cfg\" }\n    script:\n      \"\"\"\n        for file in ${profiles} ; do awk 'FNR==1{print \"\"}1' \\${file} | sed 's/,/\\t/g' | sed -E \"s/^[^SF][^ai]\\\\S+/\\${file}/\" | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'; done | sort -ru  > tmp.tsv\n        chewie ExtractCgMLST --input-file tmp.tsv --output-directory . --threshold 0\n      \"\"\"\n}\n\nprocess prepare_metadata {\n    container \"ubuntu:20.04\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(metadata)\n    output:\n      path '*'\n      path 'reportree_metadata.tsv', emit: metadata\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'reportree_metadata.tsv' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"prepare_metadata.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"prepare_metadata.cfg\" }\n    script:\n      \"\"\"\n         sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' ${metadata} > reportree_metadata.tsv\n      \"\"\"\n}\n\nprocess reportree_gt {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(profiles)\n      path(metadata)\n      path(nomenclature_path), stageAs: 'prev_nomenclature.tsv'\n    output:\n      path '*'\n      path 'gt_dist_hamming.tsv', emit: matrix\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'gt_zooms.txt', saveAs: { \"zooms.txt\" }\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*_*[0-9]'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_gt.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_gt.cfg\" }\n    script:\n      thr = param('multi_clustering__reportree__thr')      \n      soi = optional('multi_clustering__reportree__sample_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      soi_par = soi ? \" --sample_of_interest ${soi}\" : ''\n      zoom = optional('multi_clustering__reportree__zoom_cluster_of_interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_par = zoom ? \" --zoom-cluster-of-interest  ${zoom}\" : ''\n      zoom_subtree = optional('multi_clustering__reportree__subtree-of-interest').replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      zoom_subtree_par = zoom_subtree ? \" --subtree-of-interest  ${zoom_subtree}\" : ''\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      extra = optional('multi_clustering__reportree__extra')\n      nomenclature = !nomenclature_path.empty() ? \"--nomenclature-file prev_nomenclature.tsv\" : \"\"\n      \"\"\"     \n      reportree.py \\\n        -m ${metadata} \\\n        -a ${profiles} \\\n        -out 'gt' \\\n        --analysis grapetree \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --matrix-4-grapetree \\\n        --mx-transpose \\\n        --n_proc ${task.cpus} \\\n        --thr ${thr}  \\\n        --loci-called ${lociCalled} \\\n        --unzip \\\n        --site-inclusion ${siteInclusion} ${soi_par} ${zoom_par} ${zoom_subtree_par} ${extra} ${nomenclature} \n      touch gt_zooms.txt        \n    \"\"\"\n}\n\nprocess reportree_hc {\n    container \"ghcr.io/genpat-it/reportree:2.4.1--088b6651b8\"\n    cpus { [32, params.max_cpus as int].min() }   \n    input:\n      path(profiles)\n      path(metadata)\n    output:\n      path '*'\n      path 'hc_*.nwk', emit: nwk_hc\n      path 'hc_metadata_w_partitions.tsv', emit: metadata_hc\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '{hc_*.tsv,*.txt,*.nwk}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"reportree_hc.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"reportree_hc.cfg\" }\n    script:\n      hcMethod = param('multi_clustering__reportree__HC_threshold')\n      lociCalled = param('multi_clustering__reportree__loci_called')   \n      siteInclusion = param('multi_clustering__reportree__site_inclusion')   \n      \"\"\"\n      reportree.py \\\n        -m ${metadata} \\\n        -a ${profiles} \\\n        -out hc \\\n        --analysis HC \\\n        --columns_summary_report ${SUMMARY_COLUMNS} \\\n        --mx-transpose \\\n        --loci-called ${lociCalled} \\\n        --site-inclusion ${siteInclusion} \\\n        --HC-threshold ${hcMethod}\n      \"\"\"\n}\n\nprocess find_closest {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/multi_clustering__reportree:/scripts:ro\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'sample_of_interest_summary.txt'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"find_closest.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"find_closest.cfg\" }\n    script:\n      samples =  param('multi_clustering__reportree__sample_of_interest').split(',').collect { it.trim() }.join(',')\n      threshold = param('multi_clustering__reportree__report_threshold')\n      \"\"\"\n        /scripts/filter-matrix-distance.py ${matrix} ${threshold} ${samples} sample_of_interest_summary.txt\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' | sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\n\nworkflow multi_clustering__reportree {\n    take: \n        input\n        raw_metadata\n        geodata\n        nomenclature\n    main:\n        metadata = prepare_metadata(raw_metadata).metadata\n        profiles = merge_profiles(input.collect()).all_profiles\n        matrix = reportree_gt(profiles, metadata, nomenclature).matrix    \n\n        if (optional('multi_clustering__reportree__sample_of_interest') && optional('multi_clustering__reportree__report_threshold')) {\n            find_closest(matrix)\n        }\n        if (optional('multi_clustering__reportree__HC_threshold')) {\n          reportree_hc(profiles, metadata)\n          augur(reportree_hc.out.nwk_hc, reportree_hc.out.metadata_hc, geodata)\n        }\n}\n\nworkflow {\n    multi_clustering__reportree(getAlleles(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}\n\ndef getAlleles() {\n    if (!params.containsKey('input')) {\n      exit 2, \"missing required param: input\";\n    }\n    def schema = params.containsKey('schema') ? params.schema : null\n    assert params.input instanceof ArrayList\n    params.input.inject(Channel.empty()) {\n        res, val -> res.mix(_getAlleles(val.cmp, val.riscd, schema))\n    }        \n}"}
{"file_name": "multi_clustering__vcf2mst.nf", "file_path": "/multi/multi_clustering__vcf2mst.nf", "language": "nextflow", "id": "multi_clustering__vcf2mst", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory } from '../functions/common.nf'\ninclude { getVCFs } from '../functions/parameters.nf'\n\nprocess vcf2mst {\n    container \"ghcr.io/genpat-it/vcf2mst:0.0.1--d587d682e9\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(vcf_files)\n    output:\n      path '*'\n      path 'HDmatrix.tsv', emit: matrix\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'tree.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"vcf2mst.cfg\" }\n    script:\n      \"\"\"\n        mkdir vcf_files \n        for FILE in ${vcf_files}; do CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([[:digit:]]+\\\\.[[:alnum:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+).+/\\\\1/'`; cp \\$FILE vcf_files/\\${CMP} ; done\n        find vcf_files -mindepth 1 > vcf_list            \n        vcf2mst.pl vcf_list tree.nwk vcf > vcf2mst.log\n        cp /tmp/hamming_distance_matrix.tsv HDmatrix.tsv\n      \"\"\"\n}\n\nprocess dists {\n    container \"quay.io/biocontainers/cgmlst-dists:0.4.0--hec16e2b_2\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"cgmlst-dists.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cgmlst-dists.cfg\" }\n    script:\n      \"\"\"\n        cgmlst-dists -c ${matrix} > vcf2mst_dists_matrix.csv\n      \"\"\"\n}\n\n\nworkflow multi_clustering__vcf2mst {\n    take: \n        input\n    main:\n        matrix=vcf2mst(input).matrix\n        dists(matrix)\n}\n\nworkflow {\n    getVCFs()\n    .map { it[1] }\n    .collect()\n    .set { inputSet }\n    multi_clustering__vcf2mst(inputSet)\n}\n"}
{"file_name": "multi_pangenome__panaroo.nf", "file_path": "/multi/multi_pangenome__panaroo.nf", "language": "nextflow", "id": "multi_pangenome__panaroo", "content": "nextflow.enable.dsl=2\n\ninclude { getInput;param;optional } from '../functions/parameters.nf'\ninclude { flattenPath } from '../functions/common.nf'\n\nprocess panaroo {\n    container \"quay.io/biocontainers/panaroo:1.3.3--pyhdfd78af_0\"\n    input:\n      path gffs\n    output:\n      path '**'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"panaroo.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"panaroo.log\" }\n    script:\n      \"\"\"          \n         panaroo -i *.gff \\\n            -o results \\\n            --clean-mode ${param('multi_pangenome__panaroo__clean_mode')} \\\n            --remove-invalid-genes \\\n            --threshold ${param('multi_pangenome__panaroo__threshold')} \\\n            --family_threshold ${param('multi_pangenome__panaroo__family_threshold')} \\\n            --len_dif_percent ${param('multi_pangenome__panaroo__len_dif_percent')} \\\n            -t ${param('multi_pangenome__panaroo__threads')} \\\n            --alignment core \\\n            --aligner mafft \\\n            --merge_paralogs  \\\n            ${optional('multi_pangenome__panaroo__extra')}\n      \"\"\"\n}\n\nworkflow multi_pangenome__panaroo {\n    take: \n        input\n    main:      \n        input\n            .map { it[1] }\n            .collect()\n            .set { gffs }   \n        panaroo(gffs)\n}\n\nworkflow {\n    multi_pangenome__panaroo(getInput())\n}"}
{"file_name": "multi_clustering__grapetree.nf", "file_path": "/multi/multi_clustering__grapetree.nf", "language": "nextflow", "id": "multi_clustering__grapetree", "content": "nextflow.enable.dsl=2\n\ninclude { parseRISCD;taskMemory } from '../functions/common.nf'\ninclude { _getAlleles;param } from '../functions/parameters.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nprocess extract_cgMLST {\n    container \"ghcr.io/genpat-it/chewbbaca-w-chewie-schemas:2.8.5--16b816c96d\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(alleles)\n    output:\n      path '**'\n      path 'cgMLST.tsv', emit: cgMLST\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'mdata_stats.tsv', saveAs: { \"missing_loci.tsv\" }\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'cgMLST.tsv', saveAs: { \"cgMLST.tsv\" }\n    publishDir mode: 'copy', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"extract_cgMLST.cfg\" }\n    script:\n      \"\"\"\n        for file in ${alleles} ; do awk 'FNR==1{print \"\"}1' \\${file} | sed 's/,/\\t/g' | sed -E \"s/^[^SF][^ai]\\\\S+/\\${file}/\" | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'; done | sort -ru  > results_alleles_all.tsv\n        chewie ExtractCgMLST -i results_alleles_all.tsv -o . > extract_cgMLST.log\n      \"\"\"\n}\n\nprocess dists {\n    container \"quay.io/biocontainers/cgmlst-dists:0.4.0--hec16e2b_2\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(cgMLST)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"cgmlst-dists.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cgmlst-dists.cfg\" }\n    script:\n      \"\"\"\n        cgmlst-dists -c ${cgMLST} > cgMLST_dists_matrix.csv\n      \"\"\"\n}\n\nprocess grapetree {\n    container \"quay.io/biocontainers/grapetree:2.1--pyh3252c3a_0\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(cgMLST)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n      path(\"cgMLST_NJ.nwk\"), emit: nwk_nj\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"grapetree.cfg\" }\n    script:\n      \"\"\"\n        grapetree -p ${cgMLST} > cgMLST.nwk\n        grapetree --method RapidNJ -p ${cgMLST} > cgMLST_NJ.nwk\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__grapetree {\n    take: \n        input\n        metadata\n        geodata\n    main:\n        cgMLST = extract_cgMLST(input.collect()).cgMLST\n        dists(cgMLST)\n        nwk_nj = grapetree(cgMLST).nwk_nj\n        augur(nwk_nj, metadata, geodata)\n}\n\nworkflow {\n    multi_clustering__grapetree(getInput(), param('metadata'), param('geodata'));\n}\n\ndef getInput() {\n    if (!params.containsKey('input')) {\n      exit 2, \"missing required param: input\";\n    }\n    def schema = params.containsKey('schema') ? params.schema : null\n    assert params.input instanceof ArrayList\n    params.input.inject(Channel.empty()) {\n        res, val -> res.mix(_getAlleles(val.cmp, val.riscd, schema))\n    }        \n}\n\n"}
{"file_name": "multi_clustering__reportree.nf", "file_path": "/multi/multi_clustering__reportree.nf", "language": "nextflow", "id": "multi_clustering__reportree", "content": "nextflow.enable.dsl=2\n\ninclude { getVCFs;param;optionalOrDefault } from '../functions/parameters.nf'\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\n\nif (getReportreeInputType() == 'alleles') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alleles\"\n  include { getAlleles as inputFn } from \"../multi/multi_clustering__reportree_alleles\"\n} else if (getReportreeInputType() == 'alignment') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alignment\"\n  include { getInput as inputFn } from '../functions/parameters.nf'\n} else {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_vcf\"\n  include { getVCFs as inputFn } from '../functions/parameters.nf'\n}\n\ndef getReportreeInputType() {\n    def res = param('multi_clustering__reportree__input')\n    if (!(res in ['alleles', 'vcf', 'alignment'])) {\n        exit 2, \"params (multi_clustering__reportree__input) not valid\"    \n    } \n    return res\n}\n\nworkflow {    \n    multi_clustering__reportree(inputFn(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}"}
{"file_name": "multi_alignment__snippycore.nf", "file_path": "/multi/multi_alignment__snippycore.nf", "language": "nextflow", "id": "multi_alignment__snippycore", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;getReferenceUnkeyed;getInputFolders;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent  } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef ENTRYPOINT = \"multi_alignment__snippycore\"\n\nprocess snippy {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path 'snippy', emit: results\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base_ref}.log\" }    \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_snippy_${reference}\"\n      is_fasta = r1.getName() ==~ /.+\\.fa(sta)?$/\n      if (is_fasta) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --ctgs ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"     \n      } else if (isIlluminaPaired(reads)) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --R1 ${r1} --R2 ${r2} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --se ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"      \n      }      \n}\n\n\nprocess snippy_core {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path vcf_files, stageAs: 'data?'\n      tuple val(ref_riscd), val(ref_code), path(ref_file)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{core*}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"snippy_core.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log', saveAs: { \"snippy_core.log\" }\n    script:\n      // XXX renaming folders getting sample name from the first vcf file inside\n      \"\"\"          \n        #!/bin/bash -euo pipefail\n        mkdir inputs && cd inputs && for dir in ${vcf_files}; do ln -s ../\\${dir} `ls ../\\${dir}/*.vcf | head -n 1 | sed -E 's/.+DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`; done && cd ..\n        snippy-core --ref ${ref_file} --prefix core --inprefix snps inputs/*\n      \"\"\"\n}\n\nworkflow multi_alignment__snippycore {\n    take: \n        reads      \n        reference\n    main:\n        reads.combine(reference)\n                .multiMap { \n                    reads: it[0..1] // riscd, R[]\n                    reference:  it[2..4] // riscd, code, path\n                }.set { input }\n        folders = snippy(input.reads,input.reference).results\n        snippy_core(folders.collect(), reference)\n}\n\nworkflow multi_alignment__snippycore_vcf {\n    take: \n        input\n        reference\n    main:\n        input\n            .map { it[1] }\n            .collect()\n            .set { vcfs }       \n        snippy_core(vcfs, reference)\n}\n\nworkflow {\n    // 1PP_* => snippy + snippycore\n    multi_alignment__snippycore(getInput().filter( ~/^.*\\/1PP_.*/ ), getReferenceUnkeyed('gb'))\n    // 2AS_* => snippycore only\n    multi_alignment__snippycore_vcf(getInputFolders().filter( ~/^.*\\/2AS_.*/ ), getReferenceUnkeyed('gb'))\n}"}
{"file_name": "multi_clustering__ksnp3.nf", "file_path": "/multi/multi_clustering__ksnp3.nf", "language": "nextflow", "id": "multi_clustering__ksnp3", "content": "nextflow.enable.dsl=2\n\ninclude { getInput;param } from '../functions/parameters.nf'\ninclude { taskMemory;flattenPath } from '../functions/common.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nprocess ksnp3 {\n    container \"ghcr.io/genpat-it/ksnp3:3.0--addd2c2d0e\"\n    input:\n      path(assembly)\n      val(kmers_size)\n      val(analysis_type)\n    output:\n      path(\"results/${outfile}\"), emit: fasta\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}\", pattern: \"*results/${outfile}\", saveAs: { \"matrix.fasta\" }   \n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*results/*.fasta', saveAs: { filename -> flattenPath(filename) }   \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"ksnp3.cfg\" }\n    script:\n      extra_params = (analysis_type == 'core' ? '-core' : '')\n      outfile = (analysis_type == 'core' ? 'core_SNPs_matrix.fasta' : 'SNPs_all_matrix.fasta')\n      \"\"\"\n        trap \"rm -Rf results/TemporaryFilesToDelete\" EXIT\n        for FILE in ${assembly}; do \\\n          CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/' | sed 's/\\\\./-/g'`; \\\n          ln -s \\$FILE \\${CMP}.fasta ; \\\n          echo -e \"`pwd`/\\${CMP}.fasta\\t\\$CMP\" >> input.tsv ; \\\n        done        \n        kSNP3 -in input.tsv -k ${kmers_size} -NJ ${extra_params} -outdir results > ksnp3.log\n      \"\"\"\n}\n\nprocess iqtree {\n    container \"quay.io/biocontainers/iqtree:1.6.12--he513fc3_0\"\n    input:\n      path(fasta)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n      path(\"*.treefile\"), emit: nwk\n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*.treefile', saveAs: { \"matrix.nwk\" } \n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*.treefile', saveAs: { filename -> filename.replace(\".treefile\", \".nwk\") } \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"iqtree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"iqtree.cfg\" }\n    script:\n      \"\"\"\n        iqtree -nt AUTO -s ${fasta}      \n        sed -i 's/-/./g' *.treefile\n      \"\"\"\n}\n\nprocess grapetree {\n    container \"quay.io/biocontainers/grapetree:2.1--pyh3252c3a_0\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"grapetree.cfg\" }\n    script:\n      \"\"\"\n        grapetree -p ${matrix} > matrix.nwk\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__ksnp3 {\n    take: \n        input\n        kmers_size\n        analysis_type\n        metadata\n        geodata\n    main:\n        input\n          .map { it[1] }\n          .collect()\n          .set { inputSet }\n        matrix = ksnp3(inputSet, kmers_size, analysis_type).fasta\n        iqtree(matrix)\n        grapetree(matrix)\n        augur(iqtree.out.nwk, metadata, geodata)\n}\n\nworkflow {\n    multi_clustering__ksnp3(getInput(), param('kmers_size'), param('analysis_type'), param('metadata'), param('geodata'));\n}\n"}
{"file_name": "filter-matrix-distance.py", "file_path": "/scripts/multi_clustering__reportree/filter-matrix-distance.py", "language": "python", "id": "filter-matrix-distance", "content": "#!/usr/bin/env python3\n\nimport os\nimport sys\n\n\ndef distance_report(matrix_file, thresholds, samples, output_file):\n    if os.path.isfile(matrix_file):\n        with open(output_file, 'w') as res:\n            for threshold in thresholds.split(','):            \n                res.write(\"Distance report, threshold: {}\\n\\n\".format(threshold))\n                for sampleraw in samples.split(','):\n                    sample = sampleraw.strip()\n                    header = ''\n                    with open(matrix_file, 'r') as dists:\n                        for line in dists:\n                            content_written = 0\n                            row = line.strip().split('\\t')\n                            if header == '':\n                                header = line.strip().split('\\t')\n                            elif row[0] == sample:\n                                for col in range(len(header)):\n                                    if col == 0:\n                                        continue\n                                    if header[col] == sample:\n                                        continue\n                                    if int(row[col]) <= int(threshold):\n                                        if content_written == 0:\n                                            res.write(\"Sample: {}\\n\".format(sample))\n                                            content_written = 1\n                                        res.write(\"{}\\t{}\\n\".format(header[col], row[col]))\n                                if content_written == 1:\n                                    res.write(\"---\\n\")\n                res.write(\"====\\n\\n\")\n    else:\n        sys.stderr.write('could not open input files')\n        sys.exit(1)\n    with open(output_file, 'r') as res:\n        print(res.read())\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 5:\n        sys.stderr.write(\n            \"Usage: python %s <cgmlst_dist_result> <threshold> <samples_of_interest_file> <output_file>\" % (\n                sys.argv[0]))\n        sys.exit(1)\n    else:\n        distance_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n"}
{"file_name": "ParseSpeciesFile_newDB.py", "file_path": "/scripts/step_3TX_species__kmerfinder/ParseSpeciesFile_newDB.py", "language": "python", "id": "ParseSpeciesFile_newDB", "content": "#!/usr/bin/env python2\n\nimport os\nimport sys\nimport string\nimport csv\nfrom operator import *\n\n# INFO JSON\ndbBacteria=sys.argv[2]+\"/Bacteria/DB_summary_complete_genome.txt\"\ndbViral=sys.argv[2]+\"/Viral/DB_summary_complete_genome.txt\"\n\n# INPUT\ninfilenameB = sys.argv[1]\nbasename = infilenameB.replace(\"_kmerfinder_bacterial.tsv\", \"\")\noutname = basename+\"_kmerfinder.check\"\ninfilenameV = basename+\"_kmerfinder_viral.tsv\"\n\n# Creo diz per parsare le info dbBacteria\ndizBacteria={}\nfor line in open(dbBacteria,'r'):\n\tsline = line.strip().split(\"\\t\")\n\tkey = sline[0]\n\tdizBacteria.update({key:sline})\n\n# Creo diz per parsare le info dbViral\ndizViral={}\nfor line in open(dbViral,'r'):\n\tsline = line.strip().split(\"\\t\")\n\tkey = sline[0]\n\tdizViral.update({key:sline})\n\nguessFileBacteria = open(infilenameB, 'r')\nguessFileViral = open(infilenameV, 'r')\noutfile = open(outname, 'w')\n\nlinesBacteria = guessFileBacteria.readlines()\nlinesViral = guessFileViral.readlines()\n\n# Info Templato Virus \ntry:\n\tgenomeViral = linesViral[1].split(\"\\t\")[0].strip()\n\tqcovViral = float(linesViral[1].split(\"\\t\")[5].strip())\n\ttcovViral = float(linesViral[1].split(\"\\t\")[6].strip())\n\tDBinfoViral = dizViral.get(genomeViral)\n\tassAccViral=DBinfoViral[0]\n\tguessViral=DBinfoViral[1]\nexcept:\n\tguessViral = \"NoVirus\"\n\tgenomeViral=\"NA\"\n\tqcovViral = \"NA\"\n\ttcovViral = \"NA\"\n\tassAccViral=\"NA\"\n\n# Info Templato bacteria\ntry:\n\tgenomeBacteria = linesBacteria[1].split(\"\\t\")[0].strip()\n\tqcovBacteria = float(linesBacteria[1].split(\"\\t\")[5].strip())\n\ttcovBacteria = float(linesBacteria[1].split(\"\\t\")[6].strip())\n\tDBinfoBacteria = dizBacteria.get(genomeBacteria)\n\tguessBacteria=DBinfoBacteria[2]\n\tassAccBacteria=DBinfoBacteria[0]\n\tdesBacteria=DBinfoBacteria[1]\nexcept:\n\tguessBacteria = \"NoBacteria\"\n\tgenomeBacteria = \"NA\"\n\tqcovBacteria = \"NA\"\n\ttcovBacteria = \"NA\"\n\tassAccBacteria=\"NA\"\n\tdesBacteria= \"NA\"\n\n# Verifico copertura templato Viral\ncheckViral=\"\"\nif guessViral != \"NoVirus\":\t\n\tif qcovViral > 50 and tcovViral > 50:\n\t\tcheckViral=\"PASS\"\n\telif qcovViral > 40 or tcovViral > 40:\n\t\tcheckViral=\"WARNING\"\n\telse:\n\t\tcheckViral=\"ALERT\"\nelse:\n\tcheckViral=\"FAIL\"\n\n# Verifico copertura templato Bacteria\ncheckBacteria=\"\"\nif guessBacteria != \"NoBacteria\":\t\n\tif qcovBacteria > 50 and tcovBacteria > 50:\n\t\tcheckBacteria=\"PASS\"\n\telif qcovBacteria > 40 or tcovBacteria > 40:\n\t\tcheckBacteria=\"WARNING\"\n\telse:\n\t\tcheckBacteria=\"ALERT\"\nelse:\n\tcheckBacteria=\"FAIL\"\n\n#print(guessViral+\"=\"+checkViral+\":\"+guessBacteria+\"=\"+checkBacteria)\n\n# Set species/folder name\nspecies=\"\"\nif (checkBacteria in [\"PASS\",\"WARNING\"]) or (checkViral in [\"PASS\",\"WARNING\"]):\n\tif (checkBacteria in [\"FAIL\",\"ALERT\"]) and (checkViral in [\"PASS\",\"WARNING\"]):\n\t\tspecies=guessViral\n\telif (checkBacteria in [\"PASS\",\"WARNING\"]) and (checkViral in [\"FAIL\",\"ALERT\"]):\n\t\tspecies=guessBacteria\n\telif checkBacteria==\"PASS\" and checkViral==\"WARNING\":\n\t\tspecies=guessBacteria\n\telif checkBacteria==\"WARNING\" and checkViral==\"PASS\":\n\t\tspecies=guessViral\n\telif checkBacteria==checkViral==\"WARNING\" or checkBacteria==checkViral==\"PASS\":\n\t\tspecies=\"Contamination\"\nelse:\n\tspecies=\"NoSpecies\"\n\noutfile.write(\"# filename\\tspeciesAssigned\\tviralGuess\\tcheckViral\\tsampleKmerCovViral\\ttemplateKmerCovViral\\tassembly_accViral\\tbacteriaGuess\\tcheckBacteria\\tsampleKmerCovBacteria\\ttemplateKmerCovBacteria\\tassembly_accBacteria\\tdescBacteria\\n\")\noutfile.write(basename+\"\\t\"+species+\"\\t\"+guessViral+\"\\t\"+checkViral+\"\\t\"+str(qcovViral)+\"\\t\"+str(tcovViral)+\"\\t\"+assAccViral+\"\\t\"+guessBacteria+\"\\t\"+checkBacteria+\"\\t\"+str(qcovBacteria)+\"\\t\"+str(tcovBacteria)+\"\\t\"+assAccBacteria+\"\\t\"+desBacteria+\"\\n\")\noutfile.close()\n\n#print(species)\n\nif os.path.exists(species) == False:\n\tos.mkdir(species)\n\nos.system(\"mv %s* %s\" % (basename, species))\nos.system(\"cp %s/%s .\" % (species, outname))\t\n\n\n\n\n"}
{"file_name": "FindTemplate.py", "file_path": "/scripts/step_3TX_species__kmerfinder/FindTemplate.py", "language": "python", "id": "FindTemplate", "content": "#!/usr/bin/env python2\n\n# Copyright (c) 2014, Ole Lund, Technical University of Denmark\n# All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n##########################################################################\n# IMPORT LIBRARIES\n##########################################################################\n\nimport sys\nimport time\nimport os\nfrom math import sqrt, pow\nfrom optparse import OptionParser\nfrom operator import itemgetter\nimport re\nimport cPickle as pickle\n\n\n##########################################################################\n# FUNCTIONS\n##########################################################################\n\n#--------------------------------------\n# reverse complement sequence:\n#--------------------------------------\ndef reversecomplement(seq):\n    '''Reverse complement'''\n    comp = ''\n    for s in seq:\n        if s == 'A':\n            comp = comp + 'T'\n        elif s == 'T':\n            comp = comp + 'A'\n        elif s == 'C':\n            comp = comp + 'G'\n        elif s == 'G':\n            comp = comp + 'C'\n        else:\n            comp = comp + s\n    return comp[::-1]\n\n#--------------------------------------\n# save kmers of querysequence:\n#--------------------------------------\n\n\ndef save_kmers(queryseq):\n\n    global qtotlen, prefix, queryindex, querymers, uquerymers\n\n    seqlen = len(queryseq)\n    qtotlen += seqlen\n\n    # store kmers in original and reverse complement sequence:\n    for qseq in[queryseq, reversecomplement(queryseq)]:\n        for j in range(0, seqlen - kmersize + 1):\n            submer = qseq[j:j + kmersize]\n            if prefix == qseq[j:j + prefixlen]:\n                if submer in queryindex:\n                    if submer in templates:\n                        queryindex[submer] += 1\n                    querymers += 1\n                else:\n                    if submer in templates:\n                        queryindex[submer] = 1\n                    querymers += 1\n                    uquerymers += 1\n\n#-------------------------------------\n# search for matches:\n#-------------------------------------\n\n\ndef find_matches():\n\n    global queryindex, mincoverage, templates\n\n    templateentries = {}\n    templateentries_tot = {}\n    Nhits = 0\n\n    for submer in queryindex:\n        if queryindex[submer] >= mincoverage:\n            matches = templates[submer].split(\",\")\n\n            # get list of unique matches:\n            umatches = list(set(matches))\n\n            # Nhits = sum of scores over all templates:\n            Nhits = Nhits + len(umatches)\n\n            for match in umatches:\n\n                # get unique scores:\n                if match in templateentries:\n                    templateentries[match] += 1\n                else:\n                    templateentries[match] = 1\n\n                # get total amount of kmers found in template (total score):\n                if match in templateentries_tot:\n                    templateentries_tot[match] += queryindex[submer]\n                else:\n                    templateentries_tot[match] = queryindex[submer]\n\n    return(templateentries, templateentries_tot, Nhits)\n\n#------------------------------------------------\n# Conservative two sided p-value from z-score:\n#------------------------------------------------\n\n\ndef z_from_two_samples(r1, n1, r2, n2):\n    '''Comparison of two fractions, Statistical methods in medical research, Armitage et al. p. 125'''\n    #\n    # r1: positives in sample 1\n    # n1: size of sample 1\n    # r2: positives in sample 2\n    # n2: size of sample 2\n\n    p1 = float(r1) / (float(n1) + etta)\n    p2 = float(r2) / (float(n2) + etta)\n    q1 = 1 - p1\n    q2 = 1 - p2\n    p = (float(r1) + r2) / (n1 + n2 + etta)\n    q = 1 - p\n\n    if q < 0: q = 0.00001 # debuggato cosi\n    #print \"debug\",  p, q, (1 / (n1 + etta) + 1 / (n2 + etta)) + etta\n\n    z = (p1 - p2) / sqrt(p * q * (1 / (n1 + etta) + 1 / (n2 + etta)) + etta)\n\n    return z\n\n#-------------------------------------------------\n# Conservative two sided p-value from z-score:\n#-------------------------------------------------\n\n\ndef fastp(z):\n    '''Conservative two sided p-value from z-score'''\n    if z > 10.7016:\n        p = 1e-26\n    elif z > 10.4862:\n        p = 1e-25\n    elif z > 10.2663:\n        p = 1e-24\n    elif z > 10.0416:\n        p = 1e-23\n    elif z > 9.81197:\n        p = 1e-22\n    elif z > 9.5769:\n        p = 1e-21\n    elif z > 9.33604:\n        p = 1e-20\n    elif z > 9.08895:\n        p = 1e-19\n    elif z > 8.83511:\n        p = 1e-18\n    elif z > 8.57394:\n        p = 1e-17\n    elif z > 8.30479:\n        p = 1e-16\n    elif z > 8.02686:\n        p = 1e-15\n    elif z > 7.73926:\n        p = 1e-14\n    elif z > 7.4409:\n        p = 1e-13\n    elif z > 7.13051:\n        p = 1e-12\n    elif z > 6.8065:\n        p = 1e-11\n    elif z > 6.46695:\n        p = 1e-10\n    elif z > 6.10941:\n        p = 1e-9\n    elif z > 5.73073:\n        p = 1e-8\n    elif z > 5.32672:\n        p = 1e-7\n    elif z > 4.89164:\n        p = 1e-6\n    elif z > 4.41717:\n        p = 1e-5\n    elif z > 3.89059:\n        p = 1e-4\n    elif z > 3.29053:\n        p = 1e-3\n    elif z > 2.57583:\n        p = 0.01\n    elif z > 1.95996:\n        p = 0.05\n    elif z > 1.64485:\n        p = 0.1\n    else:\n        p = 1.0\n    return p\n\n\n##########################################################################\n#\tDEFINE GLOBAL VARIABLES\n##########################################################################\n\nglobal templates, queryindex, prefix, qtotlen, mincoverage, querymers, uquerymers\n\n\n##########################################################################\n# PARSE COMMAND LINE OPTIONS\n##########################################################################\n\nparser = OptionParser()\nparser.add_option(\"-i\", \"--inputfile\", dest=\"inputfilename\",help=\"read from INFILE\", metavar=\"INFILE\")\nparser.add_option(\"-t\", \"--templatefile\", dest=\"templatefilename\",help=\"read from TEMFILE\", metavar=\"TEMFILE\")\nparser.add_option(\"-o\", \"--outputfile\", dest=\"outputfilename\",help=\"write to OUTFILE\", metavar=\"OUTFILE\")\nparser.add_option(\"-k\", \"--kmersize\", dest=\"kmersize\",help=\"Size of k-mer, default 16\", metavar=\"KMERSIZE\")\nparser.add_option(\"-x\", \"--prefix\", dest=\"prefix\",help=\"prefix, e.g. ATGAC, default none\", metavar=\"_id\")\nparser.add_option(\"-a\", \"--printall\", dest=\"printall\", action=\"store_true\",help=\"Print matches to all templates in templatefile unsorted\")\nparser.add_option(\"-w\", \"--winnertakesitall\", dest=\"wta\", action=\"store_true\",help=\"kmer hits are only assigned to most similar template\")\nparser.add_option(\"-e\", \"--evalue\", dest=\"evalue\", help=\"Maximum E-value\", metavar=\"EVALUE\")\n(options, args) = parser.parse_args()\n\n# set up prefix filtering:\nif options.prefix != None:\n    prefix = options.prefix\nelse:\n    prefix = ''\nprefixlen = len(prefix)\n\n\n# get e-value:\nif options.evalue != None:\n    evalue = float(options.evalue)\nelse:\n    evalue = float(0.05)\n\n\n# Open queryfile:\nt0 = time.time()\nif options.inputfilename != None:\n    if options.inputfilename == \"--\":\n        inputfile = sys.stdin\n    else:\n        inputfile = open(options.inputfilename, \"r\")\n\n# open templatefile:\nif options.templatefilename != None:\n    templatefile = open(options.templatefilename + \".p\", \"rb\")\n    templatefile_lengths = open(options.templatefilename + \".len.p\", \"rb\")\n    try:\n        templatefile_ulengths = open(\n            options.templatefilename + \".ulen.p\", \"rb\")\n    except:\n        # do nothing\n        two = 2\n    templatefile_descriptions = open(\n        options.templatefilename + \".desc.p\", \"rb\")\nelse:\n    sys.exit(\"No template file specified\")\n\n# open outputfile:\nif options.outputfilename != None:\n    outputfile = open(options.outputfilename, \"w\")\nelse:  # If no output filename choose the same as the input filename\n    outputfilename = os.path.splitext(options.inputfilename)[0]\n    outputfile = open(outputfilename, \"w\")\n\n# get kmer size:\nif options.kmersize != None:\n    kmersize = int(options.kmersize)\nelse:\n    kmersize = 16\n\n\n##########################################################################\n# READ DATABASE OF TEMPLATES\n##########################################################################\n\ntemplates = {}\nNtemplates = 0\n\n\n# Read Template file:\nsys.stdout.write(\"%s\\n\" % (\"# Reading database of templates\"))\ntemplates = pickle.load(templatefile)\ntemplates_lengths = pickle.load(templatefile_lengths)\ntry:\n    templates_ulengths = pickle.load(templatefile_ulengths)\nexcept:\n    sys.stderr.write('No ulen.p file found for database')\n    SystemExit()\ntemplates_descriptions = pickle.load(templatefile_descriptions)\n\n# Count number of k-mers, and sum of unique k-mers over all templates:\ntemplate_tot_len = 0\ntemplate_tot_ulen = 0\nNtemplates = 0\n# length added\nfor name in templates_lengths:\n    template_tot_len += templates_lengths[name]\n    template_tot_ulen += templates_ulengths[name]\n    Ntemplates += 1\n\n\n##########################################################################\n# READ INPUTFILE\n##########################################################################\n\nqueryseq = \"\"\nqueryseqsegments = []\nNquerys = 0\nqueryindex = {}\nqtotlen = 0\nquerymers = 0\nuquerymers = 0\ni = 0\n\nif options.inputfilename != None:\n    sys.stdout.write(\"%s\\n\" % (\"# Reading inputfile\"))\n    for line in inputfile:\n        fields = line.split()\n        if len(fields) >= 1:\n            # FASTA file:\n            if fields[0][0] == \">\":\n                if (i > 0):\n                    queryseq = ''.join(queryseqsegments)\n\n                    # Update dictionary of kmers\n                    save_kmers(queryseq)\n\n                del queryseqsegments\n                queryseqsegments = []\n                i = 0\n\n            # Fastq file:\n            elif fields[0][0] == \"@\":\n                # Fastq file\n                if (i > 0):\n                    queryseq = ''.join(queryseqsegments)\n\n                    # Update dictionary of kmers:\n                    save_kmers(queryseq)\n\n                del queryseqsegments\n                queryseqsegments = []\n                i = 0\n                queryseq = \"\"\n\n                try:\n                    line = inputfile.next()\n                    fields = line.split()\n                    queryseqsegments.append(\"\")\n                    queryseqsegments[i] = fields[0]\n                    i += 1\n                    line = inputfile.next()\n                    line = inputfile.next()\n                except:\n                    break\n            else:\n                queryseqsegments.append(\"\")\n                queryseqsegments[i] = fields[0].upper()\n                i += 1\n\n    queryseq = ''.join(queryseqsegments)\n\n    # Update dictionary of K-mers:\n    save_kmers(queryseq)\n\ndel queryseqsegments\n\n\n##########################################################################\n# SEARCH FOR MATCHES\n##########################################################################\n\nsys.stdout.write(\"%s\\n\" % (\"# Searching for matches of input in template\"))\nmincoverage = 1\ntemplateentries = {}\ntemplateentries_tot = {}\nNhits = 0\n\n(templateentries, templateentries_tot, Nhits) = find_matches()\n\n\n##########################################################################\n#\tDO STATISTICS\n##########################################################################\n\n\nminscore = 0\netta = 1.0e-8  # etta is a small number to avoid division by zero\n\n# report search statistics:\nsys.stdout.write(\"%s\\n\" % (\"# Search statistics\"))\nsys.stdout.write(\"%s\\n\" % (\"# Total number of hits: %s\") % (Nhits))\nsys.stdout.write(\"%s\\n\" % (\"# Total number of kmers in templates : %s\") % (template_tot_len))\nsys.stdout.write(\"%s\\n\" % (\"# Minimum number of k-mer hits to report template: %s\") % (minscore))\nsys.stdout.write(\"%s\\n\" % (\"# Maximum multiple testing corrected E-value to report match : %s\") % (evalue))\nsys.stdout.write(\"%s\\n\" % (\"# Printing best matches\"))\n\n\n# print heading of outputfile:\nif options.wta != True:\n    outputfile.write(\"#Template\\tScore\\tExpected\\tz\\tp_value\\tquery coverage [%]\\ttemplate coverage [%]\\tdepth\\tKmers in Template\\tDescription\\n\")\nelif options.wta == True:\n    outputfile.write(\"#Template\\tScore\\tExpected\\tz\\tp_value\\tquery coverage [%]\\ttemplate coverage [%]\\tdepth\\ttotal query coverage [%]\\ttotal template coverage [%]\\ttotal depth\\tKmers in Template\\tDescription\\n\")\n\n\n##########################################################################\n#\tSTANDARD SCORING SCHEME\n##########################################################################\n\n\nif not options.wta == True:\n\n    sortedlist = sorted(\n        templateentries.items(), key=itemgetter(1), reverse=True)\n    for template, score in sortedlist:\n        if score > minscore:\n            expected = float(\n                Nhits) * float(templates_ulengths[template]) / float(template_tot_ulen)\n            #z = (score - expected)/sqrt(score + expected+etta)\n            #p  = fastp(z)\n            #\n            # If expected < 1 the above poisson approximation is a poor model\n            # Use instead: probabilyty of seing X hits is p**X if probability\n            # of seing one hit is p (like tossing a dice X times)\n            #\n            # if expected <1:\n            #  p = expected**score\n            #\n            # Comparison of two fractions, Statistical methods in medical\n            # research, Armitage et al. p. 125:\n            z = z_from_two_samples(\n                score, templates_ulengths[template], Nhits, template_tot_ulen)\n            p = fastp(z)\n\n            # Correction for multiple testing:\n            p_corr = p * Ntemplates\n            frac_q = ( score / (float(uquerymers) + etta)) * 100\n            frac_d = (score / (templates_ulengths[template] + etta)) * 100\n            coverage = templateentries_tot[\n                template] / float(templates_lengths[template])\n\n            # print str(p) + \" \" + str(p_corr) + \" \" + str(evalue)\n            # p_corr=0\n            if p_corr <= evalue:\n                outputfile.write(\"%-12s\\t%8d\\t%8d\\t%8.2f\\t%4.1e\\t%8.2f\\t%8.2f\\t%8.2f\\t%8d\\t%s\\n\" %\n                                 (template, score, int(round(expected)), round(z, 1), p_corr, frac_q, frac_d, coverage, templates_ulengths[template], templates_descriptions[template].strip()))\n\n\n##########################################################################\n#\tWINNER TAKES IT ALL\n##########################################################################\n\nif options.wta == True:\n\n    w_templateentries = templateentries.copy()\n    w_templateentries_tot = templateentries_tot.copy()\n    w_Nhits = Nhits\n\n    maxhits = 100\n    hitcounter = 1\n    stop = False\n    while not stop:\n        hitcounter += 1\n        if hitcounter > maxhits:\n            stop = True\n        sortedlist = sorted(\n            w_templateentries.items(), key=itemgetter(1), reverse=True)\n        for template, score in sortedlist:\n            if score > minscore:\n                expected = float(\n                    w_Nhits) * float(templates_ulengths[template]) / float(template_tot_ulen)\n                #z = (score - expected)/sqrt(score + expected+etta)\n                #p  = fastp(z)\n                #\n                # If expected < 1 the above poisson approximation is a poor model\n                #\n                # if expected <1:\n                #  p = expected**score\n                #\n                # Comparison of two fractions, Statistical methods in medical\n                # research, Armitage et al. p. 125:\n                z = z_from_two_samples(\n                    score, templates_ulengths[template], w_Nhits, template_tot_ulen)\n                p = fastp(z)\n\n                # correction for multiple testing:\n                p_corr = p * Ntemplates\n                # print score,float(uquerymers),etta\n                frac_q = (score / (float(uquerymers) + etta)) * 100\n                frac_d = (score / (templates_ulengths[template] + etta)) * 100\n                coverage = w_templateentries_tot[\n                    template] / float(templates_lengths[template])\n\n                # calculate total values:\n                tot_frac_q = (templateentries[template] / (float(uquerymers) + etta)) * 100\n                tot_frac_d = (templateentries[template] / (templates_ulengths[template] + etta)) * 100\n                tot_coverage = templateentries_tot[template] / float(templates_lengths[template])\n\n                # print results to outputfile:\n                if p_corr <= evalue:\n                    outputfile.write(\"%-12s\\t%8d\\t%8d\\t%8.1f\\t%4.2e\\t%8.2f\\t%8.2f\\t%4.2f\\t%8.2f\\t%8.2f\\t%4.2f\\t%8d\\t%s\\n\" %\n                                     (template, score, int(round(expected)), round(z, 1), p_corr, frac_q, frac_d, coverage, tot_frac_q, tot_frac_d, tot_coverage, templates_ulengths[template], templates_descriptions[template].strip()))\n\n                    # remove all kmers in best hit from queryindex\n                    for submer in queryindex:\n                        matches = templates[submer].split(\",\")\n                        if template in matches:\n                            queryindex[submer] = 0\n\n                    # find best hit like before:\n                    del w_templateentries\n                    del w_templateentries_tot\n                    w_templateentries = {}\n                    w_templateentries_tot = {}\n                    w_Nhits = 0\n\n                    (w_templateentries, w_templateentries_tot,\n                     w_Nhits) = find_matches()\n\n                else:\n                    stop = True\n            break\n\n##########################################################################\n# CLOSE FILES\n##########################################################################\n\nt1 = time.time()\n\nsys.stdout.write(\"\\r# %s kmers (%s kmers / s). Total time used: %s sec\" %(\"{:,}\".format(querymers), \"{:,}\".format(querymers / (t1 - t0)), int(t1 - t0)))\nsys.stdout.flush()\nsys.stdout.write(\"\\n\")\n#sys.stderr.write(\"DONE!\")\nsys.stdout.write(\"# Closing files\\n\")\n"}
{"file_name": "cleanDenovo.py", "file_path": "/scripts/step_2AS_filtering__seqio/cleanDenovo.py", "language": "python", "id": "cleanDenovo", "content": "#!/usr/bin/env python3\n\nimport sys\nfrom Bio import SeqIO\n\n\ndef clean_denovo(l200_file, calls_file, reference, output_file):\n    reference_plain_name = reference.replace(\"_\", \"\") \n    abricateout = open(calls_file, \"r\").readlines()\n    node = []\n    for riga in abricateout:\n        if reference_plain_name in riga.replace(\"_\", \"\"): #NC_045512 -> NC045512 FIXME it's more robust to modify nextflow steps instead and look for reference\n            node.append(riga.split()[1].strip())\n    filt_sequences = []\n    denovo = open(l200_file, 'r')\n    for record in SeqIO.parse(denovo, \"fasta\"):\n        head_seq = record.id\n        if head_seq in node:\n            filt_sequences.append(record)\n    if len(filt_sequences) > 0:\n        with open(output_file, \"w\") as output_handle:\n            SeqIO.write(filt_sequences, output_handle, \"fasta\")\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 5:\n        sys.stderr.write(\"Usage: %s <input_file> <cmp> <reference> <output_file>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        clean_denovo(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])\n"}
{"file_name": "AssemblyFilter.py", "file_path": "/scripts/step_2AS_denovo__spades/AssemblyFilter.py", "language": "python", "id": "AssemblyFilter", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import median,average\nimport sys\nimport argparse\n\n \ndef filt_seq(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_spades_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_spades.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.id\n\t\tcoverage = float(headSeq.split('cov_')[1])\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\ndef filt_seq_unicycler(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_unicycler_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_unicycler.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.description\n\t\tprint(headSeq)\n\t\tcoverage = float(headSeq.split(' ')[2].lstrip(\"depth=\").strip(\"x\"))\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\nif __name__==\"__main__\":\n\t\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-f', '--fasta',help='Scaffolds fasta',type=str, required=True)\n\tparser.add_argument('-l', '--length',help='Min length',type=int, required=True)\n\tparser.add_argument('-c', '--cov',help='Min kmerCov ',type=int, required=True)\n\tparser.add_argument('-u', '--unicycler',help='Unicycler fasta',action='store_true')\n\targs = parser.parse_args()\n\t\n\tif not args.unicycler:\n\t\tfilt_seq(args.name,args.fasta,args.length,args.cov)\n\telse:\n\t\tfilt_seq_unicycler(args.name,args.fasta,args.length,args.cov)"}
{"file_name": "seq_filter_by_id.py", "file_path": "/scripts/module_obitools/seq_filter_by_id.py", "language": "python", "id": "seq_filter_by_id", "content": "#!/usr/bin/env python\n\"\"\"Filter a FASTA, FASTQ or SSF file with IDs from a tabular file.\n\nTakes six command line options, tabular filename, ID column numbers (comma\nseparated list using one based counting), input filename, input type (e.g.\nFASTA or SFF) and up to two output filenames (for records with and without\nthe given IDs, same format as input sequence file).\n\nWhen filtering an SFF file, any Roche XML manifest in the input file is\npreserved in both output files.\n\nNote in the default NCBI BLAST+ tabular output, the query sequence ID is\nin column one, and the ID of the match from the database is in column two.\nHere sensible values for the column numbers would therefore be \"1\" or \"2\".\n\nThis tool is a short Python script which requires Biopython 1.54 or later.\nIf you use this tool in scientific work leading to a publication, please\ncite the Biopython application note:\n\nCock et al 2009. Biopython: freely available Python tools for computational\nmolecular biology and bioinformatics. Bioinformatics 25(11) 1422-3.\nhttps://doi.org/10.1093/bioinformatics/btp163 pmid:19304878.\n\nThis script is copyright 2010-2023 by Peter Cock, The James Hutton Institute\n(formerly the Scottish Crop Research Institute, SCRI), UK. All rights reserved.\nSee accompanying text file for licence details (MIT license).\n\nUse -v or --version to get the version, -h or --help for help.\n\"\"\"\n\nfrom __future__ import print_function\n\nimport os\nimport re\nimport sys\n\nfrom optparse import OptionParser\n\n# Parse Command Line\nusage = \"\"\"Use as follows:\n\n$ python seq_filter_by_id.py [options] tab1 cols1 [, tab2 cols2, ...]\n\ne.g. Positive matches using column one from tabular file:\n\n$ seq_filter_by_id.py -i my_seqs.fastq -f fastq -p matches.fastq ids.tabular 1\n\nMultiple tabular files and column numbers may be given, or replaced with\nthe -t or --text option.\n\"\"\"\nparser = OptionParser(usage=usage)\nparser.add_option(\n    \"-i\",\n    \"--input\",\n    dest=\"input\",\n    default=None,\n    help=\"Input sequences filename\",\n    metavar=\"FILE\",\n)\nparser.add_option(\n    \"-f\",\n    \"--format\",\n    dest=\"format\",\n    default=None,\n    help=\"Input sequence format (e.g. fasta, fastq, sff)\",\n)\nparser.add_option(\n    \"-t\",\n    \"--text\",\n    dest=\"id_list\",\n    default=None,\n    help=\"Lists of white space separated IDs (instead of a tabular file)\",\n)\nparser.add_option(\n    \"-p\",\n    \"--positive\",\n    dest=\"output_positive\",\n    default=None,\n    help=\"Output filename for matches\",\n    metavar=\"FILE\",\n)\nparser.add_option(\n    \"-n\",\n    \"--negative\",\n    dest=\"output_negative\",\n    default=None,\n    help=\"Output filename for non-matches\",\n    metavar=\"FILE\",\n)\nparser.add_option(\n    \"-l\",\n    \"--logic\",\n    dest=\"logic\",\n    default=\"UNION\",\n    help=\"How to combined multiple ID columns (UNION or INTERSECTION)\",\n)\nparser.add_option(\n    \"-s\",\n    \"--suffix\",\n    dest=\"suffix\",\n    action=\"store_true\",\n    help=\"Ignore pair-read suffixes for matching names\",\n)\nparser.add_option(\n    \"-v\",\n    \"--version\",\n    dest=\"version\",\n    default=False,\n    action=\"store_true\",\n    help=\"Show version and quit\",\n)\n\noptions, args = parser.parse_args()\n\nif options.version:\n    print(\"v0.2.9\")\n    sys.exit(0)\n\nin_file = options.input\nseq_format = options.format\nout_positive_file = options.output_positive\nout_negative_file = options.output_negative\nlogic = options.logic\ndrop_suffixes = bool(options.suffix)\n\nif in_file is None or not os.path.isfile(in_file):\n    sys.exit(\"Missing input file: %r\" % in_file)\nif out_positive_file is None and out_negative_file is None:\n    sys.exit(\"Neither output file requested\")\nif seq_format is None:\n    sys.exit(\"Missing sequence format\")\nif logic not in [\"UNION\", \"INTERSECTION\"]:\n    sys.exit(\"Logic agrument should be 'UNION' or 'INTERSECTION', not %r\" % logic)\nif options.id_list and args:\n    sys.exit(\"Cannot accept IDs via both -t in the command line, and as tabular files\")\nelif not options.id_list and not args:\n    sys.exit(\"Expected matched pairs of tabular files and columns (or -t given)\")\nif len(args) % 2:\n    sys.exit(\"Expected matched pairs of tabular files and columns, not: %r\" % args)\n\n\n# Cope with three widely used suffix naming convensions,\n# Illumina: /1 or /2\n# Forward/revered: .f or .r\n# Sanger, e.g. .p1k and .q1k\n# See http://staden.sourceforge.net/manual/pregap4_unix_50.html\n# re_f = re.compile(r\"(/1|\\.f|\\.[sfp]\\d\\w*)$\")\n# re_r = re.compile(r\"(/2|\\.r|\\.[rq]\\d\\w*)$\")\nre_suffix = re.compile(r\"(/1|\\.f|\\.[sfp]\\d\\w*|/2|\\.r|\\.[rq]\\d\\w*)$\")\nassert re_suffix.search(\"demo.f\")\nassert re_suffix.search(\"demo.s1\")\nassert re_suffix.search(\"demo.f1k\")\nassert re_suffix.search(\"demo.p1\")\nassert re_suffix.search(\"demo.p1k\")\nassert re_suffix.search(\"demo.p1lk\")\nassert re_suffix.search(\"demo/2\")\nassert re_suffix.search(\"demo.r\")\nassert re_suffix.search(\"demo.q1\")\nassert re_suffix.search(\"demo.q1lk\")\n\nidentifiers = []\nfor i in range(len(args) // 2):\n    tabular_file = args[2 * i]\n    cols_arg = args[2 * i + 1]\n    if not os.path.isfile(tabular_file):\n        sys.exit(\"Missing tabular identifier file %r\" % tabular_file)\n    try:\n        columns = [int(arg) - 1 for arg in cols_arg.split(\",\")]\n    except ValueError:\n        sys.exit(\n            \"Expected list of columns (comma separated integers), got %r\" % cols_arg\n        )\n    if min(columns) < 0:\n        sys.exit(\n            \"Expect one-based column numbers (not zero-based counting), got %r\"\n            % cols_arg\n        )\n    identifiers.append((tabular_file, columns))\n\nname_warn = False\n\n\ndef check_white_space(name):\n    \"\"\"Check identifier for white space, take first word only.\"\"\"\n    parts = name.split(None, 1)\n    global name_warn\n    if not name_warn and len(parts) > 1:\n        name_warn = (\n            \"WARNING: Some of your identifiers had white space in them, \"\n            + \"using first word only. e.g.:\\n%s\\n\" % name\n        )\n    return parts[0]\n\n\nif drop_suffixes:\n\n    def clean_name(name):\n        \"\"\"Remove suffix.\"\"\"\n        name = check_white_space(name)\n        match = re_suffix.search(name)\n        if match:\n            # Use the fact this is a suffix, and regular expression will be\n            # anchored to the end of the name:\n            return name[: match.start()]\n        else:\n            # Nothing to do\n            return name\n\n    assert clean_name(\"foo/1\") == \"foo\"\n    assert clean_name(\"foo/2\") == \"foo\"\n    assert clean_name(\"bar.f\") == \"bar\"\n    assert clean_name(\"bar.r\") == \"bar\"\n    assert clean_name(\"baz.p1\") == \"baz\"\n    assert clean_name(\"baz.q2\") == \"baz\"\nelse:\n    # Just check the white space\n    clean_name = check_white_space\n\n\nmapped_chars = {\n    \">\": \"__gt__\",\n    \"<\": \"__lt__\",\n    \"'\": \"__sq__\",\n    '\"': \"__dq__\",\n    \"[\": \"__ob__\",\n    \"]\": \"__cb__\",\n    \"{\": \"__oc__\",\n    \"}\": \"__cc__\",\n    \"@\": \"__at__\",\n    \"\\n\": \"__cn__\",\n    \"\\r\": \"__cr__\",\n    \"\\t\": \"__tc__\",\n    \"#\": \"__pd__\",\n}\n\n# Read tabular file(s) and record all specified identifiers\nids = None  # Will be a set\nif options.id_list:\n    assert not identifiers\n    ids = set()\n    id_list = options.id_list\n    # Galaxy turns \\r into __cr__ (CR) etc\n    for k in mapped_chars:\n        id_list = id_list.replace(mapped_chars[k], k)\n    for x in options.id_list.split():\n        ids.add(clean_name(x.strip()))\n    print(\"Have %i unique identifiers from list\" % len(ids))\nfor tabular_file, columns in identifiers:\n    file_ids = set()\n    handle = open(tabular_file)\n    if len(columns) > 1:\n        # General case of many columns\n        for line in handle:\n            if line.startswith(\"#\"):\n                # Ignore comments\n                continue\n            parts = line.rstrip(\"\\n\").split(\"\\t\")\n            for col in columns:\n                name = clean_name(parts[col])\n                if name:\n                    file_ids.add(name)\n    else:\n        # Single column, special case speed up\n        col = columns[0]\n        for line in handle:\n            if not line.strip():  # skip empty lines\n                continue\n            if not line.startswith(\"#\"):\n                name = clean_name(line.rstrip(\"\\n\").split(\"\\t\")[col])\n                if name:\n                    file_ids.add(name)\n    print(\n        \"Using %i IDs from column %s in tabular file\"\n        % (len(file_ids), \", \".join(str(col + 1) for col in columns))\n    )\n    if ids is None:\n        ids = file_ids\n    if logic == \"UNION\":\n        ids.update(file_ids)\n    else:\n        ids.intersection_update(file_ids)\n    handle.close()\nif len(identifiers) > 1:\n    if logic == \"UNION\":\n        print(\n            \"Have %i IDs combined from %i tabular files\" % (len(ids), len(identifiers))\n        )\n    else:\n        print(\n            \"Have %i IDs in common from %i tabular files\" % (len(ids), len(identifiers))\n        )\nif name_warn:\n    sys.stderr.write(name_warn)\n\n\ndef crude_fasta_iterator(handle):\n    \"\"\"Parse FASTA file yielding tuples of (name, sequence).\"\"\"\n    while True:\n        line = handle.readline()\n        if line == \"\":\n            return  # Premature end of file, or just empty?\n        if line[0] == \">\":\n            break\n\n    no_id_warned = False\n    while True:\n        if line[0] != \">\":\n            raise ValueError(\"Records in Fasta files should start with '>' character\")\n        try:\n            id = line[1:].split(None, 1)[0]\n        except IndexError:\n            if not no_id_warned:\n                sys.stderr.write(\"WARNING - Malformed FASTA entry with no identifier\\n\")\n                no_id_warned = True\n            id = None\n        lines = [line]\n        line = handle.readline()\n        while True:\n            if not line:\n                break\n            if line[0] == \">\":\n                break\n            lines.append(line)\n            line = handle.readline()\n        yield id, \"\".join(lines)\n        if not line:\n            return  # StopIteration\n\n\ndef fasta_filter(in_file, pos_file, neg_file, wanted):\n    \"\"\"FASTA filter producing 60 character line wrapped outout.\"\"\"\n    pos_count = neg_count = 0\n    # Galaxy now requires Python 2.5+ so can use with statements,\n    with open(in_file) as in_handle:\n        # Doing the if statement outside the loop for speed\n        # (with the downside of three very similar loops).\n        if pos_file is not None and neg_file is not None:\n            print(\"Generating two FASTA files\")\n            with open(pos_file, \"w\") as pos_handle:\n                with open(neg_file, \"w\") as neg_handle:\n                    for identifier, record in crude_fasta_iterator(in_handle):\n                        if clean_name(identifier) in wanted:\n                            pos_handle.write(record)\n                            pos_count += 1\n                        else:\n                            neg_handle.write(record)\n                            neg_count += 1\n        elif pos_file is not None:\n            print(\"Generating matching FASTA file\")\n            with open(pos_file, \"w\") as pos_handle:\n                for identifier, record in crude_fasta_iterator(in_handle):\n                    if clean_name(identifier) in wanted:\n                        pos_handle.write(record)\n                        pos_count += 1\n                    else:\n                        neg_count += 1\n        else:\n            print(\"Generating non-matching FASTA file\")\n            assert neg_file is not None\n            with open(neg_file, \"w\") as neg_handle:\n                for identifier, record in crude_fasta_iterator(in_handle):\n                    if clean_name(identifier) in wanted:\n                        pos_count += 1\n                    else:\n                        neg_handle.write(record)\n                        neg_count += 1\n    return pos_count, neg_count\n\n\ndef fastq_filter(in_file, pos_file, neg_file, wanted):\n    \"\"\"FASTQ filter.\"\"\"\n    from Bio.SeqIO.QualityIO import FastqGeneralIterator\n\n    handle = open(in_file, \"r\")\n    if pos_file is not None and neg_file is not None:\n        print(\"Generating two FASTQ files\")\n        positive_handle = open(pos_file, \"w\")\n        negative_handle = open(neg_file, \"w\")\n        print(in_file)\n        for title, seq, qual in FastqGeneralIterator(handle):\n            print(\"%s --> %s\" % (title, clean_name(title.split(None, 1)[0])))\n            if clean_name(title.split(None, 1)[0]) in wanted:\n                positive_handle.write(\"@%s\\n%s\\n+\\n%s\\n\" % (title, seq, qual))\n            else:\n                negative_handle.write(\"@%s\\n%s\\n+\\n%s\\n\" % (title, seq, qual))\n        positive_handle.close()\n        negative_handle.close()\n    elif pos_file is not None:\n        print(\"Generating matching FASTQ file\")\n        positive_handle = open(pos_file, \"w\")\n        for title, seq, qual in FastqGeneralIterator(handle):\n            if clean_name(title.split(None, 1)[0]) in wanted:\n                positive_handle.write(\"@%s\\n%s\\n+\\n%s\\n\" % (title, seq, qual))\n        positive_handle.close()\n    elif neg_file is not None:\n        print(\"Generating non-matching FASTQ file\")\n        negative_handle = open(neg_file, \"w\")\n        for title, seq, qual in FastqGeneralIterator(handle):\n            if clean_name(title.split(None, 1)[0]) not in wanted:\n                negative_handle.write(\"@%s\\n%s\\n+\\n%s\\n\" % (title, seq, qual))\n        negative_handle.close()\n    handle.close()\n    # This does not currently bother to record record counts (faster)\n\n\ndef sff_filter(in_file, pos_file, neg_file, wanted):\n    \"\"\"SFF filter.\"\"\"\n    try:\n        from Bio.SeqIO.SffIO import SffIterator, SffWriter\n    except ImportError:\n        sys.exit(\"SFF filtering requires Biopython 1.54 or later\")\n\n    try:\n        from Bio.SeqIO.SffIO import ReadRocheXmlManifest\n    except ImportError:\n        # Prior to Biopython 1.56 this was a private function\n        from Bio.SeqIO.SffIO import _sff_read_roche_index_xml as ReadRocheXmlManifest\n\n    in_handle = open(in_file, \"rb\")  # must be binary mode!\n    try:\n        manifest = ReadRocheXmlManifest(in_handle)\n    except ValueError:\n        manifest = None\n\n    # This makes two passes though the SFF file with isn't so efficient,\n    # but this makes the code simple.\n    pos_count = neg_count = 0\n    if pos_file is not None:\n        out_handle = open(pos_file, \"wb\")\n        writer = SffWriter(out_handle, xml=manifest)\n        in_handle.seek(0)  # start again after getting manifest\n        pos_count = writer.write_file(\n            rec for rec in SffIterator(in_handle) if clean_name(rec.id) in wanted\n        )\n        out_handle.close()\n    if neg_file is not None:\n        out_handle = open(neg_file, \"wb\")\n        writer = SffWriter(out_handle, xml=manifest)\n        in_handle.seek(0)  # start again\n        neg_count = writer.write_file(\n            rec for rec in SffIterator(in_handle) if clean_name(rec.id) not in wanted\n        )\n        out_handle.close()\n    # And we're done\n    in_handle.close()\n    # At the time of writing, Galaxy doesn't show SFF file read counts,\n    # so it is useful to put them in stdout and thus shown in job info.\n    return pos_count, neg_count\n\n\nif seq_format.lower() == \"sff\":\n    # Now write filtered SFF file based on IDs wanted\n    pos_count, neg_count = sff_filter(\n        in_file, out_positive_file, out_negative_file, ids\n    )\n    # At the time of writing, Galaxy doesn't show SFF file read counts,\n    # so it is useful to put them in stdout and thus shown in job info.\nelif seq_format.lower() == \"fasta\":\n    # Write filtered FASTA file based on IDs from tabular file\n    pos_count, neg_count = fasta_filter(\n        in_file, out_positive_file, out_negative_file, ids\n    )\n    print(\"%i with and %i without specified IDs\" % (pos_count, neg_count))\nelif seq_format.lower().startswith(\"fastq\"):\n    # Write filtered FASTQ file based on IDs from tabular file\n    fastq_filter(in_file, out_positive_file, out_negative_file, ids)\n    # This does not currently track the counts\nelse:\n    sys.exit(\"Unsupported file type %r\" % seq_format)"}
{"file_name": "AssemblyFilter.py", "file_path": "/scripts/step_2AS_denovo__unicycler/AssemblyFilter.py", "language": "python", "id": "AssemblyFilter", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import median,average\nimport sys\nimport argparse\n\n \ndef filt_seq(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_spades_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_spades.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.id\n\t\tcoverage = float(headSeq.split('cov_')[1])\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\ndef filt_seq_unicycler(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_unicycler_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_unicycler.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.description\n\t\tprint(headSeq)\n\t\tcoverage = float(headSeq.split(' ')[2].lstrip(\"depth=\").strip(\"x\"))\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\nif __name__==\"__main__\":\n\t\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-f', '--fasta',help='Scaffolds fasta',type=str, required=True)\n\tparser.add_argument('-l', '--length',help='Min length',type=int, required=True)\n\tparser.add_argument('-c', '--cov',help='Min kmerCov ',type=int, required=True)\n\tparser.add_argument('-u', '--unicycler',help='Unicycler fasta',action='store_true')\n\targs = parser.parse_args()\n\t\n\tif not args.unicycler:\n\t\tfilt_seq(args.name,args.fasta,args.length,args.cov)\n\telse:\n\t\tfilt_seq_unicycler(args.name,args.fasta,args.length,args.cov)"}
{"file_name": "SampleReadsCheck_ionTorrent.py", "file_path": "/scripts/step_1PP_trimming__trimmomatic/SampleReadsCheck_ionTorrent.py", "language": "python", "id": "SampleReadsCheck_ionTorrent", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import median,average\nimport sys\nimport argparse\n#import multiprocessing as mp\n\ndef safe_open(file, mode='rt'):\n\t# safe open file\n\tif file.endswith('.gz'):\n\t\timport gzip\n\t\treturn gzip.open(file, mode)\n\telse:\n\t\treturn open(file, mode)\n\t\ndef readsInfo(fastq):\n\tres = {}\n\tlengthReads=[]\n\treadsQual30=0\n\tavgQual=[]\n\thandle = safe_open(fastq)\n\tfor record in SeqIO.parse(handle, \"fastq\"):\n\t\tlengthReads.append(len(record.seq))\n\t\tphredQualityList = record.letter_annotations['phred_quality']\n\t\tavgQual.append(average(phredQualityList))\n\t\tif average(phredQualityList)>30:\n\t\t\treadsQual30+=1\n\thandle.close()\n\t\n\tq30Reads = round(readsQual30/float(len(lengthReads))*100,2)\n\tMbases = round(float(sum(lengthReads))/1000000,2)\n\n\tres.update({\"lengthReads\":lengthReads})\n\tres.update({\"avgQual\":avgQual})\n\tres.update({\"readsQual30\":readsQual30})\n\tres.update({\"q30Reads\":q30Reads})\n\tres.update({\"Mbases\":Mbases})\n\treturn res\n\n# Class Definition\nclass SampleRaw:\n\t\n\tdef __init__(self, read1):\n\t\tprint(\"Processing raw reads...\")\n\t\tself.R1 = readsInfo(read1)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_R1(self):\n\t\treturn self.R1\n\t\t\n\tdef get_warnings(self):\n\t\treturn self.warnings\n\t\t\n\tdef get_fails(self):\n\t\treturn self.fails\n\t\n\tdef getNR(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef get_q30Reads(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treadsQual30 = self.R1.get(\"readsQual30\")\n\t\treturn round(readsQual30/float(len(lengthReads))*100,2)\n\t\t\n\tdef get_Qavg(self):\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\treturn round(median(avgQual),2)\n\t\n\tdef get_stats(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\tMbases = round(float(sum(lengthReads))/1000000,2)\n\t\treturn [len(lengthReads),Mbases,min(lengthReads),max(lengthReads),round(average(lengthReads),2),self.get_q30Reads(),min(avgQual),max(avgQual),round(median(avgQual),2)]\n\t\n\tdef checkQ30(self):\n\t\tq30R = self.get_q30Reads()\n\t\tif float(q30R) >= 25:\n\t\t\treturn \n\t\telif float(q30R) >= 5 and float(q30R) < 25:\n\t\t\tself.warnings.append(\"WARN_rawQ30\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawQ30\")\n\t\t\t\n\tdef checkQavg(self):\n\t\tavgQualR = self.get_Qavg()\n\t\tif float(avgQualR) >= 22:\n\t\t\treturn \n\t\telif float(avgQualR) >= 16 and float(avgQualR) < 22:\n\t\t\tself.warnings.append(\"WARN_rawQavg\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawQavg\")\n\t\n\tdef checkNR_Bact(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 350000:\n\t\t\treturn \n\t\telif float(readsTot) >= 250000 and float(readsTot) < 350000:\n\t\t\tself.warnings.append(\"WARN_rawNR_bact\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawNR_bact\")\n\t\t\t\n\tdef checkNR_Vir(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 100000:\n\t\t\treturn \n\t\telif float(readsTot) >= 500000 and float(readsTot) < 100000:\n\t\t\tself.warnings.append(\"WARN_rawNR_vir\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawNR_vir\")\n\nclass SampleTrimmed:\n\t\n\tdef __init__(self, read1):\n\t\tprint(\"Processing trimmed reads...\")\n\t\tself.R1 = readsInfo(read1)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_R1(self):\n\t\treturn self.R1\n\t\n\tdef get_warnings(self):\n\t\treturn self.warnings\n\t\t\n\tdef get_fails(self):\n\t\treturn self.fails\n\t\n\tdef getNR_total(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef getNR(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef get_q30Reads(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treadsQual30 = self.R1.get(\"readsQual30\")\n\t\treturn round(readsQual30/float(len(lengthReads))*100,2)\n\t\t\n\tdef get_Qavg(self):\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\treturn round(median(avgQual),2)\n\t\n\tdef get_stats(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\tMbases = round(float(sum(lengthReads))/1000000,2)\n\t\treturn [self.getNR_total(),0,0,Mbases,min(lengthReads),max(lengthReads),round(average(lengthReads),2),self.get_q30Reads(),min(avgQual),max(avgQual),round(median(avgQual),2)]\n\t\n\tdef checkQ30(self):\n\t\tq30R = self.get_q30Reads()\n\t\tif float(q30R) >= 35:\n\t\t\treturn \n\t\telif float(q30R) >= 10 and float(q30R) < 35:\n\t\t\tself.warnings.append(\"WARN_trimQ30\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimQ30\")\n\t\t\t\n\tdef checkQavg(self):\n\t\tavgQualR = self.get_Qavg()\n\t\tif float(avgQualR) >= 24:\n\t\t\treturn \n\t\telif float(avgQualR) >= 18 and float(avgQualR) < 24:\n\t\t\tself.warnings.append(\"WARN_trimQavg\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimQavg\")\n\t\n\tdef checkNR_Bact(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 300000:\n\t\t\treturn \n\t\telif float(readsTot) >= 50000 and float(readsTot) < 300000:\n\t\t\tself.warnings.append(\"WARN_trimNR_bact\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimNR_bact\")\n\t\t\t\n\tdef checkNR_Vir(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 90000:\n\t\t\treturn \n\t\telif float(readsTot) >= 40000 and float(readsTot) < 90000:\n\t\t\tself.warnings.append(\"WARN_trimNR_vir\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimNR_vir\")\n\t\nclass Sample:\n\t\n\tdef __init__(self, sampleName, R1, T1=None):\n\t\tself.name = sampleName\n\t\tself.Raw = SampleRaw(R1)\n\t\tif T1 != None:\n\t\t\tself.Trimmed = SampleTrimmed(T1)\n\t\telse:\n\t\t\tself.Trimmed = None\n\t\t#self.Pear = SamplePear(Fpear,Rpear,Merged)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_name(self):\n\t\treturn self.name\n\n\tdef raw_check(self):\n\t\tself.Raw.checkQ30()\n\t\tself.Raw.checkQavg()\n\t\tself.Raw.checkNR_Bact()\n\t\tself.Raw.checkNR_Vir()\n\t\twith open(self.get_name()+\"_SRC_raw.csv\",'w') as checkOut:\n\t\t\tresults = \",\".join(str(x) for x in self.Raw.get_stats())\n\t\t\tcheckOut.write(\"#Sample_name,Total_reads,Total_Mbases,Min_length,Max_length,Mean_length,Q30_reads,Min_avgQual,Max_avgQual,Med_avgQual\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\"\\n\")\n\t\treturn\n\t\n\tdef raw_esito(self):\n\t\tresWarn = \":\".join(self.Raw.get_warnings())\n\t\tresFail = \":\".join(self.Raw.get_fails())\n\t\twith open(self.get_name()+\"_SRC_raw.check\",'w') as resOut:\n\t\t\tresOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n\t\t\tresOut.write(self.get_name()+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\n\tdef trimm_check(self):\n\t\tself.Trimmed.checkQ30()\n\t\tself.Trimmed.checkQavg()\n\t\tself.Trimmed.checkNR_Bact()\n\t\tself.Trimmed.checkNR_Vir()\n\t\twith open(self.get_name()+\"_SRC_treads.csv\",'w') as checkOut:\n\t\t\tresults = \",\".join(str(x) for x in self.Trimmed.get_stats())\n\t\t\tcheckOut.write(\"#Sample_name,Total_reads,Unpaired_reads,Paired_reads,Paired_Mbases,Min_PairedLength,Max_PairedLength,Mean_PairedLength,Q30_PairedReads,Min_PairedAvgQual,Max_PairedAvgQual,Mean_PairedAvgQual\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\"\\n\")\n\t\treturn\n\t\t\t\n\tdef trimm_esito(self):\n\t\tresWarn = \":\".join(self.Trimmed.get_warnings())\n\t\tresFail = \":\".join(self.Trimmed.get_fails())\n\t\twith open(self.get_name()+\"_SRC_treads.check\",'w') as resOut:\n\t\t\tresOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n\t\t\tresOut.write(self.get_name()+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\t\n\tdef trimDiscarded(self):\n\t\tdiscarded = self.Raw.getNR()-self.Trimmed.getNR_total()\n\t\treturn discarded\n\t\t\n\tdef trimOverlap(self):\n\t\toverlap = round(float(self.Pear.getNR_Merged())/float(self.Trimmed.getNR())*100,2)\n\t\treturn overlap\n\t\t\n\t\t\t\n\tdef makeReport(self):\n\t\twith open(self.get_name()+\"_readsCheck.csv\",'w') as checkOut:\n\t\t\tresRaw = self.Raw.get_stats()\n\t\t\tif self.Trimmed != None:\n\t\t\t\tresTrim = self.Trimmed.get_stats()\n\t\t\t\tresWarn = \":\".join(self.Raw.get_warnings()+self.Trimmed.get_warnings())\n\t\t\t\tresFail = \":\".join(self.Raw.get_fails()+self.Trimmed.get_fails())\n\t\t\t\tresults = \",\".join(str(x) for x in [resRaw[0],resRaw[1],resRaw[4],resRaw[5],resRaw[8],resTrim[0],self.trimDiscarded(),resTrim[1],resTrim[2],resTrim[3],resTrim[6],resTrim[7],resTrim[10],0])\n\t\t\telse:\n\t\t\t\tresTrim = [0,0,0,0,0,0,0,0,0,0,0]\n\t\t\t\tresWarn = \":\".join(self.Raw.get_warnings())\n\t\t\t\tresFail = \":\".join(self.Raw.get_fails())\n\t\t\t\tresults = \",\".join(str(x) for x in [resRaw[0],resRaw[1],resRaw[4],resRaw[5],resRaw[8],resTrim[0],0,resTrim[1],resTrim[2],resTrim[3],resTrim[6],resTrim[7],resTrim[10],0])\n\t\t\tcheckOut.write(\"#Sample_name,Total_rawReads,Total_rawMbases,Mean_rawLength,Q30_rawReads,Mean_rawAvgQual,Total_trimReads,trimDiscarded,trimUnpaired,trimPaired,trimPairedMbases,Mean_trimPairedLength,Q30_trimPairedReads,Mean_trimPairedAvgQual,Overlap_trimPaired,WARNING,FAIL\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\n# MAIN\nif __name__ == '__main__':\n\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-R1', '--Read1',help='Fastq R1',type=str, required=True)\n\t#parser.add_argument('-R2', '--Read2',help='Fastq R2',type=str)\n\tparser.add_argument('-T1', '--Treads1',help='Fastq R1',type=str)\n\t# parser.add_argument('-T2', '--Treads2',help='Fastq R2',type=str, required=True)\n\t# parser.add_argument('-U', '--Unpaired',help='Fastq Unpaired',type=str, required=True)\n\t# parser.add_argument('-F', '--Frw',help='Fastq Forward Pear',type=str, required=True)\n\t# parser.add_argument('-R', '--Rev',help='Fastq Reverse Pear',type=str, required=True)\n\t# parser.add_argument('-M', '--Mrg',help='Fastq Merged Pear',type=str, required=True)\n\targs = parser.parse_args()\n\n\tif args.Treads1 != None:\n\t\tsample = Sample(args.name,args.Read1,args.Treads1)\n\t\t# RAW CHECK\n\t\tsample.raw_check()\n\t\tsample.raw_esito()\n\t\t\n\t\t# TRIMMED CHECK\n\t\tsample.trimm_check()\n\t\tsample.trimm_esito()\n\t\t\n\t\t# CREATE REPORT\n\t\tsample.makeReport()\n\telse:\n\t\tsample = Sample(args.name,args.Read1)\n\t\t# RAW CHECK\n\t\tsample.raw_check()\n\t\tsample.raw_esito()\n\n\t\t# CREATE REPORT\n\t\tsample.makeReport()\n\t\n\t\n\t\n\t\n"}
{"file_name": "SampleReadsCheck.py", "file_path": "/scripts/step_1PP_trimming__trimmomatic/SampleReadsCheck.py", "language": "python", "id": "SampleReadsCheck", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import average\nimport os\nimport sys\nimport argparse\nfrom multiprocessing import Pool\nimport functools\n\ndef decode(c):\n    return ord(c) - 33\t\n\ndef length_and_quality(title, sequence, quality):\n    qual = functools.reduce(lambda a, b: a+b, map(decode, quality)) / len(quality)\n    return [ len(sequence), qual ]\n\ndef safe_open(file, mode='rt'):\n    # safe open file\n    if file.endswith('.gz'):\n        import gzip\n        return gzip.open(file, mode)\n    else:\n        return open(file, mode)\n\n\ndef readsInfo(fastq):\n    res = {}\n    lengthReads = []\n    readsQual30 = 0\n    avgQual = []\n    try:\n        pool_size = min(os.cpu_count(), 8)\n\n        with safe_open(fastq) as handle, Pool(pool_size) as pool:\n            result = pool.starmap(length_and_quality, SeqIO.QualityIO.FastqGeneralIterator(handle))\n        \n        for (seqlen, qual) in result:\n            lengthReads.append(seqlen)\n            avgQual.append(qual)\n            if qual > 30:\n                readsQual30 += 1\n        \n        # lengthReads = list(map(lambda a: a[0], result)) \n        # avgQual = list(map(lambda a: a[1], result)) \n        # readsQual30 = len(list(filter(lambda x: x > 30, avgQual)))\n\n    except Exception as e: \n        print(e)\n        print(\"Errore nel file \" + fastq)\n        exit(1)\n\n    if len(lengthReads) != 0:\n        q30Reads = round(readsQual30/float(len(lengthReads))*100,2)\n        Mbases = round(float(sum(lengthReads))/1000000,2)\n        res.update({\"countReads\":len(lengthReads)})\n        res.update({\"minLengthReads\":min(lengthReads)})\n        res.update({\"maxLengthReads\":max(lengthReads)})\n        res.update({\"avgLengthReads\":average(lengthReads)})\n        res.update({\"minQual\":min(avgQual)})\n        res.update({\"maxQual\":max(avgQual)})\n        res.update({\"avgQual\":average(avgQual)})\n        res.update({\"readsQ30\":readsQual30})\n        res.update({\"percQ30Reads\":q30Reads})\n        res.update({\"Mbases\":Mbases})\n    else:\n        res.update({\"countReads\":0})\n        res.update({\"minLengthReads\":0})\n        res.update({\"maxLengthReads\":0})\n        res.update({\"avgLengthReads\":0})\n        res.update({\"minQual\":0})\n        res.update({\"maxQual\":0})\n        res.update({\"avgQual\":0})\n        res.update({\"readsQ30\":0})\n        res.update({\"percQ30Reads\":0})\n        res.update({\"Mbases\":0})\n\n    return res\n\n\n# Class Definition\nclass SampleRaw:\n\n    def __init__(self, read1, read2):\n        # pool = mp.Pool(processes=2)\n        print(\"Raw reads:\")\n        # fastqInfo = pool.map(readsInfo,[read1,read2])\n        # self.R1 = fastqInfo[0]\n        # self.R2 = fastqInfo[1]\n        print(\"Processing R1...\")\n        self.R1 = readsInfo(read1)\n        print(\"Processing R2...\")\n        self.R2 = readsInfo(read2)\n        self.warnings = []\n        self.fails = []\n\n    def get_R1(self):\n        return self.R1\n\n    def get_R2(self):\n        return self.R2\n\n    def get_warnings(self):\n        return self.warnings\n\n    def get_fails(self):\n        return self.fails\n\n    def getNR(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        return totReads\n\n    def get_q30Reads(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        totQ30 = self.R1.get(\"readsQ30\") + self.R2.get(\"readsQ30\")\n        if float(totReads) == 0:\n            return 0\n        return round(totQ30 / float(totReads) * 100, 2)\n\n    def get_stats(self):\n        minLength = min(self.R1.get(\"minLengthReads\"), self.R2.get(\"minLengthReads\"))\n        maxLength = min(self.R1.get(\"maxLengthReads\"), self.R2.get(\"maxLengthReads\"))\n        avgLength = round((self.R1.get(\"avgLengthReads\") + self.R2.get(\"avgLengthReads\")) / 2, 2)\n        minQual = min(self.R1.get(\"minQual\"), self.R2.get(\"minQual\"))\n        maxQual = min(self.R1.get(\"maxQual\"), self.R2.get(\"maxQual\"))\n        avgQual = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        totMbases = self.R1.get(\"Mbases\") + self.R2.get(\"Mbases\")\n        return [self.getNR(), totMbases, minLength, maxLength, avgLength, self.get_q30Reads(), minQual, maxQual,\n                avgQual]\n\n    def checkPair(self):\n        if self.R1.get(\"countReads\") != self.R2.get(\"countReads\"):\n            self.fails.append(\"FAIL_raw_pairNR\")\n\n    def checkQ30(self):\n        q30R = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(q30R) >= 25:\n            return\n        elif float(q30R) >= 5 and float(q30R) < 25:\n            self.warnings.append(\"WARN_rawQ30\")\n        else:\n            self.fails.append(\"FAIL_rawQ30\")\n\n    def checkQavg(self):\n        avgQualR = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(avgQualR) >= 22:\n            return\n        elif float(avgQualR) >= 16 and float(avgQualR) < 22:\n            self.warnings.append(\"WARN_rawQavg\")\n        else:\n            self.fails.append(\"FAIL_rawQavg\")\n\n    def checkNR_Bact(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 350000:\n            return\n        elif float(readsTot) >= 250000 and float(readsTot) < 350000:\n            self.warnings.append(\"WARN_rawNR_bact\")\n        else:\n            self.fails.append(\"FAIL_rawNR_bact\")\n\n    def checkNR_Vir(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 100000:\n            return\n        elif float(readsTot) >= 500000 and float(readsTot) < 100000:\n            self.warnings.append(\"WARN_rawNR_vir\")\n        else:\n            self.fails.append(\"FAIL_rawNR_vir\")\n\n\nclass SampleTrimmed:\n\n    def __init__(self, read1, read2, unp):\n        # pool = mp.Pool(processes=3)\n        print(\"Trimmed reads:\")\n        # fastqInfo = pool.map(readsInfo,[read1,read2,unp])\n        # self.R1 = fastqInfo[0]\n        # self.R2 = fastqInfo[1]\n        # self.U = fastqInfo[2]\n        print(\"Processing R1...\")\n        self.R1 = readsInfo(read1)\n        print(\"Processing R2...\")\n        self.R2 = readsInfo(read2)\n        print(\"Processing Unpaired...\")\n        self.U = readsInfo(unp)\n        self.warnings = []\n        self.fails = []\n\n    def get_R1(self):\n        return self.R1\n\n    def get_R2(self):\n        return self.R2\n\n    def get_unpaired(self):\n        return self.U\n\n    def get_warnings(self):\n        return self.warnings\n\n    def get_fails(self):\n        return self.fails\n\n    def getNR_total(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\") + self.U.get(\"countReads\")\n        return totReads\n\n    def getNR(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        return totReads\n\n    def getNR_unpaired(self):\n        return self.U.get(\"countReads\")\n\n    def get_q30Reads(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        totQ30 = self.R1.get(\"readsQ30\") + self.R2.get(\"readsQ30\")\n        if float(totReads) == 0:\n            return 0\n        return round(totQ30 / float(totReads) * 100, 2)\n\n    def get_stats(self):\n        minLength = min(self.R1.get(\"minLengthReads\"), self.R2.get(\"minLengthReads\"))\n        maxLength = min(self.R1.get(\"maxLengthReads\"), self.R2.get(\"maxLengthReads\"))\n        avgLength = round((self.R1.get(\"avgLengthReads\") + self.R2.get(\"avgLengthReads\")) / 2, 2)\n        minQual = min(self.R1.get(\"minQual\"), self.R2.get(\"minQual\"))\n        maxQual = min(self.R1.get(\"maxQual\"), self.R2.get(\"maxQual\"))\n        avgQual = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        totMbases = self.R1.get(\"Mbases\") + self.R2.get(\"Mbases\")\n        return [self.getNR_total(), self.getNR_unpaired(), self.getNR(), totMbases, minLength, maxLength, avgLength,\n                self.get_q30Reads(), minQual, maxQual, avgQual]\n\n    def checkPair(self):\n        if self.R1.get(\"countReads\") != self.R2.get(\"countReads\"):\n            self.fails.append(\"FAIL_trim_pairNR\")\n\n    def checkQ30(self):\n        q30R = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(q30R) >= 35:\n            return\n        elif float(q30R) >= 10 and float(q30R) < 35:\n            self.warnings.append(\"WARN_trimQ30\")\n        else:\n            self.fails.append(\"FAIL_trimQ30\")\n\n    def checkQavg(self):\n        avgQualR = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(avgQualR) >= 24:\n            return\n        elif float(avgQualR) >= 18 and float(avgQualR) < 24:\n            self.warnings.append(\"WARN_trimQavg\")\n        else:\n            self.fails.append(\"FAIL_trimQavg\")\n\n    def checkNR_Bact(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 300000:\n            return\n        elif float(readsTot) >= 50000 and float(readsTot) < 300000:\n            self.warnings.append(\"WARN_trimNR_bact\")\n        else:\n            self.fails.append(\"FAIL_trimNR_bact\")\n\n    def checkNR_Vir(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 90000:\n            return\n        elif float(readsTot) >= 40000 and float(readsTot) < 90000:\n            self.warnings.append(\"WARN_trimNR_vir\")\n        else:\n            self.fails.append(\"FAIL_trimNR_vir\")\n\n\nclass Sample:\n\n    def __init__(self, sampleName, R1, R2, T1, T2, Unp):\n        self.name = sampleName\n        self.Raw = SampleRaw(R1, R2)\n        self.Trimmed = SampleTrimmed(T1, T2, Unp)\n        self.warnings = []\n        self.fails = []\n\n    def get_name(self):\n        return self.name\n\n    def raw_check(self):\n        self.Raw.checkPair()\n        self.Raw.checkQ30()\n        self.Raw.checkQavg()\n        self.Raw.checkNR_Bact()\n        self.Raw.checkNR_Vir()\n        with open(self.get_name() + \"_SRC_raw.csv\", 'w') as checkOut:\n            results = \",\".join(str(x) for x in self.Raw.get_stats())\n            checkOut.write(\n                \"#Sample_name,Total_reads,Total_Mbases,Min_length,Max_length,Mean_length,Q30_reads,Min_avgQual,Max_avgQual,Mean_avgQual\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \"\\n\")\n        return\n\n    def raw_esito(self):\n        resWarn = \":\".join(self.Raw.get_warnings())\n        resFail = \":\".join(self.Raw.get_fails())\n        with open(self.get_name() + \"_SRC_raw.check\", 'w') as resOut:\n            resOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n            resOut.write(self.get_name() + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n    def trimm_check(self):\n        self.Trimmed.checkPair()\n        self.Trimmed.checkQ30()\n        self.Trimmed.checkQavg()\n        self.Trimmed.checkNR_Bact()\n        self.Trimmed.checkNR_Vir()\n        with open(self.get_name() + \"_SRC_treads.csv\", 'w') as checkOut:\n            results = \",\".join(str(x) for x in self.Trimmed.get_stats())\n            checkOut.write(\n                \"#Sample_name,Total_reads,Unpaired_reads,Paired_reads,Paired_Mbases,Min_PairedLength,Max_PairedLength,Mean_PairedLength,Q30_PairedReads,Min_PairedAvgQual,Max_PairedAvgQual,Mean_PairedAvgQual\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \"\\n\")\n        return\n\n    def trimm_esito(self):\n        resWarn = \":\".join(self.Trimmed.get_warnings())\n        resFail = \":\".join(self.Trimmed.get_fails())\n        with open(self.get_name() + \"_SRC_treads.check\", 'w') as resOut:\n            resOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n            resOut.write(self.get_name() + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n    def trimDiscarded(self):\n        discarded = self.Raw.getNR() - self.Trimmed.getNR_total()\n        return discarded\n\n    def makeReport(self):\n        with open(self.get_name() + \"_readsCheck.csv\", 'w') as checkOut:\n            resRaw = self.Raw.get_stats()\n            resTrim = self.Trimmed.get_stats()\n            resWarn = \":\".join(self.Raw.get_warnings() + self.Trimmed.get_warnings())\n            resFail = \":\".join(self.Raw.get_fails() + self.Trimmed.get_fails())\n            results = \",\".join(str(x) for x in\n                               [resRaw[0], resRaw[1], resRaw[4], resRaw[5], resRaw[8], resTrim[0], self.trimDiscarded(),\n                                resTrim[1], resTrim[2], resTrim[3], resTrim[6], resTrim[7], resTrim[10], 0])\n            checkOut.write(\n                \"#Sample_name,Total_rawReads,Total_rawMbases,Mean_rawLength,Q30_rawReads,Mean_rawAvgQual,Total_trimReads,trimDiscarded,trimUnpaired,trimPaired,trimPairedMbases,Mean_trimPairedLength,Q30_trimPairedReads,Mean_trimPairedAvgQual,Overlap_trimPaired,WARNING,FAIL\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n\n# MAIN\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='FASTQ Report')\n    parser.add_argument('-n', '--name', help='Sample name', type=str, required=True)\n    parser.add_argument('-R1', '--Read1', help='Fastq R1', type=str, required=True)\n    parser.add_argument('-R2', '--Read2', help='Fastq R2', type=str, required=True)\n    parser.add_argument('-T1', '--Treads1', help='Fastq trimmed R1', type=str, required=True)\n    parser.add_argument('-T2', '--Treads2', help='Fastq trimmed R2', type=str, required=True)\n    parser.add_argument('-U', '--Unpaired', help='Fastq unpaired', type=str, required=True)\n    args = parser.parse_args()\n\n    sample = Sample(args.name, args.Read1, args.Read2, args.Treads1, args.Treads2, args.Unpaired)\n\n    # RAW CHECK\n    print(\"Raw reads CHECK\")\n    sample.raw_check()\n    print(\"Raw reads REPORT\")\n    sample.raw_esito()\n\n    # TRIMMED CHECK\n    print(\"Trimmed reads CHECK\")\n    sample.trimm_check()\n    print(\"Trimmed reads REPORT\")\n    sample.trimm_esito()\n\n    # CREATE REPORT\n    print(\"Create final report\")\n    sample.makeReport()\n\n    print(\"DONE\")\n\n\n"}
{"file_name": "create_import.py", "file_path": "/scripts/step_3TX_class__kraken2/create_import.py", "language": "python", "id": "create_import", "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\n\n\ndef get_info(data, num):\n    try:\n        d = data[num]\n        d_info = d.strip().split(\"\\t\")\n        return d_info[0] + \",\" + d_info[6] + \",\" + d_info[5]\n    except:\n        return \"null,null,null\"\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Create taxa file for import')\n    parser.add_argument('-ds', '--ds', help='DS sample', type=str, required=True)\n    parser.add_argument('-o', '--outfile', help='Outfile name', type=str, required=True)\n    parser.add_argument('-g', '--genus', help='Braken genus', type=str, required=True)\n    parser.add_argument('-sp', '--species', help='Braken species', type=str, required=True)\n    parser.add_argument('-k', '--kraken', help='Kraken report', type=str, required=True)\n    args = parser.parse_args()\n\n    if os.path.exists(args.genus) and os.path.exists(args.species) and os.path.exists(args.kraken):\n        info = []\n        with open(args.outfile, 'w') as out:\n            out.write(\n                \"SAMPLE,UNCLASS_PC_READS,UNCLASS_NUM_READS,FIRSTSPE_TAXA,FIRSTSPE_PC_READS,FIRSTSPE_NUM_READS,\"\n                \"FIRSTSPE_COVERAGE,GEN1RANK_TAXA,GEN1RANK_PC_READS,GEN1RANK_NUM_READS,GEN2RANK_TAXA,\"\n                \"GEN2RANK_PC_READS,GEN2RANK_NUM_READS,GEN3RANK_TAXA,GEN3RANK_PC_READS,GEN3RANK_NUM_READS,\"\n                \"SPE2RANK_TAXA,SPE2RANK_PC_READS,SPE2RANK_NUM_READS,SPE3RANK_TAXA,SPE3RANK_PC_READS,\"\n                \"SPE3RANK_NUM_READS,RAW_COVERAGE,TRIMMED_COVERAGE\\n\") \n            info.append(args.ds.replace(\"DS\", \"\"))\n            # UNCL\n            uncl = open(args.kraken).readlines()[0]\n            uncl_info = uncl.split(\"\\t\")\n            info.append(uncl_info[0])\n            info.append(uncl_info[1])\n            # SPECIES + GENUS\n            genus = open(args.genus).readlines()\n            species = open(args.species).readlines()\n            info.append(get_info(species, 1))\n            # Set species coverage to null\n            info.append(\"null\")\n            info.append(get_info(genus, 1))\n            info.append(get_info(genus, 2))\n            info.append(get_info(genus, 3))\n            info.append(get_info(species, 2))\n            info.append(get_info(species, 3))\n            # Set coverage to null\n            info.append(\"null,null\\n\")\n            out.write(\",\".join(info))\n    else:\n        print(\"ERROR: input file not exists\")\n"}
{"file_name": "coverage_minmax.py", "file_path": "/scripts/step_2AS_mapping__ivar/coverage_minmax.py", "language": "python", "id": "coverage_minmax", "content": "#!/usr/bin/env python3\nimport sys\n\n\ndef create_coverage_report(sample, ds, samtools_view_output, samtools_depth_output, import_file_result):\n    v_file = open(samtools_view_output, \"r\")\n    d_file = open(samtools_depth_output, \"r\")\n    num_mapped_reads = v_file.readlines()[0].strip()\n    # get [min, max] vcov   \n    if num_mapped_reads == '0':\n        min_vcov = 0\n        max_vcov = 0\n    else:\n        min_vcov = sys.maxsize\n        max_vcov = -1\n        for line in d_file.readlines():\n            split_line = line.strip().split(\"\\t\")\n            if len(split_line) > 2:\n                value = int(split_line[2])\n                if value > max_vcov:\n                    max_vcov = value\n                if value < min_vcov:\n                    min_vcov = value    \n        if min_vcov == sys.maxsize:\n            min_vcov = 0\n        if max_vcov == -1:\n            min_vcov = 0\n    result = open(import_file_result, 'w')\n    result.write(\"CMP_ID,SAMPLE_DS,NUM_MAPPED_READS,MIN_VCOV,MAX_VCOV\\n\")\n    result.write(\"{},{},{},{},{}\\n\".format(sample, ds.replace(\"DS\", \"\"), num_mapped_reads, min_vcov, max_vcov))\n    result.close()\n    v_file.close()\n    d_file.close()\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 6:\n        sys.stderr.write(\n            \"Usage: %s <sample> <ds> <samtools_view_out> <samtools_depth_out> <import_file_result>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n"}
{"file_name": "coverage_plot.py", "file_path": "/scripts/step_2AS_mapping__ivar/coverage_plot.py", "language": "python", "id": "coverage_plot", "content": "#!/usr/bin/env python3\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl \n\nmpl.rcParams['agg.path.chunksize'] = 10000\n\ndef plotCov(depthFile,plotFile):\n\tx = []\n\ty = []\n\n\ttable = open(depthFile, 'r')\n\tfor tline in table:\n\t\tline = tline.split(\"\\t\")\n\t\tx.append(int(line[1]))\n\t\ty.append(int(line[2]))\n\tavg = np.average(y)\n\tplt.plot(x, y, 'k')\n\tplt.xlabel('Position in Genome')\n\tplt.ylabel('Depth of Coverage')\n\tplt.hlines(avg, 0, 30000, color=\"red\", linestyles='solid')\n\tplt.text(30000,avg, \"AVG depth\",color=\"red\",ha=\"left\", va=\"center\")\n\t# plt.show()\n\tplt.savefig(plotFile,dpi= 1000)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        sys.stderr.write(\n            \"Usage: %s <samtools_depth_out> <plot_name>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        plotCov(sys.argv[1], sys.argv[2])"}
{"file_name": "coverage.py", "file_path": "/scripts/step_2AS_mapping__ivar/coverage.py", "language": "python", "id": "coverage", "content": "#!/usr/bin/env python3\n\nimport sys\nfrom Bio import SeqIO\nfrom Bio.Alphabet import IUPAC\n\n\ndef create_coverage_report(coverage_file, consensus_file, reference, cmp, ds, output_file, coverage_import_file):\n    cov = open(coverage_file, \"r\").readlines()\n\n    with open(output_file, 'w') as check, open(coverage_import_file, 'w') as cov_import:\n        check.write(\"#Mapping\\nReference\\tCov\\tHCov\\n\")\n        check.write(reference + \"\\t\" + str(cov[0].rstrip()) + \"\\t\" + str(cov[1].rstrip()) + \"\\n\")\n\n        n_number = 0\n        iupac_number = 0\n        seq_length = 0\n\n        consensus = SeqIO.index(consensus_file, \"fasta\")\n        for seqID in consensus.keys():\n            seq = consensus[seqID].seq.upper()\n            seq_length = len(seq)\n            check.write(\"\\n#Consensus\\nTotal_Length: \" + str(seq_length) + \"\\n\")\n            check.write(\"\\nIUPAC\\tCOUNT\\tPERC\\n\")\n            for letter in IUPAC.ambiguous_dna.letters:\n                if letter not in IUPAC.unambiguous_dna.letters:\n                    count = seq.count(letter)\n                    if seq_length != 0:\n                        perc = round(float(seq.count(letter)) / float(seq_length) * 100, 3)\n                    else:\n                        perc = 0\n                    check.write(letter + \"\\t\" + str(count) + \"\\t\" + str(perc) + \"\\n\")\n                    if letter == 'N':\n                        n_number = count\n                    else:\n                        iupac_number += count\n            break  # considering only first key!\n\n        perc_ns = round(n_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n        perc_iupac = round(iupac_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n\n        cov_import.write(\"CMP_ID,SAMPLE_DS,COV,H_COV,NOTE,PERC_IUPAC,PERC_NS,CONSENSUS_LENGTH\\n\")\n        cov_import.write(\n            \"{},{},{},{},mapping on {},{},{},{}\\n\".format(cmp, ds, cov[0].rstrip(), cov[1].rstrip(), reference, perc_iupac,\n                                                       perc_ns,seq_length))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 8:\n        sys.stderr.write(\n            \"Usage: %s <coverage_file> <consensus_file> <reference> <cmp> <ds> <output_file> \"\n            \"<output_file_import_coverage>\" % (\n                sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5], sys.argv[6],\n                               sys.argv[7])\n"}
{"file_name": "process-mlst-result.py", "file_path": "/scripts/step_4TY_flaA__flaA/process-mlst-result.py", "language": "python", "id": "process-mlst-result", "content": "#!/usr/bin/env python3\n\nimport string\nimport sys\nimport os\n\n\ndef add_cc(input_file, output_file, mlst_db_folder):\n    summary = open(input_file, \"r\").readlines()\n    complete = open(output_file, \"w\")\n    complete.write(\"DS;GENPAT;SCHEMA;CC;ST;loci(varianti)\\n\")\n    for riga in summary:\n        sample = riga.split(\"\\t\")[0]\n        ds = sample.split(\"DS\")[1].split(\"-\")[0]\n        genpat = sample.split(\"/\")[-1].split(\"_\")[1]\n        schema = riga.split(\"\\t\")[1]\n        st = riga.split(\"\\t\")[2].strip()\n        loci = \" \".join(riga.split(\"\\t\")[3:])\n        if schema == '-':\n            complete.write(\"%s;%s;%s;;;%s\" % (ds, genpat, schema, loci))\n            break;           \n        try:\n            schema_file = open(mlst_db_folder + \"/\" + schema + \"/\" + schema + \".txt\", \"r\").readlines()\n            header = schema_file[0]\n            cc = \"\"\n            if \"Lineage\" in header:\n                for combination in schema_file[1:]:\n                    if combination.split(\"\\t\")[0].strip() == st:\n                        cc = combination.split(\"\\t\")[-2].strip()\n            else:\n                for combination in schema_file[1:]:\n                    if combination.split(\"\\t\")[0].strip() == st:\n                        cc = combination.split(\"\\t\")[-1].strip()\n            complete.write(\"%s;%s;%s;%s;%s;%s\" % (ds, genpat, schema, cc, st, loci))\n        except Exception as ex:\n            print(\"exception :\" + str(ex))\n            complete.write(\"%s;%s;%s;;%s;%s\" % (ds, genpat, schema, st, loci))\n            with open(os.path.splitext(output_file)[0] + \"_exception.log\", 'aw') as log:\n                log.write(\n                    \"Error while looking for clonal complex in: %s/%s/%s.txt \" % (mlst_db_folder, schema, schema))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 4:\n        sys.stderr.write(\"Usage: %s <input_file> <base_name> <db_folder>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        add_cc(sys.argv[1], sys.argv[2], sys.argv[3])\n"}
{"file_name": "coverage_minmax.py", "file_path": "/scripts/step_2AS_mapping__bowtie/coverage_minmax.py", "language": "python", "id": "coverage_minmax", "content": "#!/usr/bin/env python3\nimport sys\n\n\ndef create_coverage_report(sample, ds, samtools_view_output, samtools_depth_output, import_file_result):\n    v_file = open(samtools_view_output, \"r\")\n    d_file = open(samtools_depth_output, \"r\")\n    num_mapped_reads = v_file.readlines()[0].strip()\n    # get [min, max] vcov   \n    if num_mapped_reads == '0':\n        min_vcov = 0\n        max_vcov = 0\n    else:\n        min_vcov = sys.maxsize\n        max_vcov = -1\n        for line in d_file.readlines():\n            split_line = line.strip().split(\"\\t\")\n            if len(split_line) > 2:\n                value = int(split_line[2])\n                if value > max_vcov:\n                    max_vcov = value\n                if value < min_vcov:\n                    min_vcov = value    \n        if min_vcov == sys.maxsize:\n            min_vcov = 0\n        if max_vcov == -1:\n            min_vcov = 0\n    result = open(import_file_result, 'w')\n    result.write(\"CMP_ID,SAMPLE_DS,NUM_MAPPED_READS,MIN_VCOV,MAX_VCOV\\n\")\n    result.write(\"{},{},{},{},{}\\n\".format(sample, ds.replace(\"DS\", \"\"), num_mapped_reads, min_vcov, max_vcov))\n    result.close()\n    v_file.close()\n    d_file.close()\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 6:\n        sys.stderr.write(\n            \"Usage: %s <sample> <ds> <samtools_view_out> <samtools_depth_out> <import_file_result>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n"}
{"file_name": "coverage_plot.py", "file_path": "/scripts/step_2AS_mapping__bowtie/coverage_plot.py", "language": "python", "id": "coverage_plot", "content": "#!/usr/bin/env python3\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl \n\nmpl.rcParams['agg.path.chunksize'] = 10000\n\ndef plotCov(depthFile,plotFile):\n\tx = []\n\ty = []\n\n\ttable = open(depthFile, 'r')\n\tfor tline in table:\n\t\tline = tline.split(\"\\t\")\n\t\tx.append(int(line[1]))\n\t\ty.append(int(line[2]))\n\tavg = np.average(y)\n\tplt.plot(x, y, 'k')\n\tplt.xlabel('Position in Genome')\n\tplt.ylabel('Depth of Coverage')\n\tplt.hlines(avg, 0, 30000, color=\"red\", linestyles='solid')\n\tplt.text(30000,avg, \"AVG depth\",color=\"red\",ha=\"left\", va=\"center\")\n\t# plt.show()\n\tplt.savefig(plotFile,dpi= 1000)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        sys.stderr.write(\n            \"Usage: %s <samtools_depth_out> <plot_name>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        plotCov(sys.argv[1], sys.argv[2])"}
{"file_name": "coverage.py", "file_path": "/scripts/step_2AS_mapping__bowtie/coverage.py", "language": "python", "id": "coverage", "content": "#!/usr/bin/env python3\n\nimport sys\nfrom Bio import SeqIO\nfrom Bio.Alphabet import IUPAC\n\n\ndef create_coverage_report(coverage_file, consensus_file, reference, cmp, ds, output_file, coverage_import_file):\n    cov = open(coverage_file, \"r\").readlines()\n\n    with open(output_file, 'w') as check, open(coverage_import_file, 'w') as cov_import:\n        check.write(\"#Mapping\\nReference\\tCov\\tHCov\\n\")\n        check.write(reference + \"\\t\" + str(cov[0].rstrip()) + \"\\t\" + str(cov[1].rstrip()) + \"\\n\")\n\n        n_number = 0\n        iupac_number = 0\n        seq_length = 0\n\n        consensus = SeqIO.index(consensus_file, \"fasta\")\n        for seqID in consensus.keys():\n            seq = consensus[seqID].seq.upper()\n            seq_length = len(seq)\n            check.write(\"\\n#Consensus\\nTotal_Length: \" + str(seq_length) + \"\\n\")\n            check.write(\"\\nIUPAC\\tCOUNT\\tPERC\\n\")\n            for letter in IUPAC.ambiguous_dna.letters:\n                if letter not in IUPAC.unambiguous_dna.letters:\n                    count = seq.count(letter)\n                    if seq_length != 0:\n                        perc = round(float(seq.count(letter)) / float(seq_length) * 100, 3)\n                    else:\n                        perc = 0\n                    check.write(letter + \"\\t\" + str(count) + \"\\t\" + str(perc) + \"\\n\")\n                    if letter == 'N':\n                        n_number = count\n                    else:\n                        iupac_number += count\n            break  # considering only first key!\n\n        perc_ns = round(n_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n        perc_iupac = round(iupac_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n\n        cov_import.write(\"CMP_ID,SAMPLE_DS,COV,H_COV,NOTE,PERC_IUPAC,PERC_NS,CONSENSUS_LENGTH\\n\")\n        cov_import.write(\n            \"{},{},{},{},mapping on {},{},{},{}\\n\".format(cmp, ds, cov[0].rstrip(), cov[1].rstrip(), reference, perc_iupac,\n                                                       perc_ns,seq_length))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 8:\n        sys.stderr.write(\n            \"Usage: %s <coverage_file> <consensus_file> <reference> <cmp> <ds> <output_file> \"\n            \"<output_file_import_coverage>\" % (\n                sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5], sys.argv[6],\n                               sys.argv[7])\n"}
{"file_name": "create_import.py", "file_path": "/scripts/step_3TX_class__centrifuge/create_import.py", "language": "python", "id": "create_import", "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\n\n\ndef get_info(data, num):\n    try:\n        d = data[num]\n        d_info = d.strip().split(\"\\t\")\n        return d_info[0] + \",\" + d_info[6] + \",\" + d_info[5]\n    except:\n        return \"null,null,null\"\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Create taxa file for import')\n    parser.add_argument('-ds', '--ds', help='DS sample', type=str, required=True)\n    parser.add_argument('-o', '--outfile', help='Outfile name', type=str, required=True)\n    parser.add_argument('-g', '--genus', help='Braken genus', type=str, required=True)\n    parser.add_argument('-sp', '--species', help='Braken species', type=str, required=True)\n    parser.add_argument('-k', '--kraken', help='Kraken report', type=str, required=True)\n    args = parser.parse_args()\n\n    if os.path.exists(args.genus) and os.path.exists(args.species) and os.path.exists(args.kraken):\n        info = []\n        with open(args.outfile, 'w') as out:\n            out.write(\n                \"SAMPLE,UNCLASS_PC_READS,UNCLASS_NUM_READS,FIRSTSPE_TAXA,FIRSTSPE_PC_READS,FIRSTSPE_NUM_READS,\"\n                \"FIRSTSPE_COVERAGE,GEN1RANK_TAXA,GEN1RANK_PC_READS,GEN1RANK_NUM_READS,GEN2RANK_TAXA,\"\n                \"GEN2RANK_PC_READS,GEN2RANK_NUM_READS,GEN3RANK_TAXA,GEN3RANK_PC_READS,GEN3RANK_NUM_READS,\"\n                \"SPE2RANK_TAXA,SPE2RANK_PC_READS,SPE2RANK_NUM_READS,SPE3RANK_TAXA,SPE3RANK_PC_READS,\"\n                \"SPE3RANK_NUM_READS,RAW_COVERAGE,TRIMMED_COVERAGE\\n\") \n            info.append(args.ds.replace(\"DS\", \"\"))\n            # UNCL\n            uncl = open(args.kraken).readlines()[0]\n            uncl_info = uncl.split(\"\\t\")\n            info.append(uncl_info[0])\n            info.append(uncl_info[1])\n            # SPECIES + GENUS\n            genus = open(args.genus).readlines()\n            species = open(args.species).readlines()\n            info.append(get_info(species, 1))\n            # Set species coverage to null\n            info.append(\"null\")\n            info.append(get_info(genus, 1))\n            info.append(get_info(genus, 2))\n            info.append(get_info(genus, 3))\n            info.append(get_info(species, 2))\n            info.append(get_info(species, 3))\n            # Set coverage to null\n            info.append(\"null,null\\n\")\n            out.write(\",\".join(info))\n    else:\n        print(\"ERROR: input file not exists\")\n"}
{"file_name": "chewieCheck.py", "file_path": "/scripts/step_4TY_cgMLST__chewbbaca/chewieCheck.py", "language": "python", "id": "chewieCheck", "content": "#!/usr/bin/env python3\n\nimport os\nimport argparse\n\nif __name__ == \"__main__\" :\n\tparser = argparse.ArgumentParser(description='chewbbaca')\n\t# parser.add_argument('--outDir',help='output folder',type=str, required=True)\n\tparser.add_argument('--stat',type=str,help=\"stats file chewbbaca\",required=True)\n\targs = parser.parse_args() \n\t# if args.outDir==\".\":\n\t# \toutdir = os.getcwd()\n\t# else:\n\t# \toutdir = args.outDir\n\tif os.path.exists(args.stat):\n\t\ts = open(args.stat,'r')\n\t\tdata = s.readlines()[1]\n\t\theader = \"genome,calledPerc,calledNum,annotated,new,notFound,discarded\"\n\t\tg,exc,inf,lnf,plot,niph,alm,asm = data.split(\"\\t\")\n\t\ttotal = int(exc)+int(inf)+int(lnf)+int(plot)+int(niph)+int(alm)+int(asm)\n\t\tperc = round((float(exc)+float(inf))/total,3)\n\t\tcalled = int(exc)+int(inf)\n\t\tdisc = int(plot)+int(niph)+int(alm)+int(asm)\n\t\tdata_out = g+\",\"+str(perc)+\",\"+str(called)+\",\"+exc+\",\"+inf+\",\"+lnf+\",\"+str(disc)\n\t\tprint(header)\n\t\tprint(data_out)\n\telse:\n\t\tprint(\"Statistics file not found \"+args.stat)\n\t\texit(2)"}
{"file_name": "AssemblyFilterPlasmids.py", "file_path": "/scripts/step_2AS_denovo__plasmidspades/AssemblyFilterPlasmids.py", "language": "python", "id": "AssemblyFilterPlasmids", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nimport argparse\n\n\ndef filter_seq(output_name, check_name, scaffolds, min_length, min_cov):\n    filtered_sequences = []\n    with (open(scaffolds, 'r') as input_handle,\n          open(output_name, \"w\") as output_handle,\n          open(check_name, \"w\") as check_handle):\n        check_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n        for record in SeqIO.parse(input_handle, \"fasta\"):\n            head_seq = record.id\n            coverage = float(head_seq.split('_')[5])\n            if len(record.seq) >= min_length and coverage >= min_cov:\n                filtered_sequences.append(record)\n                check_handle.write(head_seq + \"\\tPASS\\n\")\n            else:\n                check_handle.write(head_seq + \"\\tFAILED\\n\")\n\n        SeqIO.write(filtered_sequences, output_handle, \"fasta\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description='FASTQ Report')\n    parser.add_argument('-o', '--out', help='Output file name', type=str, required=True)\n    parser.add_argument('-oc', '--out_check', help='Output check file name', type=str, required=True)\n    parser.add_argument('-f', '--fasta', help='Scaffolds fasta', type=str, required=True)\n    parser.add_argument('-l', '--length', help='Min length', type=int, required=True)\n    parser.add_argument('-c', '--cov', help='Min kmerCov ', type=int, required=True)\n    args = parser.parse_args()\n\n    filter_seq(args.out, args.out_check, args.fasta, args.length, args.cov)\n"}
{"file_name": "coverage_minmax.py", "file_path": "/scripts/step_2AS_mapping__medaka/coverage_minmax.py", "language": "python", "id": "coverage_minmax", "content": "#!/usr/bin/env python3\nimport sys\n\n\ndef create_coverage_report(sample, ds, samtools_view_output, samtools_depth_output, import_file_result):\n    v_file = open(samtools_view_output, \"r\")\n    d_file = open(samtools_depth_output, \"r\")\n    num_mapped_reads = v_file.readlines()[0].strip()\n    # get [min, max] vcov   \n    if num_mapped_reads == '0':\n        min_vcov = 0\n        max_vcov = 0\n    else:\n        min_vcov = sys.maxsize\n        max_vcov = -1\n        for line in d_file.readlines():\n            split_line = line.strip().split(\"\\t\")\n            if len(split_line) > 2:\n                value = int(split_line[2])\n                if value > max_vcov:\n                    max_vcov = value\n                if value < min_vcov:\n                    min_vcov = value    \n        if min_vcov == sys.maxsize:\n            min_vcov = 0\n        if max_vcov == -1:\n            min_vcov = 0\n    result = open(import_file_result, 'w')\n    result.write(\"CMP_ID,SAMPLE_DS,NUM_MAPPED_READS,MIN_VCOV,MAX_VCOV\\n\")\n    result.write(\"{},{},{},{},{}\\n\".format(sample, ds.replace(\"DS\", \"\"), num_mapped_reads, min_vcov, max_vcov))\n    result.close()\n    v_file.close()\n    d_file.close()\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 6:\n        sys.stderr.write(\n            \"Usage: %s <sample> <ds> <samtools_view_out> <samtools_depth_out> <import_file_result>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5])\n"}
{"file_name": "coverage_plot.py", "file_path": "/scripts/step_2AS_mapping__medaka/coverage_plot.py", "language": "python", "id": "coverage_plot", "content": "#!/usr/bin/env python3\nimport sys\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib as mpl \n\nmpl.rcParams['agg.path.chunksize'] = 10000\n\ndef plotCov(depthFile,plotFile):\n\tx = []\n\ty = []\n\n\ttable = open(depthFile, 'r')\n\tfor tline in table:\n\t\tline = tline.split(\"\\t\")\n\t\tx.append(int(line[1]))\n\t\ty.append(int(line[2]))\n\tavg = np.average(y)\n\tplt.plot(x, y, 'k')\n\tplt.xlabel('Position in Genome')\n\tplt.ylabel('Depth of Coverage')\n\tplt.hlines(avg, 0, 30000, color=\"red\", linestyles='solid')\n\tplt.text(30000,avg, \"AVG depth\",color=\"red\",ha=\"left\", va=\"center\")\n\t# plt.show()\n\tplt.savefig(plotFile,dpi= 1000)\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 2:\n        sys.stderr.write(\n            \"Usage: %s <samtools_depth_out> <plot_name>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        plotCov(sys.argv[1], sys.argv[2])"}
{"file_name": "coverage.py", "file_path": "/scripts/step_2AS_mapping__medaka/coverage.py", "language": "python", "id": "coverage", "content": "#!/usr/bin/env python3\n\nimport sys\nfrom Bio import SeqIO\nfrom Bio.Alphabet import IUPAC\n\n\ndef create_coverage_report(coverage_file, consensus_file, reference, cmp, ds, output_file, coverage_import_file):\n    cov = open(coverage_file, \"r\").readlines()\n\n    with open(output_file, 'w') as check, open(coverage_import_file, 'w') as cov_import:\n        check.write(\"#Mapping\\nReference\\tCov\\tHCov\\n\")\n        check.write(reference + \"\\t\" + str(cov[0].rstrip()) + \"\\t\" + str(cov[1].rstrip()) + \"\\n\")\n\n        n_number = 0\n        iupac_number = 0\n        seq_length = 0\n\n        consensus = SeqIO.index(consensus_file, \"fasta\")\n        for seqID in consensus.keys():\n            seq = consensus[seqID].seq.upper()\n            seq_length = len(seq)\n            check.write(\"\\n#Consensus\\nTotal_Length: \" + str(seq_length) + \"\\n\")\n            check.write(\"\\nIUPAC\\tCOUNT\\tPERC\\n\")\n            for letter in IUPAC.ambiguous_dna.letters:\n                if letter not in IUPAC.unambiguous_dna.letters:\n                    count = seq.count(letter)\n                    if seq_length != 0:\n                        perc = round(float(seq.count(letter)) / float(seq_length) * 100, 3)\n                    else:\n                        perc = 0\n                    check.write(letter + \"\\t\" + str(count) + \"\\t\" + str(perc) + \"\\n\")\n                    if letter == 'N':\n                        n_number = count\n                    else:\n                        iupac_number += count\n            break  # considering only first key!\n\n        perc_ns = round(n_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n        perc_iupac = round(iupac_number / float(seq_length) * 100, 3) if seq_length > 0 else 0\n\n        cov_import.write(\"CMP_ID,SAMPLE_DS,COV,H_COV,NOTE,PERC_IUPAC,PERC_NS,CONSENSUS_LENGTH\\n\")\n        cov_import.write(\n            \"{},{},{},{},mapping on {},{},{},{}\\n\".format(cmp, ds, cov[0].rstrip(), cov[1].rstrip(), reference, perc_iupac,\n                                                       perc_ns,seq_length))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 8:\n        sys.stderr.write(\n            \"Usage: %s <coverage_file> <consensus_file> <reference> <cmp> <ds> <output_file> \"\n            \"<output_file_import_coverage>\" % (\n                sys.argv[0]))\n        sys.exit(1)\n    else:\n        create_coverage_report(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4], sys.argv[5], sys.argv[6],\n                               sys.argv[7])\n"}
{"file_name": "create_import.py", "file_path": "/scripts/step_3TX_class__kraken/create_import.py", "language": "python", "id": "create_import", "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\n\n\ndef get_info(data, num):\n    try:\n        d = data[num]\n        d_info = d.strip().split(\"\\t\")\n        return d_info[0] + \",\" + d_info[6] + \",\" + d_info[5]\n    except:\n        return \"null,null,null\"\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Create taxa file for import')\n    parser.add_argument('-ds', '--ds', help='DS sample', type=str, required=True)\n    parser.add_argument('-o', '--outfile', help='Outfile name', type=str, required=True)\n    parser.add_argument('-g', '--genus', help='Braken genus', type=str, required=True)\n    parser.add_argument('-sp', '--species', help='Braken species', type=str, required=True)\n    parser.add_argument('-k', '--kraken', help='Kraken report', type=str, required=True)\n    args = parser.parse_args()\n\n    if os.path.exists(args.genus) and os.path.exists(args.species) and os.path.exists(args.kraken):\n        info = []\n        with open(args.outfile, 'w') as out:\n            out.write(\n                \"SAMPLE,UNCLASS_PC_READS,UNCLASS_NUM_READS,FIRSTSPE_TAXA,FIRSTSPE_PC_READS,FIRSTSPE_NUM_READS,\"\n                \"FIRSTSPE_COVERAGE,GEN1RANK_TAXA,GEN1RANK_PC_READS,GEN1RANK_NUM_READS,GEN2RANK_TAXA,\"\n                \"GEN2RANK_PC_READS,GEN2RANK_NUM_READS,GEN3RANK_TAXA,GEN3RANK_PC_READS,GEN3RANK_NUM_READS,\"\n                \"SPE2RANK_TAXA,SPE2RANK_PC_READS,SPE2RANK_NUM_READS,SPE3RANK_TAXA,SPE3RANK_PC_READS,\"\n                \"SPE3RANK_NUM_READS,RAW_COVERAGE,TRIMMED_COVERAGE\\n\") \n            info.append(args.ds.replace(\"DS\", \"\"))\n            # UNCL\n            uncl = open(args.kraken).readlines()[0]\n            uncl_info = uncl.split(\"\\t\")\n            info.append(uncl_info[0])\n            info.append(uncl_info[1])\n            # SPECIES + GENUS\n            genus = open(args.genus).readlines()\n            species = open(args.species).readlines()\n            info.append(get_info(species, 1))\n            # Set species coverage to null\n            info.append(\"null\")\n            info.append(get_info(genus, 1))\n            info.append(get_info(genus, 2))\n            info.append(get_info(genus, 3))\n            info.append(get_info(species, 2))\n            info.append(get_info(species, 3))\n            # Set coverage to null\n            info.append(\"null,null\\n\")\n            out.write(\",\".join(info))\n    else:\n        print(\"ERROR: input file not exists\")\n"}
{"file_name": "AssemblyFilter.py", "file_path": "/scripts/step_2MG_denovo__metaspades/AssemblyFilter.py", "language": "python", "id": "AssemblyFilter", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import median,average\nimport sys\nimport argparse\n\n \ndef filt_seq(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_spades_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_spades.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.id\n\t\tcoverage = float(headSeq.split('cov_')[1])\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\ndef filt_seq_unicycler(sampleName, scaffolds, minLength, minCov):\n\t\n\tfilt_sequences = []\n\tinput_handle=open(scaffolds,'r')\n\toutput_handle = open(sampleName+\"_unicycler_scaffolds_L200.fasta\", \"w\")\n\tcheck_handle = open(sampleName+\"_unicycler.check\", \"w\")\n\tcheck_handle.write(\"Nodes_ID\\tCHECK\\n\")\n\n\tfor record in SeqIO.parse(input_handle, \"fasta\"):\n\t\theadSeq = record.description\n\t\tprint(headSeq)\n\t\tcoverage = float(headSeq.split(' ')[2].lstrip(\"depth=\").strip(\"x\"))\n\t\tif len(record.seq) >= minLength and coverage >= minCov:\n\t\t\tfilt_sequences.append(record)\n\t\t\tcheck_handle.write(headSeq+\"\\tPASS\\n\")\n\t\telse:\n\t\t\tcheck_handle.write(headSeq+\"\\tFAILED\\n\")\n\t\n\tSeqIO.write(filt_sequences, output_handle, \"fasta\")\n\tinput_handle.close()\n\toutput_handle.close()\n\tcheck_handle.close()\n\treturn\n\nif __name__==\"__main__\":\n\t\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-f', '--fasta',help='Scaffolds fasta',type=str, required=True)\n\tparser.add_argument('-l', '--length',help='Min length',type=int, required=True)\n\tparser.add_argument('-c', '--cov',help='Min kmerCov ',type=int, required=True)\n\tparser.add_argument('-u', '--unicycler',help='Unicycler fasta',action='store_true')\n\targs = parser.parse_args()\n\t\n\tif not args.unicycler:\n\t\tfilt_seq(args.name,args.fasta,args.length,args.cov)\n\telse:\n\t\tfilt_seq_unicycler(args.name,args.fasta,args.length,args.cov)"}
{"file_name": "fasta2fastq.py", "file_path": "/scripts/step_1PP_generated__fasta2fastq/fasta2fastq.py", "language": "python", "id": "fasta2fastq", "content": "#!/usr/bin/env python3\n\nimport string \nimport os\nimport argparse\nfrom Bio.Seq import Seq\n\nif __name__ == \"__main__\" :\n\tparser = argparse.ArgumentParser(description='fasta2fastq')\n\tparser.add_argument('--fasta', help='input folder', type=str, required=True)\n\tparser.add_argument('--fastq', type=str, help=\"output folder\", required=True)\n\targs = parser.parse_args() \n\toutdir = args.fastq\n\tindir = args.fasta\n\tif os.path.exists(outdir) == False:\n\t\tos.system(\"mkdir \"+outdir)\n\n\tfor fastafile in os.listdir(indir):\n\t\tnomefastqR1 = fastafile.replace(\".fasta\",\"_R1.fastq\")\n\t\tR1 = open(outdir+\"/\"+nomefastqR1,\"w\")\n\t\tnomefastqR2 = fastafile.replace(\".fasta\",\"_R2.fastq\")\n\t\tR2 = open(outdir+\"/\"+nomefastqR2,\"w\")\n\t\tfasta = open(indir+\"/\"+fastafile).readlines()\n\t\tsequenza = \"\"\n\t\theader = fasta[0][1:].strip()\n\t\tfor riga in fasta[1:]:\n\t\t\tsequenza = sequenza + riga.strip()\n\t\tnumeri = range(0,len(sequenza))\n\t\tc = 2\n\t\tfor numero in numeri:\n\t\t\tif numero + 150 < len(sequenza):\n\t\t\t\tpezzo = sequenza[numero:numero+150]\n\t\t\telse:\n\t\t\t\t#print str(numero)+\":\"+str(len(sequenza))\n\t\t\t\tpezzo = sequenza[numero:len(sequenza)]\n\t\t\tmy_dna = Seq(pezzo)\n\t\t\tR1.write(\"@\"+header+\"-\"+str(c)+\"/1\\n\"+pezzo+\"\\n+\\n\"+(\"I\"*len(pezzo))+\"\\n\")\n\t\t\trc = my_dna.reverse_complement()\n\t\t\tR2.write(\"@\"+header+\"-\"+str(c)+\"/2\\n\"+str(rc)+\"\\n+\\n\"+(\"I\"*len(pezzo))+\"\\n\")\n\t\t\tc+=2\n\t\tR1.close()\n\t\tR2.close()"}
{"file_name": "chewieCheck.py", "file_path": "/scripts/step_4TY_wgMLST__chewbbaca/chewieCheck.py", "language": "python", "id": "chewieCheck", "content": "#!/usr/bin/env python3\n\nimport os\nimport argparse\n\nif __name__ == \"__main__\" :\n\tparser = argparse.ArgumentParser(description='chewbbaca')\n\t# parser.add_argument('--outDir',help='output folder',type=str, required=True)\n\tparser.add_argument('--stat',type=str,help=\"stats file chewbbaca\",required=True)\n\targs = parser.parse_args() \n\t# if args.outDir==\".\":\n\t# \toutdir = os.getcwd()\n\t# else:\n\t# \toutdir = args.outDir\n\tif os.path.exists(args.stat):\n\t\ts = open(args.stat,'r')\n\t\tdata = s.readlines()[1]\n\t\theader = \"genome,calledPerc,calledNum,annotated,new,notFound,discarded\"\n\t\tg,exc,inf,lnf,plot,niph,alm,asm = data.split(\"\\t\")\n\t\ttotal = int(exc)+int(inf)+int(lnf)+int(plot)+int(niph)+int(alm)+int(asm)\n\t\tperc = round((float(exc)+float(inf))/total,3)\n\t\tcalled = int(exc)+int(inf)\n\t\tdisc = int(plot)+int(niph)+int(alm)+int(asm)\n\t\tdata_out = g+\",\"+str(perc)+\",\"+str(called)+\",\"+exc+\",\"+inf+\",\"+lnf+\",\"+str(disc)\n\t\tprint(header)\n\t\tprint(data_out)\n\telse:\n\t\tprint(\"Statistics file not found \"+args.stat)\n\t\texit(2)"}
{"file_name": "select_lineage.py", "file_path": "/scripts/step_4TY_lineage__westnile/select_lineage.py", "language": "python", "id": "select_lineage", "content": "#!/usr/bin/env python3\n\nimport argparse\n\n\ndef select_coverage_file(coverage_files, threshold, output):\n    selected_coverage = []\n    for coverage_file in coverage_files:\n        lines = coverage_file.readlines()\n        if len(lines) != 4:\n            exit(1)\n        if float(lines[1].strip()) >= threshold:\n            selected_coverage.append(lines)\n    with open(output, 'w') as output_file:\n        if len(selected_coverage) == 0:\n            with open('errors.log', 'w') as errors_log:\n                errors_log.write('no lineage could be found')\n                output_file.write(\"lineage,reference,hcov\\n\")\n                output_file.write(\"NA,-,-\")    \n        elif len(selected_coverage) > 1:\n            with open('errors.log', 'w') as errors_log:\n                errors_log.write('The sample cannot be associated to more than one lineage')\n                output_file.write(\"lineage,reference,hcov\\n\")\n                output_file.write(\"ND,-,-\")    \n        else:\n                lines = selected_coverage[0]            \n                output_file.write(\"lineage,reference,hcov\\n\")\n                output_file.write(\"%s,%s,%s\" % (lines[3].strip(), lines[2].strip(), lines[1].strip()))\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--file_list', dest='coverage_files', required=True,\n                        help='coverage_files', nargs='+',\n                        type=argparse.FileType('r'))\n    parser.add_argument('--threshold', dest='threshold', required=True,\n                        help='minimum hcov',\n                        type=float)\n    parser.add_argument('--output', dest='output', required=True,\n                        help='output file name',\n                        type=str)\n    args = parser.parse_args()\n\n    select_coverage_file(args.coverage_files, args.threshold, args.output)\n"}
{"file_name": "SampleReadsCheck_nanopore.py", "file_path": "/scripts/step_0SQ_rawreads__fastq/SampleReadsCheck_nanopore.py", "language": "python", "id": "SampleReadsCheck_nanopore", "content": "#!/usr/bin/env python3\n\nimport sys\nimport argparse\nimport os\n\ndef tsv2diz(fileStat):\n\tdiz = {}\n\twith open(fileStat,'r') as checkIn:\n\t\tfor line in checkIn:\n\t\t\tkey, val = line.strip().split(\"\\t\")\n\t\t\tdiz[key]= val\n\treturn diz\n\ndef makeReport(name,stat):\n\twith open(name+\"_readsCheck.csv\",'w') as checkOut:\n\t\tnoVal = \",\".join([\"NULL\"]*11)\n\t\tcheckOut.write(\"#Sample_name,Total_rawReads,Total_rawMbases,Mean_rawLength,Q30_rawReads,Mean_rawAvgQual,Total_trimReads,trimDiscarded,trimUnpaired,trimPaired,trimPairedMbases,Mean_trimPairedLength,Q30_trimPairedReads,Mean_trimPairedAvgQual,Overlap_trimPaired,WARNING,FAIL\\n\")\n\t\tcheckOut.write(name+\",\"+str(stat[\"number_of_reads\"])+\",\"+str(stat[\"number_of_bases\"])+\",\"+str(stat[\"mean_read_length\"])+\",NULL,\"+str(stat[\"mean_qual\"])+\",\"+noVal+\"\\n\")\n\treturn\n\n# MAIN\nif __name__ == '__main__':\n\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-s', '--stats',help='Output Nanoplot (NanoStas.txt)',type=str, required=True)\n\targs = parser.parse_args()\n\n\tif os.path.exists(args.stats):\n\t\tstatDiz = tsv2diz(args.stats)\n\t\tmakeReport(args.name,statDiz)\n\telse:\n\t\tprint(\"Error, statistic file not found\")\n\t\n\t\n\t\n\t\n"}
{"file_name": "SampleReadsCheck_ionTorrent.py", "file_path": "/scripts/step_1PP_trimming__fastp/SampleReadsCheck_ionTorrent.py", "language": "python", "id": "SampleReadsCheck_ionTorrent", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import median,average\nimport sys\nimport argparse\n#import multiprocessing as mp\n\ndef safe_open(file, mode='rt'):\n\t# safe open file\n\tif file.endswith('.gz'):\n\t\timport gzip\n\t\treturn gzip.open(file, mode)\n\telse:\n\t\treturn open(file, mode)\n\t\ndef readsInfo(fastq):\n\tres = {}\n\tlengthReads=[]\n\treadsQual30=0\n\tavgQual=[]\n\thandle = safe_open(fastq)\n\tfor record in SeqIO.parse(handle, \"fastq\"):\n\t\tlengthReads.append(len(record.seq))\n\t\tphredQualityList = record.letter_annotations['phred_quality']\n\t\tavgQual.append(average(phredQualityList))\n\t\tif average(phredQualityList)>30:\n\t\t\treadsQual30+=1\n\thandle.close()\n\t\n\tq30Reads = round(readsQual30/float(len(lengthReads))*100,2)\n\tMbases = round(float(sum(lengthReads))/1000000,2)\n\n\tres.update({\"lengthReads\":lengthReads})\n\tres.update({\"avgQual\":avgQual})\n\tres.update({\"readsQual30\":readsQual30})\n\tres.update({\"q30Reads\":q30Reads})\n\tres.update({\"Mbases\":Mbases})\n\treturn res\n\n# Class Definition\nclass SampleRaw:\n\t\n\tdef __init__(self, read1):\n\t\tprint(\"Processing raw reads...\")\n\t\tself.R1 = readsInfo(read1)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_R1(self):\n\t\treturn self.R1\n\t\t\n\tdef get_warnings(self):\n\t\treturn self.warnings\n\t\t\n\tdef get_fails(self):\n\t\treturn self.fails\n\t\n\tdef getNR(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef get_q30Reads(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treadsQual30 = self.R1.get(\"readsQual30\")\n\t\treturn round(readsQual30/float(len(lengthReads))*100,2)\n\t\t\n\tdef get_Qavg(self):\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\treturn round(median(avgQual),2)\n\t\n\tdef get_stats(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\tMbases = round(float(sum(lengthReads))/1000000,2)\n\t\treturn [len(lengthReads),Mbases,min(lengthReads),max(lengthReads),round(average(lengthReads),2),self.get_q30Reads(),min(avgQual),max(avgQual),round(median(avgQual),2)]\n\t\n\tdef checkQ30(self):\n\t\tq30R = self.get_q30Reads()\n\t\tif float(q30R) >= 25:\n\t\t\treturn \n\t\telif float(q30R) >= 5 and float(q30R) < 25:\n\t\t\tself.warnings.append(\"WARN_rawQ30\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawQ30\")\n\t\t\t\n\tdef checkQavg(self):\n\t\tavgQualR = self.get_Qavg()\n\t\tif float(avgQualR) >= 22:\n\t\t\treturn \n\t\telif float(avgQualR) >= 16 and float(avgQualR) < 22:\n\t\t\tself.warnings.append(\"WARN_rawQavg\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawQavg\")\n\t\n\tdef checkNR_Bact(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 350000:\n\t\t\treturn \n\t\telif float(readsTot) >= 250000 and float(readsTot) < 350000:\n\t\t\tself.warnings.append(\"WARN_rawNR_bact\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawNR_bact\")\n\t\t\t\n\tdef checkNR_Vir(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 100000:\n\t\t\treturn \n\t\telif float(readsTot) >= 500000 and float(readsTot) < 100000:\n\t\t\tself.warnings.append(\"WARN_rawNR_vir\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_rawNR_vir\")\n\nclass SampleTrimmed:\n\t\n\tdef __init__(self, read1):\n\t\tprint(\"Processing trimmed reads...\")\n\t\tself.R1 = readsInfo(read1)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_R1(self):\n\t\treturn self.R1\n\t\n\tdef get_warnings(self):\n\t\treturn self.warnings\n\t\t\n\tdef get_fails(self):\n\t\treturn self.fails\n\t\n\tdef getNR_total(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef getNR(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treturn len(lengthReads)\n\t\n\tdef get_q30Reads(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\treadsQual30 = self.R1.get(\"readsQual30\")\n\t\treturn round(readsQual30/float(len(lengthReads))*100,2)\n\t\t\n\tdef get_Qavg(self):\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\treturn round(median(avgQual),2)\n\t\n\tdef get_stats(self):\n\t\tlengthReads = self.R1.get(\"lengthReads\")\n\t\tavgQual = self.R1.get(\"avgQual\")\n\t\tMbases = round(float(sum(lengthReads))/1000000,2)\n\t\treturn [self.getNR_total(),0,0,Mbases,min(lengthReads),max(lengthReads),round(average(lengthReads),2),self.get_q30Reads(),min(avgQual),max(avgQual),round(median(avgQual),2)]\n\t\n\tdef checkQ30(self):\n\t\tq30R = self.get_q30Reads()\n\t\tif float(q30R) >= 35:\n\t\t\treturn \n\t\telif float(q30R) >= 10 and float(q30R) < 35:\n\t\t\tself.warnings.append(\"WARN_trimQ30\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimQ30\")\n\t\t\t\n\tdef checkQavg(self):\n\t\tavgQualR = self.get_Qavg()\n\t\tif float(avgQualR) >= 24:\n\t\t\treturn \n\t\telif float(avgQualR) >= 18 and float(avgQualR) < 24:\n\t\t\tself.warnings.append(\"WARN_trimQavg\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimQavg\")\n\t\n\tdef checkNR_Bact(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 300000:\n\t\t\treturn \n\t\telif float(readsTot) >= 50000 and float(readsTot) < 300000:\n\t\t\tself.warnings.append(\"WARN_trimNR_bact\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimNR_bact\")\n\t\t\t\n\tdef checkNR_Vir(self):\n\t\treadsTot = self.getNR()\n\t\tif float(readsTot) >= 90000:\n\t\t\treturn \n\t\telif float(readsTot) >= 40000 and float(readsTot) < 90000:\n\t\t\tself.warnings.append(\"WARN_trimNR_vir\")\n\t\telse:\n\t\t\tself.fails.append(\"FAIL_trimNR_vir\")\n\t\nclass Sample:\n\t\n\tdef __init__(self, sampleName, R1, T1=None):\n\t\tself.name = sampleName\n\t\tself.Raw = SampleRaw(R1)\n\t\tif T1 != None:\n\t\t\tself.Trimmed = SampleTrimmed(T1)\n\t\telse:\n\t\t\tself.Trimmed = None\n\t\t#self.Pear = SamplePear(Fpear,Rpear,Merged)\n\t\tself.warnings = []\n\t\tself.fails = []\n\t\n\tdef get_name(self):\n\t\treturn self.name\n\n\tdef raw_check(self):\n\t\tself.Raw.checkQ30()\n\t\tself.Raw.checkQavg()\n\t\tself.Raw.checkNR_Bact()\n\t\tself.Raw.checkNR_Vir()\n\t\twith open(self.get_name()+\"_SRC_raw.csv\",'w') as checkOut:\n\t\t\tresults = \",\".join(str(x) for x in self.Raw.get_stats())\n\t\t\tcheckOut.write(\"#Sample_name,Total_reads,Total_Mbases,Min_length,Max_length,Mean_length,Q30_reads,Min_avgQual,Max_avgQual,Med_avgQual\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\"\\n\")\n\t\treturn\n\t\n\tdef raw_esito(self):\n\t\tresWarn = \":\".join(self.Raw.get_warnings())\n\t\tresFail = \":\".join(self.Raw.get_fails())\n\t\twith open(self.get_name()+\"_SRC_raw.check\",'w') as resOut:\n\t\t\tresOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n\t\t\tresOut.write(self.get_name()+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\n\tdef trimm_check(self):\n\t\tself.Trimmed.checkQ30()\n\t\tself.Trimmed.checkQavg()\n\t\tself.Trimmed.checkNR_Bact()\n\t\tself.Trimmed.checkNR_Vir()\n\t\twith open(self.get_name()+\"_SRC_treads.csv\",'w') as checkOut:\n\t\t\tresults = \",\".join(str(x) for x in self.Trimmed.get_stats())\n\t\t\tcheckOut.write(\"#Sample_name,Total_reads,Unpaired_reads,Paired_reads,Paired_Mbases,Min_PairedLength,Max_PairedLength,Mean_PairedLength,Q30_PairedReads,Min_PairedAvgQual,Max_PairedAvgQual,Mean_PairedAvgQual\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\"\\n\")\n\t\treturn\n\t\t\t\n\tdef trimm_esito(self):\n\t\tresWarn = \":\".join(self.Trimmed.get_warnings())\n\t\tresFail = \":\".join(self.Trimmed.get_fails())\n\t\twith open(self.get_name()+\"_SRC_treads.check\",'w') as resOut:\n\t\t\tresOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n\t\t\tresOut.write(self.get_name()+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\t\n\tdef trimDiscarded(self):\n\t\tdiscarded = self.Raw.getNR()-self.Trimmed.getNR_total()\n\t\treturn discarded\n\t\t\n\tdef trimOverlap(self):\n\t\toverlap = round(float(self.Pear.getNR_Merged())/float(self.Trimmed.getNR())*100,2)\n\t\treturn overlap\n\t\t\n\t\t\t\n\tdef makeReport(self):\n\t\twith open(self.get_name()+\"_readsCheck.csv\",'w') as checkOut:\n\t\t\tresRaw = self.Raw.get_stats()\n\t\t\tif self.Trimmed != None:\n\t\t\t\tresTrim = self.Trimmed.get_stats()\n\t\t\t\tresWarn = \":\".join(self.Raw.get_warnings()+self.Trimmed.get_warnings())\n\t\t\t\tresFail = \":\".join(self.Raw.get_fails()+self.Trimmed.get_fails())\n\t\t\t\tresults = \",\".join(str(x) for x in [resRaw[0],resRaw[1],resRaw[4],resRaw[5],resRaw[8],resTrim[0],self.trimDiscarded(),resTrim[1],resTrim[2],resTrim[3],resTrim[6],resTrim[7],resTrim[10],0])\n\t\t\telse:\n\t\t\t\tresTrim = [0,0,0,0,0,0,0,0,0,0,0]\n\t\t\t\tresWarn = \":\".join(self.Raw.get_warnings())\n\t\t\t\tresFail = \":\".join(self.Raw.get_fails())\n\t\t\t\tresults = \",\".join(str(x) for x in [resRaw[0],resRaw[1],resRaw[4],resRaw[5],resRaw[8],resTrim[0],0,resTrim[1],resTrim[2],resTrim[3],resTrim[6],resTrim[7],resTrim[10],0])\n\t\t\tcheckOut.write(\"#Sample_name,Total_rawReads,Total_rawMbases,Mean_rawLength,Q30_rawReads,Mean_rawAvgQual,Total_trimReads,trimDiscarded,trimUnpaired,trimPaired,trimPairedMbases,Mean_trimPairedLength,Q30_trimPairedReads,Mean_trimPairedAvgQual,Overlap_trimPaired,WARNING,FAIL\\n\")\n\t\t\tcheckOut.write(self.get_name()+\",\"+results+\",\"+resWarn+\",\"+resFail+\"\\n\")\n\t\treturn\n\n# MAIN\nif __name__ == '__main__':\n\n\tparser = argparse.ArgumentParser(description='FASTQ Report')\n\tparser.add_argument('-n', '--name',help='Sample name',type=str, required=True)\n\tparser.add_argument('-R1', '--Read1',help='Fastq R1',type=str, required=True)\n\t#parser.add_argument('-R2', '--Read2',help='Fastq R2',type=str)\n\tparser.add_argument('-T1', '--Treads1',help='Fastq R1',type=str)\n\t# parser.add_argument('-T2', '--Treads2',help='Fastq R2',type=str, required=True)\n\t# parser.add_argument('-U', '--Unpaired',help='Fastq Unpaired',type=str, required=True)\n\t# parser.add_argument('-F', '--Frw',help='Fastq Forward Pear',type=str, required=True)\n\t# parser.add_argument('-R', '--Rev',help='Fastq Reverse Pear',type=str, required=True)\n\t# parser.add_argument('-M', '--Mrg',help='Fastq Merged Pear',type=str, required=True)\n\targs = parser.parse_args()\n\n\tif args.Treads1 != None:\n\t\tsample = Sample(args.name,args.Read1,args.Treads1)\n\t\t# RAW CHECK\n\t\tsample.raw_check()\n\t\tsample.raw_esito()\n\t\t\n\t\t# TRIMMED CHECK\n\t\tsample.trimm_check()\n\t\tsample.trimm_esito()\n\t\t\n\t\t# CREATE REPORT\n\t\tsample.makeReport()\n\telse:\n\t\tsample = Sample(args.name,args.Read1)\n\t\t# RAW CHECK\n\t\tsample.raw_check()\n\t\tsample.raw_esito()\n\n\t\t# CREATE REPORT\n\t\tsample.makeReport()\n\t\n\t\n\t\n\t\n"}
{"file_name": "SampleReadsCheck.py", "file_path": "/scripts/step_1PP_trimming__fastp/SampleReadsCheck.py", "language": "python", "id": "SampleReadsCheck", "content": "#!/usr/bin/env python3\n\nfrom Bio import SeqIO\nfrom numpy import average\nimport os\nimport sys\nimport argparse\nfrom multiprocessing import Pool\nimport functools\n\ndef decode(c):\n    return ord(c) - 33\t\n\ndef length_and_quality(title, sequence, quality):\n    qual = functools.reduce(lambda a, b: a+b, map(decode, quality)) / len(quality)\n    return [ len(sequence), qual ]\n\ndef safe_open(file, mode='rt'):\n    # safe open file\n    if file.endswith('.gz'):\n        import gzip\n        return gzip.open(file, mode)\n    else:\n        return open(file, mode)\n\n\ndef readsInfo(fastq):\n    res = {}\n    lengthReads = []\n    readsQual30 = 0\n    avgQual = []\n    try:\n        pool_size = min(os.cpu_count(), 8)\n\n        with safe_open(fastq) as handle, Pool(pool_size) as pool:\n            result = pool.starmap(length_and_quality, SeqIO.QualityIO.FastqGeneralIterator(handle))\n        \n        for (seqlen, qual) in result:\n            lengthReads.append(seqlen)\n            avgQual.append(qual)\n            if qual > 30:\n                readsQual30 += 1\n        \n        # lengthReads = list(map(lambda a: a[0], result)) \n        # avgQual = list(map(lambda a: a[1], result)) \n        # readsQual30 = len(list(filter(lambda x: x > 30, avgQual)))\n\n    except Exception as e: \n        print(e)\n        print(\"Errore nel file \" + fastq)\n        exit(1)\n\n    if len(lengthReads) != 0:\n        q30Reads = round(readsQual30/float(len(lengthReads))*100,2)\n        Mbases = round(float(sum(lengthReads))/1000000,2)\n        res.update({\"countReads\":len(lengthReads)})\n        res.update({\"minLengthReads\":min(lengthReads)})\n        res.update({\"maxLengthReads\":max(lengthReads)})\n        res.update({\"avgLengthReads\":average(lengthReads)})\n        res.update({\"minQual\":min(avgQual)})\n        res.update({\"maxQual\":max(avgQual)})\n        res.update({\"avgQual\":average(avgQual)})\n        res.update({\"readsQ30\":readsQual30})\n        res.update({\"percQ30Reads\":q30Reads})\n        res.update({\"Mbases\":Mbases})\n    else:\n        res.update({\"countReads\":0})\n        res.update({\"minLengthReads\":0})\n        res.update({\"maxLengthReads\":0})\n        res.update({\"avgLengthReads\":0})\n        res.update({\"minQual\":0})\n        res.update({\"maxQual\":0})\n        res.update({\"avgQual\":0})\n        res.update({\"readsQ30\":0})\n        res.update({\"percQ30Reads\":0})\n        res.update({\"Mbases\":0})\n\n    return res\n\n\n# Class Definition\nclass SampleRaw:\n\n    def __init__(self, read1, read2):\n        # pool = mp.Pool(processes=2)\n        print(\"Raw reads:\")\n        # fastqInfo = pool.map(readsInfo,[read1,read2])\n        # self.R1 = fastqInfo[0]\n        # self.R2 = fastqInfo[1]\n        print(\"Processing R1...\")\n        self.R1 = readsInfo(read1)\n        print(\"Processing R2...\")\n        self.R2 = readsInfo(read2)\n        self.warnings = []\n        self.fails = []\n\n    def get_R1(self):\n        return self.R1\n\n    def get_R2(self):\n        return self.R2\n\n    def get_warnings(self):\n        return self.warnings\n\n    def get_fails(self):\n        return self.fails\n\n    def getNR(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        return totReads\n\n    def get_q30Reads(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        totQ30 = self.R1.get(\"readsQ30\") + self.R2.get(\"readsQ30\")\n        if float(totReads) == 0:\n            return 0\n        return round(totQ30 / float(totReads) * 100, 2)\n\n    def get_stats(self):\n        minLength = min(self.R1.get(\"minLengthReads\"), self.R2.get(\"minLengthReads\"))\n        maxLength = min(self.R1.get(\"maxLengthReads\"), self.R2.get(\"maxLengthReads\"))\n        avgLength = round((self.R1.get(\"avgLengthReads\") + self.R2.get(\"avgLengthReads\")) / 2, 2)\n        minQual = min(self.R1.get(\"minQual\"), self.R2.get(\"minQual\"))\n        maxQual = min(self.R1.get(\"maxQual\"), self.R2.get(\"maxQual\"))\n        avgQual = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        totMbases = self.R1.get(\"Mbases\") + self.R2.get(\"Mbases\")\n        return [self.getNR(), totMbases, minLength, maxLength, avgLength, self.get_q30Reads(), minQual, maxQual,\n                avgQual]\n\n    def checkPair(self):\n        if self.R1.get(\"countReads\") != self.R2.get(\"countReads\"):\n            self.fails.append(\"FAIL_raw_pairNR\")\n\n    def checkQ30(self):\n        q30R = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(q30R) >= 25:\n            return\n        elif float(q30R) >= 5 and float(q30R) < 25:\n            self.warnings.append(\"WARN_rawQ30\")\n        else:\n            self.fails.append(\"FAIL_rawQ30\")\n\n    def checkQavg(self):\n        avgQualR = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(avgQualR) >= 22:\n            return\n        elif float(avgQualR) >= 16 and float(avgQualR) < 22:\n            self.warnings.append(\"WARN_rawQavg\")\n        else:\n            self.fails.append(\"FAIL_rawQavg\")\n\n    def checkNR_Bact(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 350000:\n            return\n        elif float(readsTot) >= 250000 and float(readsTot) < 350000:\n            self.warnings.append(\"WARN_rawNR_bact\")\n        else:\n            self.fails.append(\"FAIL_rawNR_bact\")\n\n    def checkNR_Vir(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 100000:\n            return\n        elif float(readsTot) >= 500000 and float(readsTot) < 100000:\n            self.warnings.append(\"WARN_rawNR_vir\")\n        else:\n            self.fails.append(\"FAIL_rawNR_vir\")\n\n\nclass SampleTrimmed:\n\n    def __init__(self, read1, read2, unp):\n        # pool = mp.Pool(processes=3)\n        print(\"Trimmed reads:\")\n        # fastqInfo = pool.map(readsInfo,[read1,read2,unp])\n        # self.R1 = fastqInfo[0]\n        # self.R2 = fastqInfo[1]\n        # self.U = fastqInfo[2]\n        print(\"Processing R1...\")\n        self.R1 = readsInfo(read1)\n        print(\"Processing R2...\")\n        self.R2 = readsInfo(read2)\n        print(\"Processing Unpaired...\")\n        self.U = readsInfo(unp)\n        self.warnings = []\n        self.fails = []\n\n    def get_R1(self):\n        return self.R1\n\n    def get_R2(self):\n        return self.R2\n\n    def get_unpaired(self):\n        return self.U\n\n    def get_warnings(self):\n        return self.warnings\n\n    def get_fails(self):\n        return self.fails\n\n    def getNR_total(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\") + self.U.get(\"countReads\")\n        return totReads\n\n    def getNR(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        return totReads\n\n    def getNR_unpaired(self):\n        return self.U.get(\"countReads\")\n\n    def get_q30Reads(self):\n        totReads = self.R1.get(\"countReads\") + self.R2.get(\"countReads\")\n        totQ30 = self.R1.get(\"readsQ30\") + self.R2.get(\"readsQ30\")\n        if float(totReads) == 0:\n            return 0\n        return round(totQ30 / float(totReads) * 100, 2)\n\n    def get_stats(self):\n        minLength = min(self.R1.get(\"minLengthReads\"), self.R2.get(\"minLengthReads\"))\n        maxLength = min(self.R1.get(\"maxLengthReads\"), self.R2.get(\"maxLengthReads\"))\n        avgLength = round((self.R1.get(\"avgLengthReads\") + self.R2.get(\"avgLengthReads\")) / 2, 2)\n        minQual = min(self.R1.get(\"minQual\"), self.R2.get(\"minQual\"))\n        maxQual = min(self.R1.get(\"maxQual\"), self.R2.get(\"maxQual\"))\n        avgQual = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        totMbases = self.R1.get(\"Mbases\") + self.R2.get(\"Mbases\")\n        return [self.getNR_total(), self.getNR_unpaired(), self.getNR(), totMbases, minLength, maxLength, avgLength,\n                self.get_q30Reads(), minQual, maxQual, avgQual]\n\n    def checkPair(self):\n        if self.R1.get(\"countReads\") != self.R2.get(\"countReads\"):\n            self.fails.append(\"FAIL_trim_pairNR\")\n\n    def checkQ30(self):\n        q30R = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(q30R) >= 35:\n            return\n        elif float(q30R) >= 10 and float(q30R) < 35:\n            self.warnings.append(\"WARN_trimQ30\")\n        else:\n            self.fails.append(\"FAIL_trimQ30\")\n\n    def checkQavg(self):\n        avgQualR = round((self.R1.get(\"avgQual\") + self.R2.get(\"avgQual\")) / 2, 2)\n        if float(avgQualR) >= 24:\n            return\n        elif float(avgQualR) >= 18 and float(avgQualR) < 24:\n            self.warnings.append(\"WARN_trimQavg\")\n        else:\n            self.fails.append(\"FAIL_trimQavg\")\n\n    def checkNR_Bact(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 300000:\n            return\n        elif float(readsTot) >= 50000 and float(readsTot) < 300000:\n            self.warnings.append(\"WARN_trimNR_bact\")\n        else:\n            self.fails.append(\"FAIL_trimNR_bact\")\n\n    def checkNR_Vir(self):\n        readsTot = self.getNR()\n        if float(readsTot) >= 90000:\n            return\n        elif float(readsTot) >= 40000 and float(readsTot) < 90000:\n            self.warnings.append(\"WARN_trimNR_vir\")\n        else:\n            self.fails.append(\"FAIL_trimNR_vir\")\n\n\nclass Sample:\n\n    def __init__(self, sampleName, R1, R2, T1, T2, Unp):\n        self.name = sampleName\n        self.Raw = SampleRaw(R1, R2)\n        self.Trimmed = SampleTrimmed(T1, T2, Unp)\n        self.warnings = []\n        self.fails = []\n\n    def get_name(self):\n        return self.name\n\n    def raw_check(self):\n        self.Raw.checkPair()\n        self.Raw.checkQ30()\n        self.Raw.checkQavg()\n        self.Raw.checkNR_Bact()\n        self.Raw.checkNR_Vir()\n        with open(self.get_name() + \"_SRC_raw.csv\", 'w') as checkOut:\n            results = \",\".join(str(x) for x in self.Raw.get_stats())\n            checkOut.write(\n                \"#Sample_name,Total_reads,Total_Mbases,Min_length,Max_length,Mean_length,Q30_reads,Min_avgQual,Max_avgQual,Mean_avgQual\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \"\\n\")\n        return\n\n    def raw_esito(self):\n        resWarn = \":\".join(self.Raw.get_warnings())\n        resFail = \":\".join(self.Raw.get_fails())\n        with open(self.get_name() + \"_SRC_raw.check\", 'w') as resOut:\n            resOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n            resOut.write(self.get_name() + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n    def trimm_check(self):\n        self.Trimmed.checkPair()\n        self.Trimmed.checkQ30()\n        self.Trimmed.checkQavg()\n        self.Trimmed.checkNR_Bact()\n        self.Trimmed.checkNR_Vir()\n        with open(self.get_name() + \"_SRC_treads.csv\", 'w') as checkOut:\n            results = \",\".join(str(x) for x in self.Trimmed.get_stats())\n            checkOut.write(\n                \"#Sample_name,Total_reads,Unpaired_reads,Paired_reads,Paired_Mbases,Min_PairedLength,Max_PairedLength,Mean_PairedLength,Q30_PairedReads,Min_PairedAvgQual,Max_PairedAvgQual,Mean_PairedAvgQual\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \"\\n\")\n        return\n\n    def trimm_esito(self):\n        resWarn = \":\".join(self.Trimmed.get_warnings())\n        resFail = \":\".join(self.Trimmed.get_fails())\n        with open(self.get_name() + \"_SRC_treads.check\", 'w') as resOut:\n            resOut.write(\"#Sample_name,WARNING,FAIL\\n\")\n            resOut.write(self.get_name() + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n    def trimDiscarded(self):\n        discarded = self.Raw.getNR() - self.Trimmed.getNR_total()\n        return discarded\n\n    def makeReport(self):\n        with open(self.get_name() + \"_readsCheck.csv\", 'w') as checkOut:\n            resRaw = self.Raw.get_stats()\n            resTrim = self.Trimmed.get_stats()\n            resWarn = \":\".join(self.Raw.get_warnings() + self.Trimmed.get_warnings())\n            resFail = \":\".join(self.Raw.get_fails() + self.Trimmed.get_fails())\n            results = \",\".join(str(x) for x in\n                               [resRaw[0], resRaw[1], resRaw[4], resRaw[5], resRaw[8], resTrim[0], self.trimDiscarded(),\n                                resTrim[1], resTrim[2], resTrim[3], resTrim[6], resTrim[7], resTrim[10], 0])\n            checkOut.write(\n                \"#Sample_name,Total_rawReads,Total_rawMbases,Mean_rawLength,Q30_rawReads,Mean_rawAvgQual,Total_trimReads,trimDiscarded,trimUnpaired,trimPaired,trimPairedMbases,Mean_trimPairedLength,Q30_trimPairedReads,Mean_trimPairedAvgQual,Overlap_trimPaired,WARNING,FAIL\\n\")\n            checkOut.write(self.get_name() + \",\" + results + \",\" + resWarn + \",\" + resFail + \"\\n\")\n        return\n\n\n# MAIN\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='FASTQ Report')\n    parser.add_argument('-n', '--name', help='Sample name', type=str, required=True)\n    parser.add_argument('-R1', '--Read1', help='Fastq R1', type=str, required=True)\n    parser.add_argument('-R2', '--Read2', help='Fastq R2', type=str, required=True)\n    parser.add_argument('-T1', '--Treads1', help='Fastq trimmed R1', type=str, required=True)\n    parser.add_argument('-T2', '--Treads2', help='Fastq trimmed R2', type=str, required=True)\n    parser.add_argument('-U', '--Unpaired', help='Fastq unpaired', type=str, required=True)\n    args = parser.parse_args()\n\n    sample = Sample(args.name, args.Read1, args.Read2, args.Treads1, args.Treads2, args.Unpaired)\n\n    # RAW CHECK\n    print(\"Raw reads CHECK\")\n    sample.raw_check()\n    print(\"Raw reads REPORT\")\n    sample.raw_esito()\n\n    # TRIMMED CHECK\n    print(\"Trimmed reads CHECK\")\n    sample.trimm_check()\n    print(\"Trimmed reads REPORT\")\n    sample.trimm_esito()\n\n    # CREATE REPORT\n    print(\"Create final report\")\n    sample.makeReport()\n\n    print(\"DONE\")\n\n\n"}
{"file_name": "process-mlst-result.py", "file_path": "/scripts/step_4TY_MLST__mlst/process-mlst-result.py", "language": "python", "id": "process-mlst-result", "content": "#!/usr/bin/env python3\n\nimport string\nimport sys\nimport os\n\n\ndef add_cc(input_file, output_file, mlst_db_folder):\n    summary = open(input_file, \"r\").readlines()\n    complete = open(output_file, \"w\")\n    complete.write(\"DS;GENPAT;SCHEMA;CC;ST;loci(varianti)\\n\")\n    for riga in summary:\n        sample = riga.split(\"\\t\")[0]\n        ds = sample.split(\"DS\")[1].split(\"-\")[0]\n        genpat = sample.split(\"/\")[-1].split(\"_\")[1]\n        schema = riga.split(\"\\t\")[1]\n        st = riga.split(\"\\t\")[2].strip()\n        loci = \" \".join(riga.split(\"\\t\")[3:])\n        if schema == '-':\n            complete.write(\"%s;%s;%s;;;%s\" % (ds, genpat, schema, loci))\n            break;           \n        try:\n            schema_file = open(mlst_db_folder + \"/\" + schema + \"/\" + schema + \".txt\", \"r\").readlines()\n            header = schema_file[0]\n            cc = \"\"\n            if \"Lineage\" in header:\n                for combination in schema_file[1:]:\n                    if combination.split(\"\\t\")[0].strip() == st:\n                        cc = combination.split(\"\\t\")[-2].strip()\n            else:\n                for combination in schema_file[1:]:\n                    if combination.split(\"\\t\")[0].strip() == st:\n                        cc = combination.split(\"\\t\")[-1].strip()\n            complete.write(\"%s;%s;%s;%s;%s;%s\" % (ds, genpat, schema, cc, st, loci))\n        except Exception as ex:\n            print(\"exception :\" + str(ex))\n            complete.write(\"%s;%s;%s;;%s;%s\" % (ds, genpat, schema, st, loci))\n            with open(os.path.splitext(output_file)[0] + \"_exception.log\", 'aw') as log:\n                log.write(\n                    \"Error while looking for clonal complex in: %s/%s/%s.txt \" % (mlst_db_folder, schema, schema))\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 4:\n        sys.stderr.write(\"Usage: %s <input_file> <base_name> <db_folder>\" % (sys.argv[0]))\n        sys.exit(1)\n    else:\n        add_cc(sys.argv[1], sys.argv[2], sys.argv[3])\n"}
{"file_name": "pipeline_main.nf", "file_path": "/pipelines/pipeline_main.nf", "language": "nextflow", "id": "pipeline_main", "content": "nextflow.enable.dsl=2\n\ninclude { module_reads_processing } from '../modules/module_reads_processing'\ninclude { module_wgs_bacteria } from '../modules/module_wgs_bacteria'\ninclude { module_typing_bacteria } from '../modules/module_typing_bacteria'\ninclude { module_covid_emergency } from '../modules/module_covid_emergency'\ninclude { module_vdraft } from '../modules/module_vdraft'\ninclude { module_westnile } from '../modules/module_westnile'\ninclude { module_qc_quast } from '../modules/module_qc_quast'\ninclude { step_4TY_lineage__pangolin } from '../steps/step_4TY_lineage__pangolin'\n\n\ninclude { getSingleInput;param } from '../functions/parameters.nf'\ninclude { getEmpty;extractKey;isFastqRiscd;isFastaRiscd } from '../functions/common.nf'\ninclude { isNegativeControl;isNegativeControlSarsCov2;isPositiveControlSarsCov2;isVirus;isWNV;isBacterium;isSarsCov2;isAmpliseq } from '../functions/sampletypes.nf'\n\nworkflow ngsmanager_fastq {\n    take: \n      rawReads  \n    main:\n      try {       \n        trimmed = module_reads_processing(rawReads).trimmed_with_data\n\n        trimmed.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        assemblyBacteria = module_wgs_bacteria(branched.bacteria)\n        crossedBacteriaData = branched.bacteria.cross(assemblyBacteria) { extractKey(it) }.multiMap { \n          trimmed: it[0]\n          assembly: it[1]\n        }\n        module_typing_bacteria(crossedBacteriaData.trimmed, crossedBacteriaData.assembly)\n\n        /* sarscov2 stuff here */\n        module_covid_emergency(branched.sarscov2)\n\n        /* viruses stuff here */\n        // no extra modules executed\n        \n        /* wnv stuff here */\n        module_westnile(branched.wnv)\n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }\n     \n}\n\nworkflow ngsmanager_fasta {\n    take: \n      fasta\n    main:\n      try {      \n        module_qc_quast(fasta)\n\n        fasta.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        module_typing_bacteria(Channel.empty(), branched.bacteria)\n\n        /* sarscov2 stuff here */\n        step_4TY_lineage__pangolin(branched.sarscov2)       \n\n        /* wnv stuff here */\n        module_westnile(branched.wnv)        \n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }     \n}\n\nworkflow pipeline_ngsmanager {\n  if (isFastqRiscd(param('riscd'))){\n    ngsmanager_fastq(getSingleInput())\n  } else if (isFastaRiscd(param('riscd'))){\n    ngsmanager_fasta(getSingleInput())\n  } else {\n    exit 2, \"unexpected riscd provided: ${param('riscd')}\"\n  }\n}\n\nworkflow {\n  pipeline_ngsmanager()\n}"}
{"file_name": "pipeline_ngsmanager.nf", "file_path": "/pipelines/pipeline_ngsmanager.nf", "language": "nextflow", "id": "pipeline_ngsmanager", "content": "nextflow.enable.dsl=2\n\ninclude { module_reads_processing } from '../modules/module_reads_processing'\ninclude { module_wgs_bacteria } from '../modules/module_wgs_bacteria'\ninclude { module_typing_bacteria } from '../modules/module_typing_bacteria'\ninclude { module_covid_emergency } from '../modules/module_covid_emergency'\ninclude { module_vdraft } from '../modules/module_vdraft'\ninclude { module_westnile } from '../modules/module_westnile'\ninclude { module_qc_quast } from '../modules/module_qc_quast'\ninclude { step_4TY_lineage__pangolin } from '../steps/step_4TY_lineage__pangolin'\n\n\ninclude { getSingleInput;param } from '../functions/parameters.nf'\ninclude { getEmpty;extractKey;isFastqRiscd;isFastaRiscd } from '../functions/common.nf'\ninclude { isNegativeControl;isNegativeControlSarsCov2;isPositiveControlSarsCov2;isVirus;isWNV;isBacterium;isSarsCov2;isAmpliseq } from '../functions/sampletypes.nf'\n\nworkflow ngsmanager_fastq {\n    take: \n      rawReads  \n    main:\n      try {       \n        trimmed = module_reads_processing(rawReads).trimmed_with_data\n\n        trimmed.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        assemblyBacteria = module_wgs_bacteria(branched.bacteria)\n        crossedBacteriaData = branched.bacteria.cross(assemblyBacteria) { extractKey(it) }.multiMap { \n          trimmed: it[0]\n          assembly: it[1]\n        }\n        module_typing_bacteria(crossedBacteriaData.trimmed, crossedBacteriaData.assembly)\n\n        /* sarscov2 stuff here */\n        module_covid_emergency(branched.sarscov2)\n\n        /* viruses stuff here */\n        // no extra modules executed\n        \n        /* wnv stuff here */\n        module_westnile(branched.wnv)\n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }\n     \n}\n\nworkflow ngsmanager_fasta {\n    take: \n      fasta\n    main:\n      try {      \n        module_qc_quast(fasta)\n\n        fasta.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        module_typing_bacteria(Channel.empty(), branched.bacteria)\n\n        /* sarscov2 stuff here */\n        step_4TY_lineage__pangolin(branched.sarscov2)       \n\n        /* wnv stuff here */\n        module_westnile(branched.wnv)        \n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }     \n}\n\nworkflow pipeline_ngsmanager {\n  if (isFastqRiscd(param('riscd'))){\n    ngsmanager_fastq(getSingleInput())\n  } else if (isFastaRiscd(param('riscd'))){\n    ngsmanager_fasta(getSingleInput())\n  } else {\n    exit 2, \"unexpected riscd provided: ${param('riscd')}\"\n  }\n}\n\nworkflow {\n  pipeline_ngsmanager()\n}"}
{"file_name": "common.nf", "file_path": "/functions/common.nf", "language": "nextflow", "id": "common", "content": "import java.time.format.DateTimeFormatter\n\n//@see https://github.com/nextflow-io/nextflow/issues/1694\n//https://github.com/nf-core/sarek/blob/a7679b9b5c178351b1e96a3ffe7ee81ddf9aad06/main.nf#L226\nEMPTY_FILE = file(\"${workflow.workDir}/empty_file\")\nEMPTY_FILE.text = ''\n\nDT_PATTERN = DateTimeFormatter.ofPattern(\"yyMMdd\");\nDT_PATTERN_YEAR = DateTimeFormatter.ofPattern(\"yyyy\");\n\n\ndef logHeader(message){\n    // Log colors ANSI codes\n    def monolog = params.containsKey('monochrome_logs') && params.monochrome_logs\n    def c_reset = monolog  ? '' : \"\\033[0m\";\n    def c_cyan = monolog ? '' : \"\\033[0;36m\";\n    return \"\"\"    \n        ----------------------------------------------------${c_reset}\n        ${c_cyan}scriptId${c_reset}=$workflow.scriptId\n        ${c_cyan}start${c_reset}=$workflow.start\n        ${c_cyan}commandLine${c_reset}=$workflow.commandLine\n        ${c_cyan}launchDir${c_reset}=$workflow.launchDir\n        ----------------------------------------------------${c_reset}\n        ${c_cyan}  ${message} ${c_reset} \n        ----------------------------------------------------${c_reset}\n    \"\"\".stripIndent()    \n}\n\ndef flattenPath(filename){\n   try {        \n       filename.substring(filename.lastIndexOf(\"/\")+1)\n   } catch(Throwable t) {\n       exit 1, \"unexpected exception: ${t.asString()}\"\n   } \n}\n\ndef parseMetadata(input) {\n    def filename\n    if (input instanceof java.util.Collection) {\n        filename = input.flatten()[0].getName()\n    } else if (input instanceof java.nio.file.Path) {\n        filename = input.toFile().getName()\n    } else if (input instanceof java.io.File) {\n        filename = input.getName()\n    } else {\n        filename = input\n    }\n    return parseMetadataFromFileName(filename)\n}\n\ndef parseMetadataFromFileName(input){\n  try {        \n        def filename = (input instanceof java.util.Collection) ? input.flatten()[0].getName() : input\n\n        def md = [:]\n        def matcher = (filename =~ /^(DS\\d+)\\D(DT\\d+)_(\\d{4})(\\.[^\\.]+\\.\\d+\\.\\d+\\.\\d+)(_[^_]+)?.*$/)\n\n        if (matcher.matches()) {\n            md.ds = matcher.group(1)\n            md.dt = matcher.group(2)\n            md.anno = matcher.group(3)\n            md.cmp = matcher.group(3)+matcher.group(4)\n            md.suffix = matcher.group(5) //seems not to work properly\n        } else {\n            exit 1, \"unexpected file name: ${filename}\"\n        }\n        return md\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }     \n}\n\n/* execution specific stuff */\ndef executionMetadata() {\n    try {        \n        if (params.containsKey('analysis_date')) {\n            def matcher = (params.analysis_date =~ /^(\\d{2})(\\d+)$/)\n\n            if (matcher.matches()) {\n                def anno = \"20${matcher.group(1)}\"\n                def dt = \"${matcher.group(1)}${matcher.group(2)}\"\n                return [\n                    \"anno\": anno,\n                    \"dt\": \"DT${dt}\"\n                ]\n            } else {\n                exit 2, \"param 'analysis_date' should match pattern 'yyMMddHHmmSSsss' or 'yyMMdd' (e.g.210524 or 210525093000123)\"\n            }\n        } else {\n            return [\n                \"anno\":  workflow.start.format(DT_PATTERN_YEAR),\n                \"dt\": \"DT\" + workflow.start.format(DT_PATTERN)\n            ]\n        }\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }       \n}\n\ndef csv2map(_path, separator){\n    try {       \n        def path = (_path instanceof java.util.Collection) ? _path.flatten()[0] : _path\n        def lines = path.toRealPath().toFile().readLines()\n        if (lines.size() != 2) {\n            exit 1, \"${path} should have exactly 2 lines\"\n        }\n\n        def headers = lines[0].split(separator)\n        def values =  lines[1].split(separator)\n\n        if (headers.length != values.length) {\n            exit 1, \"headers size not matching values size\"\n        }\n\n        def md = [:]\n        for (int i = 0; i < values.length; i++) {\n            md[\"${headers[i]}\"] = \"${values[i]}\";\n        }\n\n        return md;\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }    \n}\n\ndef extractKey(input) {\n    try {        \n        def value = (input instanceof java.util.Collection) ? input.flatten()[0] : input\n\n        value = (value instanceof  java.nio.file.Path) ? value.getName() : value\n\n        def matcher = (value =~ /^(DS\\d+)(\\D.*)?$/)\n\n        if (matcher.matches()) {\n            //println \"${value} -> ${matcher.group(1)}\"\n            return matcher.group(1)\n        } else {\n            matcher = (value =~ /^\\d+-(\\d+)-[^-]+-.+$/)\n            if (matcher.matches()) {\n                //println \"${value} -> ${matcher.group(1)}\"\n                return 'DS'+matcher.group(1)\n            }\n        }\n        exit 1, \"could not extract DS or RISCD from: ${input}\"\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }\n}\n\ndef extractDsRef(input) {\n    try {     \n        def value = (input instanceof java.util.Collection) ? input.flatten().find { it instanceof  java.nio.file.Path } : input\n\n        value = (value instanceof  java.nio.file.Path) ? value.getName() : value\n\n        def matcher = (value =~ /^(DS\\d+)(\\D.*)_([^_]+)\\..+$/)\n\n        if (matcher.matches()) {\n            // use string, NOT GStringImpl (e.g. \"${var}\")\n            return matcher.group(1) + \"-\" + matcher.group(3)\n        } else {\n            exit 1, \"could not extract Base Name from: ${input}\"\n        }\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }\n}\n\ndef getBaseName(fileName) {\n    try {      \n        return (fileName - ~/\\.\\w+$/)\n    } catch(Throwable t) {\n        exit 1, \"could not get getBaseName, unexpected exception: ${t.asString()}\"\n    }       \n}\n\ndef getGB(inputPath) {\n    try {\n        def referencePath = (inputPath instanceof java.util.Collection) ? inputPath.flatten()[0] : inputPath\n        def realPath = referencePath.toRealPath()\n        def refName = referencePath.getName()\n        if (refName ==~ (\"^.+\\\\.gb\\$\")) {\n            // already a GB\n            return referencePath\n        }\n        if (! refName ==~ (\"^.+\\\\.fa[^\\\\.]*\\$\")) {\n            // unexpected file name\n            log.warn \"reference supposed to be a FASTA or GB\"\n            return EMPTY_FILE\n        }\n        def expectedGBName = refName.replaceFirst(\"^(.+)\\\\.fa[^\\\\.]*\\$\", \"\\$1.gb\")\n        if (refName == expectedGBName) {\n            if (refName != EMPTY_FILE.getName()) {\n                log.warn \"unexpected file name for fasta reference: ${refName}\"  \n            }\n            return EMPTY_FILE\n        }\n        def result = realPath.resolveSibling(expectedGBName)\n        if (!result || !result?.exists()) {\n          log.warn \"not found GenBank reference to run snippy for: ${refName}\"\n          return EMPTY_FILE\n        }\n        return result;\n    } catch(Throwable t) {\n        log.error \"error while looking for a GB reference for: ${inputPath}, exception: ${t.asString()}\"\n        return EMPTY_FILE\n    }        \n}\n\ndef getEmpty() {\n    return EMPTY_FILE;\n}\n\ndef parseRISCD(riscd){\n     try {        \n        def md = [:]\n        def matcher = (riscd =~ /^(\\d+)-(\\d+)-(.+)-([^_]+)(_.+)?$/)\n\n        if (matcher.matches()) {\n            md.dt = \"DT${matcher.group(1)}\"\n            md.ds = \"DS${matcher.group(2)}\"\n            md.acc = matcher.group(3)\n            md.met = matcher.group(4)\n            md.ref = matcher.group(5) ? matcher.group(5).substring(1) : ''\n        } else {\n            exit 1, \"unexpected RISCD format: ${riscd}\"\n        }\n        log.debug \"RISCD parsing: ${md.toString()}\"\n        return md\n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }    \n}\n\ndef taskMemory(obj, attempt) {\n    try {\n        def mem \n        switch(attempt) {\n            case 1: \n                mem = obj; break\n            case 2:\n                mem = obj * 3; break\n            default: \n                mem = params.max_memory as nextflow.util.MemoryUnit\n        }\n        return (mem.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1) ? params.max_memory : mem\n    } catch (t) {\n        println \"taskMemory: unexpected exception: ${t.asString()}\"\n        return obj\n    }\n}\n\ndef taskTime(obj, attempt) {\n    try {\n        def time \n        switch(attempt) {\n            case 1: \n                time = obj; break\n            default: \n                time = params.max_time as nextflow.util.Duration\n        }\n        return (time.compareTo(params.max_time as nextflow.util.Duration) == 1) ? params.max_time : time\n    } catch (t) {\n        println \"taskTime: unexpected exception: ${t.asString()}\"\n        return obj\n    }\n}\n\ndef getRisCd(md, ex, step, method) {\n  try {\n    return \"${ex.dt.drop(2)}-${md.ds.drop(2)}-${step}-${method}\"\n  } catch(Throwable t) {\n      exit 1,  \"could not get RISCD from input for: ${md}, ${ex}, ${step}, ${method}; error: ${t.message}\"\n  }\n}\n\ndef stepInputs(riscd_input, md, ex, step, method, opt) {\n    try {\n      def result  = [:]\n      if (riscd_input instanceof java.util.Collection) {\n            riscd_input = riscd_input.findAll { it != '-' && it != null}\n            if (riscd_input.size() == 0) {\n                log.warn \"riscd_input should have at least one value\"\n                return '{}'\n            }\n      } \n      result.input = riscd_input\n      result.riscd = getRisCd(md, ex, step, method)\n      def meta = workflow.toMap().findAll { it.key in ['scriptId', 'scriptName', 'commitId', 'revision', 'profile', 'sessionId'] && it.value != null }  ?: [:]\n      ['_meta_commit_id', '_meta_commit_date', '_meta_build_tag', '_meta_pipeline_timestamp'].findAll { params.containsKey(it) }.each { meta[(it -~ /^_meta_/)] = params[it] }\n      meta.nextflow = nextflow.toJsonMap().findAll { it.key in ['version', 'build'] }\n      result.meta = meta\n      result.params = opt ?: [:]\n      return groovy.json.JsonOutput.toJson(result)\n          } catch(Throwable t) {\n        exit 1,  \"could not track input for: ${md}, error: ${t}\"\n    }\n}\n\ndef isFastqRiscd(riscd) {\n    try {\n        acc = parseRISCD(riscd)?.acc;\n        return acc ==~ /0SQ.+/\n    } catch(Throwable t) {\n        exit 1,  \"could not execute isFastqRiscd for input: ${riscd}, error: ${t}\"\n    }\n}\n\ndef isFastaRiscd(riscd) {\n    try {\n        acc = parseRISCD(riscd)?.acc;\n        return acc ==~ /2AS.+/\n    } catch(Throwable t) {\n        exit 1,  \"could not execute isFastaRiscd for input: ${riscd}, error: ${t}\"\n    }\n}\n\ndef isSpeciesSupported(gsp, whitelist, reads, entrypoint) {\n  try {\n    def genus_species = gsp ? gsp.toLowerCase() : ''\n    def allowedList = whitelist.collect { it.toLowerCase() }\n    def (genus, species) = genus_species.contains(\"_\") ? genus_species.split('_') : [ genus_species, null ]\n    if (allowedList.contains(genus_species)) {\n        // genus_species allowed\n        return true;\n    }       \n    if (allowedList.contains(genus)) {\n        // ALL genus allowed\n        return true;\n    }\n    if (entrypoint && reads) {\n        def (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, '']\n        def md = parseMetadataFromFileName(r1.getName())\n        if (workflow.scriptName && workflow.scriptName.startsWith('step_')) {\n            log.warn \"${entrypoint} [${md?.cmp}] - genus_species not supported: ${genus_species}. Supported: ${allowedList}\"\n        }\n    }  \n    return false\n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n"}
{"file_name": "sampletypes.nf", "file_path": "/functions/sampletypes.nf", "language": "nextflow", "id": "sampletypes", "content": "include { parseMetadata } from './common.nf'\n\ndef getSampleSheetMetadata(value){\n    try {      \n        def riscd = (value instanceof java.util.Collection) ? value.flatten()[0] : value\n        def matcher = (riscd =~ /\\d+-(\\d+)-.+$/)\n        if (!matcher.matches()) {\n            log.warn \"cannot extract DS from riscd: ${riscd}\"\n            return [:];\n        }\n        return getDsMetadata(\"DS${matcher.group(1)}\")\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }    \n}\n\ndef isRunningFromSampleSheet() {\n    return false\n}\n\ndef isVirus(riscd) {\n   try {  \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'virus')\n        }           \n       return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isWNV(riscd) {\n   try {  \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'wnv')\n        }           \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isNGSMG16S(riscd) {\n   try {  \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'NGSMG16S')\n        }           \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isNegativeControl(riscd) {\n   try {      \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'NGTVCTRL')\n        }    \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isNegativeControlSarsCov2(riscd) {\n   try {     \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'NGTVCTRLSC2')\n\n        }          \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isPositiveControlSarsCov2(riscd) {\n   try {      \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'PSTVCTRLSC2')\n        }         \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isBacterium(riscd) {\n try {      \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'bacterium')\n        }\n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }        \n}\n\ndef isAmpliseq(riscd) {\n try {      \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'ampliseq')\n        }\n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }        \n}\n\ndef isSarsCov2(riscd) {\n     try {      \n        if (params.containsKey('sample_type')) {\n            return (params.sample_type == 'sarscov2')\n        }         \n        return false\n    } catch(Throwable t) {\n        exit 1, \"could not get sample type, unexpected exception: ${t.asString()}\"\n    }     \n}"}
{"file_name": "parameters.nf", "file_path": "/functions/parameters.nf", "language": "nextflow", "id": "parameters", "content": "include { getEmpty;parseRISCD;flattenPath;executionMetadata;getGB;getRisCd } from './common.nf'\n\ndef getTrimmedReads(optional){\n    def allowedAcc = ['1PP_trimming', '1PP_hostdepl', '1PP_generated']\n    if (!params.containsKey('cmp') || !params.containsKey('trimming-riscd') || !(params['trimming-riscd'] instanceof CharSequence)) {\n        if (optional) {\n            log.debug \"no trimmed reads available\"\n            return Channel.empty()\n        }\n        exit 2, \"missing required params (cmp,trimming-riscd)\";\n    }\n    def riscd =  params['trimming-riscd']\n    def cmp = params.cmp.replaceAll(\"-\", \".\")\n    def anno = cmp.substring(0,4)\n    def md = parseRISCD(riscd)            \n    if (!(md.acc in allowedAcc)) {\n        exit 2, \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n    }      \n    def path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/*.fastq*\"\n    def resChannel = Channel.fromPath(\n            path, checkIfExists: params.check_file_existence && !optional\n    )        \n    .toSortedList( { a, b -> flattenPath(a.getName()) <=> flattenPath(b.getName()) } )             \n    .collect()\n    .map { [ riscd , it ] }\n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel\n}\n\ndef getAssembly() {\n    def allowedAcc = ['2AS_denovo', '2AS_import', '2AS_mapping']\n    if (!params.containsKey('cmp') || !params.containsKey('riscd')) {\n        exit 2, \"missing required params (cmp,riscd)\";\n    }\n    def cmp = params.cmp.replaceAll(\"-\", \".\")\n    def anno = cmp.substring(0,4)\n    def md = parseRISCD(params.riscd)       \n    def path, resChannel     \n    if (!(md.acc in allowedAcc)) {\n        exit 2, \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n    }         \n    if (md.acc == '2AS_import' || md.acc == '2AS_mapping') {\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/*.fasta\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ params.riscd, it ] }\n\n    } else {\n        def pattern = (md.met == 'shovill' ? \"*.fasta\" : \"*L?00.fasta\")\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ params.riscd, it ] }\n    }\n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel\n}\n\ndef getDepletedReads(){\n    def allowedAcc = ['1PP_hostdepl']\n    if (!params.containsKey('cmp') || !params.containsKey('riscd')) {\n        exit 2, \"missing required params (cmp,riscd)\";\n    }\n    def cmp = params.cmp.replaceAll(\"-\", \".\")\n    def anno = cmp.substring(0,4)\n    def md = parseRISCD(params.riscd)            \n    if (!(md.acc in allowedAcc)) {\n        log.error \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n        exit 2, \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n    }        \n    def path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/*${md.ref ? '_' : ''}${md.ref}.fastq*\"\n    \n    def resChannel = Channel.fromPath(\n            path, checkIfExists: params.check_file_existence\n    )        \n    .toSortedList( { a, b -> flattenPath(a.getName()) <=> flattenPath(b.getName()) } )             \n    .collect()\n    .map { [ params.riscd, it ] }\n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel\n}\n\ndef getReferenceCodes() {\n    if (!params.containsKey('references') || !(params.references instanceof ArrayList)) {\n        return []\n    }\n    return params.references.collect { it.code }\n}\n\ndef getNCBICodes() {\n    def val = param('reference')\n    if (val instanceof ArrayList) {\n        return Channel.fromList(val)\n    } else {\n        return Channel.fromList(val.tokenize(',\\s\\n\\t\\r'))\n    }\n}\n\ndef getReferences(type) {\n    _getReferences(false, type, true, true)\n}\n\ndef getReference(type) {\n    _getReferences(false, type, true, false)\n}\n\ndef getReferenceUnkeyed(type) {\n    _getReferences(false, type, false, false).map { [ it[1], it[2], it[3] ]}\n}\n\ndef getReferenceOptional(type) {\n    _getReferences(true, type, true, false)\n}\n\ndef _getReferences(optional, type, keyed, multi) {\n    def crossValue = \"\"\n    if (keyed) {\n        // CROSSING PARAM\n        if (!params.containsKey('riscd')) {\n            exit 2, \"one of: [ds, riscd] param should be provided\";\n        } \n        crossValue = parseRISCD(params.riscd).ds         \n    }\n    if (params.containsKey('references')) {\n        assert params.references instanceof ArrayList\n        if (!params.references && !optional) {\n            exit 2, \"No reference provided\";\n        }\n        def refs = params.references.collect { refOriginal ->\n            def ref = [:]\n            refOriginal.keySet().each {\n                ref[\"ref_${it}\"] = refOriginal[it]\n            }\n            return ref\n        }\n        def result = refs.inject (Channel.empty()) {\n            res, val -> res.concat(_getSingleReference(optional, type, val, crossValue))\n        }              \n        if (!multi && params.references.size() > 1) {\n            log.warn \"${params.references.size()} references provided; only the first one will be considered.\"\n        }\n        // never returns more than one reference if multi is false\n        return multi ? result : result.first()\n    } else {\n        _getSingleReference(optional, type, params, crossValue)\n    } \n}\n\ndef _getSingleReference(optional, type, refInput, crossValue) {\n    def allowedAcc = ['2AS_mapping', '2AS_denovo', '2AS_import']\n    if (refInput.containsKey('ref_cmp') && refInput.containsKey('ref_code') &&\n          (\n            (type == 'gb' && refInput.containsKey('ref_riscd_genbank') && refInput.ref_riscd_genbank)\n              ||\n            (type == 'fa' && refInput.containsKey('ref_riscd') &&  refInput.ref_riscd)\n              ||\n            (type == 'any' && (\n                (refInput.containsKey('ref_riscd') &&  refInput.ref_riscd) || (refInput.containsKey('ref_riscd_genbank') && refInput.ref_riscd_genbank)\n                              )\n            )\n          )\n        ) {\n        def useFastaRef = (type == 'fa') || (type == 'any' && (refInput.containsKey('ref_riscd') &&  refInput.ref_riscd))\n\n        def cmp = refInput.ref_cmp.replaceAll(\"-\", \".\")\n        def anno = cmp.substring(0,4)\n        def filePattern, md, refRiscd \n        if (useFastaRef) {\n            refRiscd = refInput.ref_riscd\n            md = parseRISCD(refInput.ref_riscd)  \n            filePattern = md.met == 'spades' ? '*L?00.fa*' : '*.fa*'\n        } else {\n            allowedAcc = ['4AN_genes']\n            refRiscd = refInput.ref_riscd_genbank\n            md = parseRISCD(refInput.ref_riscd_genbank)  \n            filePattern = '*.gb*'\n        }\n        if (!(md.acc in allowedAcc)) {\n            exit 2, \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n        }\n        def path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${filePattern}\"\n        return Channel.fromPath(\n             path, checkIfExists: params.check_file_existence && !optional\n        )\n        .first().map { [ crossValue, refRiscd, refInput.ref_code, it ] }\n        .ifEmpty([ crossValue, '-', '-', getEmpty() ])       \n    } else if (!optional) {\n        exit 2, \"could not find reference of type '${type}'\";\n    } else {\n        log.debug \"Not using (optional) reference, crossValue: ${crossValue}, type: ${type}\"\n        return Channel.of([ crossValue, '-', '-', getEmpty() ]) \n    }\n}\n\ndef getHost() {\n    _getParam('host', false, true)\n}\n\ndef getHostOptional() {\n    _getParam('host', true, true)\n}\n\ndef getHostUnkeyed() {\n    _getParam('host', false, false)\n}\n\n//https://www.gitmemory.com/issue/nextflow-io/nextflow/1388/564021238\ndef getGenusSpeciesOptionalUnkeyed(){\n    _getParam('genus_species', true, false)\n}\n\ndef getGenusSpeciesOptional(){\n    _getParam('genus_species', true, true)\n}\n\ndef getGenusSpecies(){\n    _getParam('genus_species', false, true)\n}\n\ndef getSpecies(){\n    _getParam('species', false, true)\n}\n\ndef getModule(){\n    _getParam('module', false, true)\n}\n\ndef getAbricateDatabase(){\n    _getParam('abricate_database', false, true)\n}\n\ndef getBlastDatabase(){\n    _getParam('blast_database', false, true)\n}\n\ndef getBlastDatabaseUnkeyed(){\n    _getParam('blast_database', false, false)\n}\n\ndef getKingdom(){\n    _getParam('kingdom', false, true)\n}\n\ndef getTaxIdsUnkeyed(){\n    _getParam('taxids', true, false)\n}\n\ndef getParamTaxaId() {\n    _getParamAsValue('taxaid', false, null)\n}\n\ndef getParamIncludeParents() {\n    _getParamAsValue('include-parents', true, false)\n}\n\ndef getParamIncludeChildren() {\n    _getParamAsValue('include-children', true, false)\n}\n\ndef _getParamAsValue(paramName, optional, defaultValue) {\n    def res\n    if (paramName && params.containsKey(paramName) && params[paramName] != '' && params[paramName] != null) {\n        res = params[paramName]\n    } else if (optional) {      \n        res = defaultValue\n    } else  {\n        exit 2, \"missing required param: $paramName\";\n    }  \n    // no malicious chars inside\n    if (res != null && res ==~ /(?s).*$[;|&><\\(\\)\\n].*/) {\n        exit 2, \"possibile malicious content for param '${paramName}', value: '${value}'\"\n    }\n    return res\n}\n\ndef _getParam(paramName, optional, keyed){\n     def crossValue\n     if (keyed) {\n        // CROSSING PARAM\n        if (!params.containsKey('riscd')) {\n            exit 2, \"missing required param: riscd\";\n        }\n        crossValue = parseRISCD(params.riscd).ds\n        if (!crossValue) {\n            exit 2, \"could not get DS from riscd: ${params.riscd}\"\n        }\n    }\n    if (paramName && params.containsKey(paramName) && params[paramName]) {\n        if (crossValue) {          \n            return Channel.of( [ crossValue, params[paramName] ] )\n        } else {\n            return Channel.of( params[paramName] )\n        }\n    } else if (optional) {      \n        if (crossValue) {       \n            return Channel.of( [ crossValue, null ] )\n        } else {\n            return Channel.of( null )\n        }\n    } else  {\n        exit 2, \"missing required param: $paramName\";\n    }\n}\n\ndef getDS() {\n    if (!params.containsKey('riscd')) {\n        exit 2, \"missing required params (riscd)\";\n    }    \n    return parseRISCD(params.riscd).ds\n}\n\ndef isFullOutput() {\n    return (params.containsKey('full_output') && params.full_output) || (params.containsKey('fullOutput') && params.fullOutput);\n}\n\ndef getResult(cmp, riscd, filePattern) {\n    def anno = cmp.substring(0,4)\n    def md = parseRISCD(riscd)       \n    def resChannel, path\n    if (md.acc in ['0SQ_rawreads', '1PP_hostdepl', '1PP_trimming', '1PP_filtering', '1PP_downsampling', '1PP_generated']) {\n        def pattern = (filePattern != null) ? filePattern :  \"*.fastq*\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .toSortedList( { a, b -> flattenPath(a.getName()) <=> flattenPath(b.getName()) } )             \n        .collect()\n        .map { [ riscd, it ] }\n    } else if (md.acc in ['2AS_import', '2AS_mapping']) {\n        def pattern = (filePattern != null) ? filePattern :  \"*.fasta\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }        \n    } else if (md.acc in ['2AS_hybrid']) {\n        def pattern = (filePattern != null) ? filePattern :  \"*.fasta\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }        \n    } else if (md.acc in ['2AS_denovo','2MG_denovo']) {\n        def pattern = (filePattern != null) ? filePattern :  (md.met in ['spades','plasmidspades','unicycler','metaspades'] ? \"*L?00.fasta\" : \"*.fasta\")\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }\n    } else if (md.acc in ['4AN_genes']) {\n        def pattern = (filePattern != null) ? filePattern :  \"*.gff\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }        \n    } else if (md.acc in ['4TY_cgMLST'] && md.met in ['chewbbaca']) {\n        def pattern = (filePattern != null) ? filePattern : \"*_results_${params.allelic_profile_encoding}.?sv\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }                \n    } else if (md.acc in ['4AN_AMR']) {\n        def pattern = (filePattern != null) ? filePattern :  \"*abricate_calls.txt\"\n        path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${pattern}\"\n        resChannel = Channel.fromPath(\n                path, checkIfExists: params.check_file_existence\n        )        \n        .first()\n        .collect()\n        .map { [ riscd, it ] }        \n    } else {\n        exit 2, \"unexpected acc value: ${md.acc}\"\n    }    \n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel     \n}\n\ndef _getAlleles(cmp, riscd, schema) {\n    def md = parseRISCD(riscd)           \n    def filePattern = schema ? \"*_results_${schema}.?sv\" : \"*results_alleles.?sv\"\n    def anno = cmp.substring(0,4)\n    def path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/${filePattern}\"\n    def resChannel = Channel.fromPath(path, checkIfExists: params.check_file_existence)\n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel\n}\n\ndef getKrakenResults() {\n    def allowedAcc = ['3TX_class']\n    if (!params.containsKey('cmp') || !params.containsKey('riscd')) {\n        exit 2, \"missing required params (cmp,riscd)\";\n    }\n    def md = parseRISCD(params.riscd)            \n    if (!(md.acc in allowedAcc)) {\n        exit 2, \"unexpected acc value: ${md.acc}, expected: one of ${allowedAcc}\"\n    }     \n    def cmp = params.cmp.replaceAll(\"-\", \".\")\n    def anno = cmp.substring(0,4)    \n    def path = \"${params.inputdir}/${anno}/${cmp}/${md.acc}/${md.ds}-${md.dt}_${md.met}/result/*_kraken.{tsv,txt.gz}\"\n    def resChannel = Channel.fromPath(\n            path, checkIfExists: params.check_file_existence\n    )\n    .collect()\n    .map { [ params.riscd, it ] }\n    resChannel.ifEmpty { \n        log.warn(\"file not found: '${path}'\")\n    }\n    return resChannel\n}\n\ndef getParam(paramName, optional, keyed) {\n    _getParam(paramName, optional, keyed)\n}\n\ndef getInputOf(pattern) {\n    if (params.containsKey('input')) {\n        assert params.input instanceof ArrayList\n        params.input.inject (Channel.empty()) {\n            res, val -> res.mix(getResult(val.cmp, val.riscd, pattern))\n        }        \n    } else if (params.containsKey('cmp') && params.containsKey('riscd')) {\n        getResult(params.cmp, params.riscd, pattern)\n    } else {\n        exit 2, \"missing required params (cmp,riscd) or (input)\";\n    } \n}\n\ndef getInput() {\n    if (params.containsKey('input')) {\n        assert params.input instanceof ArrayList\n        params.input.inject (Channel.empty()) {\n            res, val -> res.mix(getResult(val.cmp, val.riscd, null))\n        }        \n    } else if (params.containsKey('cmp') && params.containsKey('riscd')) {\n        getResult(params.cmp, params.riscd, null)\n    } else {\n        exit 2, \"missing required params (cmp,riscd) or (input)\";\n    } \n}\n\ndef getInputFolders() {\n    getInputOf('')\n}\n\ndef getVCFs() {\n    getInputOf('{*[0-9].vcf,*.var.flt.vcf}')\n}\n\ndef getSingleInput() {\n    if (!params.containsKey('cmp') || !params.containsKey('riscd')) {\n        exit 2, \"missing required params (cmp,riscd)\";\n    }\n    getResult(params.cmp, params.riscd, null)\n}\n\ndef hasEnoughFastqData(rawreads) {\n  try {\n    def (r1,r2, r3, r4) = (rawreads instanceof java.util.Collection) ? rawreads : [rawreads,null]\n    sizeR1 = r1.toRealPath().countFastq()\n    sizeR2 = r2 ? r2.toRealPath().countFastq() : null\n    log.debug \"${r1}, size: ${sizeR1}\"\n    if (sizeR1 <= params.raw_reads_threshold && (sizeR2 == null || sizeR2 <= params.raw_reads_threshold)) {\n      log.warn \"Insufficient number of reads after trimming in: ${r1} (${sizeR1}) ${r2 ?: '-'} (${sizeR2 == null ? '-' : sizeR2}) (threshold: ${params.raw_reads_threshold})\"\n      return false;\n    }\n    return true;\n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n\ndef hasFastqData(rawreads) {\n  try {\n    def (r1,r2) = (rawreads instanceof java.util.Collection) ? rawreads : [rawreads,null]\n    sizeR1 = r1.toRealPath().countFastq()\n    sizeR2 = r2 ? r2.toRealPath().countFastq() : null\n    log.debug \"${r1}, size: ${sizeR1}\"\n    if (sizeR1 == 0 && (sizeR2 == null || sizeR2 == 0)) {\n      log.warn \"No processable reads found in: ${r1} ${r2}\"\n      return false;\n    }\n    return true;\n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n\ndef param(paramName) {\n    _getParamAsValue(paramName, false, null)\n} \n\ndef optional(paramName) {\n    _getParamAsValue(paramName, true, '')\n}\n\ndef optionalOrDefault(paramName, defaultValue) {\n    _getParamAsValue(paramName, true, defaultValue)\n}\n\n\ndef isIonTorrent(reads) {\n   try {  \n        if (params.containsKey('seq_type')) {\n            return (params.seq_type == 'ion')\n        }           \n        return isCompatibleWithSeqType(reads, 'ion', null)\n    } catch(Throwable t) {\n        exit 1, \"could not get 'seq_type', unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isNanopore(reads) {\n   try {  \n        if (params.containsKey('seq_type')) {\n            return (params.seq_type == 'nanopore')\n        }  \n        return isCompatibleWithSeqType(reads, 'nanopore', null)\n    } catch(Throwable t) {\n        exit 1, \"could not get 'seq_type', unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isIlluminaPaired(reads) {\n   try {  \n        if (params.containsKey('seq_type')) {\n            return (params.seq_type == 'illumina_paired')\n        }    \n        return isCompatibleWithSeqType(reads, 'illumina_paired', null)    \n    } catch(Throwable t) {\n        exit 1, \"could not get 'seq_type', unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isCompatibleWithSeqType(reads, types, entrypoint) {\n   try {  \n        if (!reads || !types) {\n            return false\n        }\n        def allowed = types instanceof java.util.Collection ? types : [ types ]\n        def type = params.containsKey('seq_type') ? params.seq_type : ''\n        if (!type) {\n            if (reads instanceof java.util.Collection && (reads.size() > 1)) {\n                type = 'illumina_paired'\n            } else {\n                def seReads = (reads instanceof java.util.Collection) ? reads[0] : reads\n                def gzipped = seReads.toFile().getName() ==~ /.+\\.gz/            \n                seReads.toRealPath().toFile().withInputStream {\n                    try (InputStream is = (gzipped ? new java.util.zip.GZIPInputStream(it) : it);\n                        InputStreamReader isr = new InputStreamReader(is);\n                        BufferedReader br = new BufferedReader(isr);) {\n                            // look at the header to understand if this was generated by nanopore\n                            // XXX we need a more robust check\n                            /*\n                            The headers are defined by the basecaller (guppy, Albacore, bonito, etc) during basecalling.\n                            They have changed many times over the years, but generally they start with a readID of @+UUIDv4 \n                            followed by multiple entries separated by spaces, of form key=value\n                            */\n                            type = (br.readLine() ==~ /^@[a-f0-9]{8}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{4}-[a-f0-9]{12}.*/ ) ? 'nanopore' : 'ion'\n                    }\n                }\n            }\n        }\n        def check = allowed.find { it.trim() == type.trim() }.any()\n\n        if (!check && entrypoint && params.incompatible_step_warning) {\n            log.warn \"${entrypoint} not compatible with seq type: ${type}\"\n        }\n        return check\n    } catch(Throwable t) {\n        exit 1, \"could not get 'seq_type', unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef isSegmentedMapping() {\n   try {  \n        // XXX use only parameter 'segmented_mapping' should be used\n        return (params.containsKey('segmented_mapping') && params.segmented_mapping) || \n        (workflow.scriptName && workflow.scriptName.endsWith('_segmented.nf'))  \n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef checkEnum(enumClass, name) {\n    if (!enumClass.values().any { it.name() == name }) {\n        log.warn(\"'${name}' is not a valid option. Expecting one of: ${enumClass.values()}\")\n        return false;\n    }\n    return true\n}\n\ndef getHostReference() {\n    try {  \n        if (!params.containsKey('host')) {\n            exit 2, \"host reference code should be provided\";\n        }     \n        if (!params.containsKey('hosts_dir')) {\n            exit 2, \"hosts_dir should be provided\";\n        }          \n        // CROSSING PARAM\n        if (!params.containsKey('riscd')) {\n            exit 2, \"one of: [ds, riscd] param should be provided\";\n        } \n        def crossValue = parseRISCD(params.riscd).ds         \n        def path = \"${params.hosts_dir}/${params.host}{.,_}*.f*a\"\n        return Channel.fromPath(\n                path, checkIfExists: true\n        )\n        .first().map { [ crossValue, params.host, it ] }        \n    } catch(Throwable t) {\n        exit 1, \"unexpected exception: ${t.asString()}\"\n    }     \n}\n\ndef getLongReads() {\n    if (!params.containsKey('long_reads')) {\n        exit 2, \"long_reads should be provided\";\n    }   \n    def long_reads = params.long_reads\n    if (!long_reads.containsKey('cmp') || !long_reads.containsKey('riscd')) {\n        exit 2, \"long_reads.cmp and long_reads.riscd should be provided\";\n    }\n    return getResult(long_reads.cmp, long_reads.riscd, null)\n}\n\ndef paramWrap(paramName, wrap) {\n    return wrap.replace('{}', \"${param(paramName)}\")\n}\n\ndef optWrap(paramName, wrap) {\n    def val = optional(paramName)\n    if (!val) {\n        return ''\n    }\n    return wrap.replace('{}', \"${val}\")\n}"}
{"file_name": "step_3TX_species__vdabricate.nf", "file_path": "/steps/step_3TX_species__vdabricate.nf", "language": "nextflow", "id": "step_3TX_species__vdabricate", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata; taskMemory } from '../functions/common.nf'\ninclude { param;getInput } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef abricateDBDir = param('step_3TX_species__vdabricate__db')\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_species'\ndef METHOD = 'vdabricate' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess abricate {\n    container 'quay.io/biocontainers/abricate:0.9.8--h1341992_0'\n    containerOptions = \" -v ${abricateDBDir}:${abricateDBDir}:ro\"\n    memory { taskMemory( 6.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(scaffolds), val(database)\n    output:\n      path '*'\n      tuple val(riscd), path(\"${base}_vdabricate_calls.txt\"), emit: calls\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*calls.txt'    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_vdabricate.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)\n      \"\"\"                \n        abricate ${scaffolds} --datadir ${abricateDBDir} -db ${database} &>> ${base}_vdabricate.log >> ${base}_vdabricate_calls.txt        \n      \"\"\"\n}\n\nworkflow step_3TX_species__vdabricate {\n    take: data\n    main:\n      abricate(data);\n    emit:\n      calls = abricate.out.calls\n}\n\nworkflow {\n    getInput()\n        .combine(['viruses_TREF'])\n        .set { scaffoldsAbricatedb }\n    step_3TX_species__vdabricate(scaffoldsAbricatedb)\n}\n"}
{"file_name": "step_4AN_AMR__abricate.nf", "file_path": "/steps/step_4AN_AMR__abricate.nf", "language": "nextflow", "id": "step_4AN_AMR__abricate", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4AN_AMR'\ndef METHOD = 'abricate'\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess abricate {\n    container 'staphb/abricate:1.0.0'\n    memory { taskMemory( 250.MB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(assembly)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_abricate.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.csv,*.summary,*calls.txt}'    \n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"               \n        abricate --db vfdb --csv ${assembly} > abricate.csv  \n        abricate ${assembly} -db vfdb &>> ${base}_abricate.log >> ${base}_abricate_calls.txt        \n        abricate --summary ${base}_abricate_calls.txt > ${base}_abricate.summary\n      \"\"\"\n}\n\nworkflow step_4AN_AMR__abricate {\n    take: data\n    main:\n      abricate(data);\n}\n\nworkflow {\n    step_4AN_AMR__abricate(getInput())\n}\n"}
{"file_name": "step_2AS_denovo__unicycler.nf", "file_path": "/steps/step_2AS_denovo__unicycler.nf", "language": "nextflow", "id": "step_2AS_denovo__unicycler", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'unicycler' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess unicycler {\n    container 'docker.io/biocontainers/unicycler:v0.4.7dfsg-2-deb_cv1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 20.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, 'illumina_paired', task.process)    \n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_unicycler_assembly.fasta\"), emit: scaffolds\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"{*.fasta,*.gfa}\"    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_spades.cfg\" }\n    script:\n      (t1,t2) = reads\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      unicycler -1 ${t1} -2 ${t2} -o . -t 16 > ${base}_unicycler.log 2>&1\n      mv assembly.fasta ${base}_unicycler_assembly.fasta\n      \"\"\"\n}\n\nprocess assembly_filter {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      path(scaffolds)\n    output:\n      tuple val(riscd), path(\"${base}_${METHOD}_scaffolds_L200.fasta\"), emit: fasta\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_assemblyfilter.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      /scripts/AssemblyFilter.py -n ${base} -f ${scaffolds} -u -l 200 -c 0 ;\n      \"\"\"\n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple val(_), path(l200)\n    output:\n      path '*_quast.*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_quast.cfg\" }\n    script:\n      md = parseMetadataFromFileName(l200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${l200} > ${base}_quast.log ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}_quast.csv ; \n      \"\"\"\n}\n\nworkflow step_2AS_denovo__unicycler {\n    take: data\n    main:\n      unicycler(data).scaffolds\n      assembly_filter(unicycler.out.scaffolds).fasta | quast\n    emit:\n      assembled = assembly_filter.out.fasta       \n}\n\nworkflow {\n    step_2AS_denovo__unicycler(getSingleInput())\n}\n\n"}
{"file_name": "step_1PP_trimming__trimmomatic.nf", "file_path": "/steps/step_1PP_trimming__trimmomatic.nf", "language": "nextflow", "id": "step_1PP_trimming__trimmomatic", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;extractKey } from '../functions/common'\ninclude { getInput;isIonTorrent;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_trimming'\ndef METHOD = 'trimmomatic' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess trimmomatic {\n    container 'quay.io/biocontainers/trimmomatic:0.36--6'\n    memory { taskMemory( 1.GB, task.attempt ) }\n        tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      tuple val(riscd), path('*.fastq.gz'), emit: fastq\n      path '{*trimmomatic.log,*.json}'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fastq.gz'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*trimmomatic.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_trimmomatic.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)    \n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n          trimmomatic PE -threads 2 -phred33 $r1 $r2 ${base}_R1_trimmomatic.fastq.gz ${base}_R1_unpaired.fastq.gz ${base}_R2_trimmomatic.fastq.gz ${base}_R2_unpaired.fastq.gz ILLUMINACLIP:/usr/local/share/trimmomatic-0.36-6/adapters/NexteraPE-PE.fa:2:30:10 LEADING:25 TRAILING:25 SLIDINGWINDOW:20:25 MINLEN:36 2>> ${base}_trimmomatic.log;\n          cat ${base}_R1_unpaired.fastq.gz ${base}_R2_unpaired.fastq.gz > ${base}_unpaired_trimmomatic.fastq.gz\n          rm  ${base}_R1_unpaired.fastq.gz ${base}_R2_unpaired.fastq.gz\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n          trimmomatic SE -threads 2 -phred33 $r1 ${base}_R1_trimmomatic.fastq.gz LEADING:3 TRAILING:3 SLIDINGWINDOW:5:20 MINLEN:55 2>> ${base}_trimmomatic.log\n        \"\"\"        \n      }        \n}\n\nprocess sample_reads_check {\n    container \"quay.io/biocontainers/biopython:1.78\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 32.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(_), path(reads)\n      tuple val(_), path(trimmed)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*SRC_raw.*[kg]'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*SRC_raw.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*SRC_treads.*[kg]'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '{*SRC_treads.csv,*_readsCheck.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_readscheck.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      (t1,t2,u) = (trimmed instanceof java.util.Collection) ? trimmed : [trimmed, null, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n        /scripts/SampleReadsCheck.py -n $base -R1 $r1 -R2 $r2 -T1 $t1 -T2 $t2 -U $u > ${base}_SRC_raw.log\n        cat ${base}_SRC_raw.log >\t${base}_SRC_treads.log;\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        /scripts/SampleReadsCheck_ionTorrent.py -n $base -R1 $r1 -T1 $t1 > ${base}_SRC_raw.log\n        \"\"\"\n      }\n}\n\nprocess fastqc {\n    container 'biocontainers/fastqc:v0.11.5_cv4'\n    memory { taskMemory( 400.MB, task.attempt ) }\n        tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '{*.zip,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_fastqc.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      fastqc $reads > \"${base}_fastqc.log\" 2>&1\n      \"\"\"\n}\n\nworkflow step_1PP_trimming__trimmomatic {\n    take: rawreads\n    main:\n      trimmed = trimmomatic(rawreads).fastq;\n      fastqc(trimmed)\n      readsCheckInput = rawreads.cross(trimmed) { extractKey(it) }.multiMap { \n        rawreads: it[0]\n        trimmed: it[1]\n      }      \n      sample_reads_check(readsCheckInput.rawreads, readsCheckInput.trimmed)\n    emit:\n      trimmed\n}\n\nworkflow {\n    step_1PP_trimming__trimmomatic(getInput())\n}"}
{"file_name": "step_1PP_trimming__chopper.nf", "file_path": "/steps/step_1PP_trimming__chopper.nf", "language": "nextflow", "id": "step_1PP_trimming__chopper", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;taskTime } from '../functions/common.nf'\ninclude { getInput;isCompatibleWithSeqType;paramWrap;optWrap } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_trimming'\ndef METHOD = 'chopper' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess chopper {\n    container \"quay.io/biocontainers/chopper:0.7.0--hdcf5f25_0\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    time { taskTime( 10.m, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }       \n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)    \n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      tuple val(riscd), path('*.fastq.gz'), emit: trimmed\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [seq_type: 'nanopore'])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.gz'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.json,*.html}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      quality = paramWrap('step_1PP_trimming__chopper__quality', '--quality {}')\n      minlength = paramWrap('step_1PP_trimming__chopper__minlength', '--minlength {}')\n      maxlength = optWrap('step_1PP_trimming__chopper__maxlength', '--maxlength {}')\n      headcrop = optWrap('step_1PP_trimming__chopper__headcrop', '--headcrop {}')\n      tailcrop = optWrap('step_1PP_trimming__chopper__tailcrop', '--tailcrop {}')\n        \"\"\"\n          zcat $r1 | chopper ${quality} ${minlength} ${maxlength} \\\n          ${headcrop} ${tailcrop} \\\n          --threads $task.cpus | gzip > ${base}_R1.fastq.gz\n        \"\"\"              \n}\n\nprocess nanoplot {\n    container 'quay.io/biocontainers/nanoplot:1.41.3--pyhdfd78af_0'\n    memory { taskMemory( 4.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '{*.txt,*.html}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}_nanoplot.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_nanoplot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      NanoPlot -t ${task.cpus} --fastq ${reads} --tsv_stats --no_static -o . -p ${base}_\n      \"\"\"\n}\n\nworkflow step_1PP_trimming__chopper {\n    take: \n      rawreads\n    main:\n      trimmed = chopper(rawreads).trimmed\n      nanoplot(trimmed)\n    emit:\n      trimmed      \n}\n\nworkflow {\n    step_1PP_trimming__chopper(getInput())\n}"}
{"file_name": "step_4TY_lineage__pangolin.nf", "file_path": "/steps/step_4TY_lineage__pangolin.nf", "language": "nextflow", "id": "step_4TY_lineage__pangolin", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_lineage'\ndef METHOD = 'pangolin' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess pangolin {\n    container \"ghcr.io/genpat-it/pangolin:v4.3.1-v0.1.12-v0.3.17-v1.21\"\n    containerOptions = \"--user root\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    input:\n      tuple val(riscd_input), path(consensus)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'lineage_report.csv', saveAs: { \"${base}_lineage_report.csv\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(consensus.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_pangolin\"\n      \"\"\"\n          pangolin ${consensus}  > ${base}.log\n      \"\"\"\n}\n\nworkflow step_4TY_lineage__pangolin {\n    take: \n      consensus\n    main:\n      pangolin(consensus)\n}\n\nworkflow {\n    step_4TY_lineage__pangolin(getInput())\n}"}
{"file_name": "step_4AN_AMR__filtering.nf", "file_path": "/steps/step_4AN_AMR__filtering.nf", "language": "nextflow", "id": "step_4AN_AMR__filtering", "content": "nextflow.enable.dsl=2\n\ninclude { stepInputs;parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;param } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4AN_AMR'\ndef METHOD = 'filtering'\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess abricate_filtering {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(calls)\n      val(coverage)\n      val(identity)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [identity: identity, coverage: coverage])}' > ${base}_abricate_filtering_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_abricate_filtering.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*_abricate_calls.txt'    \n    script:\n      md = parseMetadataFromFileName(calls.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"    \n        #!/usr/bin/env python3\n\n        new = open(\"${base}_COV${coverage}_ID${identity}_abricate_calls.txt\", \"w\")\n        res = open(\"${calls}\", 'r').readlines()\n        new.write(res[0])\n        for l in res:\n            try:\n                if float(l.split(\"\\t\")[9]) >= ${coverage} and float(l.split(\"\\t\")[10]) >= ${identity}:\n                    new.write(l.replace(l.split(\"\\t\")[0],\"${calls}\"))\n            except:\n                pass\n        new.close()\n      \"\"\"\n}\n\nworkflow step_4AN_AMR__filtering {\n    take: \n      data\n      coverage\n      identity\n    main:\n      abricate_filtering(data, coverage, identity);\n}\n\nworkflow {\n    step_4AN_AMR__filtering(getSingleInput(), param('coverage'), param('identity'))\n}\n"}
{"file_name": "step_4AN_genes__prokka.nf", "file_path": "/steps/step_4AN_genes__prokka.nf", "language": "nextflow", "id": "step_4AN_genes__prokka", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory;extractKey } from '../functions/common.nf'\ninclude { getInput;getKingdom;getReferenceOptional;checkEnum } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4AN_genes'\ndef METHOD = 'prokka'\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nenum KINGDOM {\n\tViruses, Bacteria, Archaea, Mitochondria\n}\n\nprocess prokka {\n    container 'quay.io/biocontainers/prokka:1.14.5--pl526_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    when:\n      checkEnum(KINGDOM, kingdom)\n    input:\n      tuple val(riscd_input), path(scaffolds200), val(kingdom), val(riscd_ref), val(reference), path(gb)\n    output:\n      path '**'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference, kingdom: kingdom])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'result/*', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"{*.log,*.json}\", saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_prokka.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"   \n      if (gb.empty()) {\n        baseMethod =  \"${base}_prokka\"\n        gbParam = \"\"\n      } else {\n        baseMethod =  \"${base}_prokka_${reference}\"\n        gbParam = \"--proteins ${gb}\"\n      }\n      extraOptions = \"--centre X --compliant\"\n      \"\"\"\n        prokka --kingdom ${kingdom} ${extraOptions} --outdir \"result\" ${gbParam} --prefix ${baseMethod}_result ${scaffolds200} 2> ${baseMethod}.log\n      \"\"\"\n}\n\nworkflow step_4AN_genes__prokka {\n    take: \n      data\n    main:\n      prokka(data);\n}\n\nworkflow {\n  getInput()\n      .cross(getKingdom()) { extractKey(it) }\n      .cross(getReferenceOptional('gb')) { extractKey(it) }\n      .map { it.flatten() }  // [ riscd assembly ds kingdom ds riscd_ref refid refpath]\n      .map { \n          [ it[0], it[1], it[3], it[5], it[6], it[7] ]\n      }\n      .set { input }\n  step_4AN_genes__prokka(input)\n}\n"}
{"file_name": "step_2AS_hybrid__unicycler.nf", "file_path": "/steps/step_2AS_hybrid__unicycler.nf", "language": "nextflow", "id": "step_2AS_hybrid__unicycler", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;getLongReads;param } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_hybrid'\ndef METHOD = 'unicycler' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess unicycler {\n    container 'quay.io/biocontainers/unicycler:0.5.0--py38h5cf8b27_3'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    cpus params.max_cpus\n    when:\n      isCompatibleWithSeqType(short_reads, 'illumina_paired', \"${ENTRYPOINT} short_reads\") // not robust enough && isCompatibleWithSeqType(long_reads, 'nanopore', \"${ENTRYPOINT} long_reads\")  \n    input:\n      tuple val(riscd_input), path(short_reads)\n      tuple val(riscd_input2), path(long_reads)\n    output:\n      path '*'\n      path(\"${base}_assembly.fasta\"), emit: scaffolds\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_input2], md, ex, STEP, METHOD, [long_reads:riscd_input2])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"*.fasta\"    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"assembly.gfa\", saveAs: { \"${base}.gfa\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (t1,t2) = short_reads\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      mode = param('step_2AS_hybrid__unicycler__mode')      \n      \"\"\"\n      unicycler -1 ${t1} -2 ${t2} -l ${long_reads} -o . -t ${task.cpus} --mode ${mode}\n      mv assembly.fasta ${base}_assembly.fasta\n      \"\"\"\n}\n\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      path(l200)\n    output:\n      path '*_quast.*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_quast.cfg\" }\n    script:\n      md = parseMetadataFromFileName(l200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${l200} > ${base}_quast.log ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}_quast.csv ; \n      \"\"\"\n}\n\nworkflow step_2AS_hybrid__unicycler {\n    take: \n      short_reads\n      long_reads\n    main:\n      scaffolds = unicycler(short_reads, long_reads).scaffolds\n      quast(scaffolds)\n    emit:\n      scaffolds = scaffolds\n}\n\nworkflow {  \n    step_2AS_hybrid__unicycler(getSingleInput(), getLongReads())\n}\n\n"}
{"file_name": "step_3TX_class__centrifuge.nf", "file_path": "/steps/step_3TX_class__centrifuge.nf", "language": "nextflow", "id": "step_3TX_class__centrifuge", "content": "nextflow.enable.dsl=2\n\ninclude { getEmpty;flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { param;isFullOutput;getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef DB_PATH=param('step_3TX_class__centrifuge__db_path')\ndef DB_NAME=param('step_3TX_class__centrifuge__db_name')\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_class'\ndef METHOD = 'centrifuge' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess centrifuge {\n    container \"quay.io/biocontainers/centrifuge:1.0.4_beta--h9a82719_6\"\n    containerOptions = \"-v ${DB_PATH}:${DB_PATH}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    cpus { [16, params.max_cpus as int].min() }\n    memory { taskMemory( 16.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      tuple path(\"${base}_genus.report\"), path(\"${base}_species.report\"), path(\"${base}_KrakenLike.report\"), emit: reports\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.output,*.report}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\" \n      \"\"\"\n        centrifuge -p ${task.cpus} -x ${DB_PATH}/${DB_NAME} -U ${reads} -S ${base}.output --report-file ${base}.report\n        head -n 1 ${base}.report > ${base}_genus.report; grep -w \"genus\" ${base}.report | sort -t \"\\t\" -nr -k5 >> ${base}_genus.report\n        head -n 1 ${base}.report > ${base}_species.report; grep -w \"species\" ${base}.report | sort -t \"\\t\" -nr -k5 >> ${base}_species.report         \n        centrifuge-kreport -X ${DB_PATH}/${DB_NAME} ${base}.report > ${base}_KrakenLike.report\n      \"\"\"    \n}\n\nprocess import_taxa {\n    container \"quay.io/biocontainers/biopython:1.78\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple path(genus), path(species), path(krakenLike)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*_import_taxa.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_import_taxa.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_import_taxa.log\" }\n    script:\n      md = parseMetadataFromFileName(genus.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      /scripts/create_import.py -ds ${md.ds} -o ${base}_import_taxa.csv -g ${genus} -sp ${species} -k ${krakenLike}\n      \"\"\"\n}\n\nworkflow step_3TX_class__centrifuge {\n    take: reads\n    main: \n      centrifuge(reads).reports | import_taxa\n}\n\nworkflow {\n  step_3TX_class__centrifuge(getSingleInput())\n}\n"}
{"file_name": "step_3TX_species__kmerfinder.nf", "file_path": "/steps/step_3TX_species__kmerfinder.nf", "language": "nextflow", "id": "step_3TX_species__kmerfinder", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata; csv2map; extractKey;taskMemory } from '../functions/common.nf'\ninclude { param;getInput } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\nKMERFINDER_SPECIES_DIR = param('step_3TX_species__kmerfinder__db')\nKMERFINDER_REFERENCE_DIR = \"${KMERFINDER_SPECIES_DIR}/Bacteria/Fasta/\"\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_species'\ndef METHOD = 'kmerfinder' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef getBacterialReferencePath(checkFile) {\n    try {\n      bacterialReferenceId = csv2map(checkFile, \"\\\\t\").assembly_accBacteria\n      return [ '-', bacterialReferenceId, file(\"${KMERFINDER_REFERENCE_DIR}/${bacterialReferenceId}*.fa\") ] // [riscd (empty), refCode, refPath]\n    } catch(Throwable t) {\n        exit 1, \"could not get bacterialReferenceId from '${checkFile}', exception: ${t.asString()}\"\n    }\n}\n\ndef getCalculatedSpecies(checkFile) {\n    try {\n      return csv2map(checkFile, \"\\\\t\").speciesAssigned\n    } catch(Throwable t) {\n        exit 1, \"could not get speciesAssigned from '${checkFile}', exception: ${t.asString()}\"\n    }\n}\n\nprocess kmerfinder {\n    container \"python:2.7.8\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro -v ${KMERFINDER_SPECIES_DIR}:${KMERFINDER_SPECIES_DIR}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    input:\n      tuple val(riscd_input), path(scaffolds200)\n    output:\n      tuple val(riscd), path(\"${base}_kmerfinder.check\"), emit: check\n      path \"**\"\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*/*.tsv', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.json,*/*.check,*/*.log}', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_kmerfinder.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      inputFile = scaffolds200\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        /scripts/FindTemplate.py -i ${inputFile} -t ${KMERFINDER_SPECIES_DIR}/Bacteria_DB -o ${base}_kmerfinder_bacterial.tsv -x ATGAC -w  > ${base}_kmerfinder.log\n        /scripts/FindTemplate.py -i ${inputFile} -t ${KMERFINDER_SPECIES_DIR}/Viral_DB -o ${base}_kmerfinder_viral.tsv -x ATGAC -w  >> ${base}_kmerfinder.log\n        /scripts/ParseSpeciesFile_newDB.py ${base}_kmerfinder_bacterial.tsv ${KMERFINDER_SPECIES_DIR}\n      \"\"\"\n}\n\nworkflow step_3TX_species__kmerfinder {\n    take: data\n    main:\n      kmerfinder(data);\n    emit:\n      assigned_species = kmerfinder.out.check.map { [ it[0], getCalculatedSpecies(it[1]), getBacterialReferencePath(it[1]) ] }\n}\n\nworkflow {\n    step_3TX_species__kmerfinder(getInput())\n}"}
{"file_name": "step_4AN_AMR__staramr.nf", "file_path": "/steps/step_4AN_AMR__staramr.nf", "language": "nextflow", "id": "step_4AN_AMR__staramr", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;isSpeciesSupported } from '../functions/common.nf'\ninclude { getSingleInput;param } from '../functions/parameters.nf'\ninclude { stepInputs;flattenPath } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4AN_AMR'\ndef METHOD = 'staramr' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef GENUS_ALLOWED = [\n  'campylobacter'\n]\n\ndef POINFINDER_ORGANISM = [\n  campylobacter : \"campylobacter\"\n]\n\ndef getPointfinderParam(gsp, map) {\n try {  \n    def genus_species = gsp ? gsp.toLowerCase() : ''\n    def (genus, species) = genus_species.contains(\"_\") ? genus_species.split('_') : [ genus_species, null ]\n\n    if (map.containsKey(genus)) {\n      return map.get(genus)\n    } \n    exit 1, \"could not find pointfinder param for: ${gsp}\"\n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n\nprocess staramr {\n    container \"ghcr.io/genpat-it/staramr:0.9.1--8fe6b5a239\"\n    containerOptions = \"--user root\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    when:\n      isSpeciesSupported(genus_species, GENUS_ALLOWED, assembly, task.process)\n    input:\n      tuple val(riscd_input), path(assembly)\n      val(genus_species)\n    output:\n      path '**'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'result/{*.tsv,*.xlsx}', saveAs: { filename -> \"${base}_${flattenPath(filename)}\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'result/hits/*', saveAs: { filename -> \"hits/${base}_hits_${flattenPath(filename) -~ /_DS.+/}.fasta\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: 'result/*.txt', saveAs: { filename -> \"${base}_${flattenPath(filename)}\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_staramr\"\n      pointfinder_organism = getPointfinderParam(genus_species, POINFINDER_ORGANISM)\n      \"\"\"\n        sed 's/^>.*/>/g' ${assembly} | awk '{for(i=1;i<=NF;i++){if(\\$i~/^>/){\\$i=\">contig\"++count}}} 1' > ${md.cmp}.fasta\n        staramr search --pointfinder-organism ${pointfinder_organism} -o result ${md.cmp}.fasta\n      \"\"\"\n}\n\nworkflow step_4AN_AMR__staramr {\n    take: \n      assembly\n      genus_species\n    main:\n      staramr(assembly, genus_species)\n}\n\nworkflow {\n    step_4AN_AMR__staramr(getSingleInput(), param('genus_species'))\n}"}
{"file_name": "step_3TX_species__mash.nf", "file_path": "/steps/step_3TX_species__mash.nf", "language": "nextflow", "id": "step_3TX_species__mash", "content": "nextflow.enable.dsl=2\n\ninclude { stepInputs;parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;isCompatibleWithSeqType } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_species'\ndef METHOD = 'mash' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess mash {\n    container \"ghcr.io/genpat-it/mash:2.3--debdd7eb23\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 10.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.tsv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, '']\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_mash\"\n      \"\"\"\n        mash screen -w -p ${task.cpus} /mash/refseq/refseq.genomes.k21s1000.msh ${r1} ${r2} > ${base}_screen.tsv\n        assess_mash_screen.py --gram /mash/BACTpipe/resources/gram_stain.txt ${base}_screen.tsv | tee ${base}_screening_results.tsv\n      \"\"\"\n}\n\nworkflow step_3TX_species__mash {\n    take: \n      reads\n    main:\n      mash(reads)\n}\n\nworkflow {\n    step_3TX_species__mash(getInput())\n}"}
{"file_name": "step_3TX_class__kraken2.nf", "file_path": "/steps/step_3TX_class__kraken2.nf", "language": "nextflow", "id": "step_3TX_class__kraken2", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { isSarsCov2;isPositiveControlSarsCov2;isNegativeControlSarsCov2 } from '../functions/sampletypes'\ninclude { param;getSingleInput;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef KRAKEN2_DB = param('step_3TX_class__kraken2__db')\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_class'\ndef METHOD = 'kraken2' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess kraken2 {\n    container \"quay.io/biocontainers/kraken2:2.1.2--pl5262h7d875b9_0\"\n    containerOptions = \"-v ${KRAKEN2_DB}:${KRAKEN2_DB}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 62.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_kraken.tsv\"), emit: report  \n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_kraken.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.tsv,*.gz}'\n    script:\n      (t1,t2) = reads\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      options = isIlluminaPaired(reads) ? ' --paired ' : ''\n      options += t1.getName().endsWith(\".gz\") ? \" --gzip-compressed \" : \"\"\n      \"\"\"\n        kraken2 --db ${KRAKEN2_DB} --threads 64 ${options} ${t1} ${t2} --output ${base}_kraken.txt --report ${base}_kraken.tsv 2> ${base}_kraken.log\n        gzip ${base}_kraken.txt\n      \"\"\"    \n}\n\nprocess braken2 {\n    container \"quay.io/biocontainers/bracken:2.6.1--py39h7cff6ad_2\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro -v ${KRAKEN2_DB}:${KRAKEN2_DB}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    input:\n      path(kraken_report)\n    output:\n      path '*'\n      tuple val(riscd), path(\"${base}_bracken_genus.tsv\"), emit: genus_report\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*_import_taxa.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_bracken.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.tsv'\n    script:\n      md = parseMetadataFromFileName(kraken_report.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        bracken -d ${KRAKEN2_DB} -i ${base}_kraken.tsv -o ${base}_bracken_species.tsv -w ${base}_bracken_species_report.tsv -l S -t 10 > ${base}_bracken_species.log\n        bracken -d ${KRAKEN2_DB} -i ${base}_kraken.tsv -o ${base}_bracken_genus.tsv -w ${base}_bracken_genus_report.tsv -l G -t 10 > ${base}_bracken_genus.log\n        /scripts/create_import.py -ds ${md.ds} -o ${base}_import_taxa.csv -g ${base}_bracken_genus.tsv -sp ${base}_bracken_species.tsv -k ${kraken_report}\n      \"\"\"    \n}\n\nworkflow step_3TX_class__kraken2 {\n    take: reads\n    main:\n      report = kraken2(reads).report\n      braken2(report)\n     emit:\n       genus_report = braken2.out.genus_report\n}\n\nworkflow {\n  step_3TX_class__kraken2(getSingleInput())\n}\n"}
{"file_name": "step_1PP_hostdepl__bowtie.nf", "file_path": "/steps/step_1PP_hostdepl__bowtie.nf", "language": "nextflow", "id": "step_1PP_hostdepl__bowtie", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common'\ninclude { param;getSingleInput;getHostUnkeyed;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\n\ndef refDir = param('hosts_dir')\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_hostdepl'\ndef METHOD = 'bowtie' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess bowtie2 {\n    container \"ghcr.io/genpat-it/bowtie2:2.1.0--37ad014737\"\n    containerOptions = \"-v ${refDir}:${refDir}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process) \n    input:\n      tuple val(riscd_input), path(reads), val(host)\n    output:\n      path '*'\n      tuple path(\"${outname}.sam\"), val(host), val(type), emit: sam\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [reference:host])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"{*.log,*.json}\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${outname}.cfg\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      outname = \"${base}_vdhost_${host?.replace(\"_\", \"\")}\"\n      type = isIlluminaPaired(reads) ? 'paired' : 'single' \n      if (isIlluminaPaired(reads)) {\n        \"\"\"\n        bowtie2 -p 8 --very-fast -x ${refDir}/${host} -1 ${t1} -2 ${t2} -S ${outname}.sam 2>> ${outname}.log\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        bowtie2 -p 8 --very-fast -x ${refDir}/${host} -U ${t1} -S ${outname}.sam 2>> ${outname}.log\n        \"\"\"      \n      }  \n}\n\nprocess samtools {\n    container 'quay.io/biocontainers/samtools:0.1.19--hf89b575_7'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 18.GB, task.attempt ) }\n    input:\n      tuple path(sam), val(host), val(type)\n    output:\n      tuple val(riscd), path('*.fastq.gz'), val(host), emit: depleted\n      path '*', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"${base}_R*.fastq.gz\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"${base}*.log\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${outname}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      tmpName = \"${base}_tmp\"\n      hostWithoutUnderscore = host?.replace(\"_\", \"\")\n      outname = \"${base}_vdhost_${hostWithoutUnderscore}\"\n      vdhostR1 = \"${base}_R1_vdhost_${hostWithoutUnderscore}.fastq.gz\"\n      vdhostR2 = \"${base}_R2_vdhost_${hostWithoutUnderscore}.fastq.gz\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      if (type == 'paired') {\n        \"\"\"\n\t\t    samtools view -bS ${sam} -@ 16 > ${tmpName}.bam 2>> ${outname}_samtools.log\n\t\t    samtools view -b -f 12 -F 256 ${tmpName}.bam -@ 16 > ${tmpName}_um.bam 2>> ${outname}_samtools.log\n\t\t    samtools sort -n ${tmpName}_um.bam ${tmpName}.sorted -@ 16 2>> ${outname}_samtools.log\n\t\t    samtools bam2fq ${tmpName}.sorted.bam > ${tmpName}.fastq 2>> ${outname}_samtools.log\n        grep '^@.*/1\\$' -A 3 ${tmpName}.fastq | grep -v -- \"^--\\$\" | gzip > ${vdhostR1}\n\t\t    grep '^@.*/2\\$' -A 3 ${tmpName}.fastq | grep -v -- \"^--\\$\" | gzip > ${vdhostR2}   \n        rm ${tmpName}*.*\n\t    \"\"\"\n}     else if (type == 'single') {\n        \"\"\"\n\t\t    samtools view -bS ${sam} -@ 16 > ${tmpName}.bam 2>> ${outname}_samtools.log\n\t\t    samtools view -b -f 4 -F 256 ${tmpName}.bam -@ 16 > ${tmpName}_um.bam 2>> ${outname}_samtools.log\n\t\t    samtools sort -n ${tmpName}_um.bam ${tmpName}.sorted -@ 16 2>> ${outname}_samtools.log\n\t\t    samtools bam2fq ${tmpName}.sorted.bam > ${tmpName}.fastq 2>> ${outname}_samtools.log\n        gzip -c ${tmpName}.fastq > ${vdhostR1} \n        rm ${tmpName}*.*\n        \"\"\"\n      }   \n}\n\nworkflow step_1PP_hostdepl__bowtie {\n    take: \n      trimmedAndHost\n    main:\n      bowtie2(trimmedAndHost)\n      samtools(bowtie2.out.sam)\n    emit:\n      samtools.out.depleted      \n}\n\nworkflow {    \n    input = getSingleInput()\n      .combine(getHostUnkeyed())\n    step_1PP_hostdepl__bowtie(input)    \n}\n"}
{"file_name": "step_3TX_class__kraken.nf", "file_path": "/steps/step_3TX_class__kraken.nf", "language": "nextflow", "id": "step_3TX_class__kraken", "content": "nextflow.enable.dsl=2\n\ninclude { getEmpty;flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { isSarsCov2;isPositiveControlSarsCov2;isNegativeControlSarsCov2;isNGSMG16S } from '../functions/sampletypes'\ninclude { param;isFullOutput;getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef db_kraken=param('step_3TX_class__kraken__db_kraken')\ndef db_bracken=param('step_3TX_class__kraken__db_bracken')\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_class'\ndef METHOD = 'kraken' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess kraken {\n    container \"quay.io/biocontainers/kraken:1.0--pl5.22.0_0\"\n    containerOptions = \"-v ${db_kraken}:${db_kraken}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    when:\n      (isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)) && !(isSarsCov2(riscd_input) || isPositiveControlSarsCov2(riscd_input) || isNegativeControlSarsCov2(riscd_input) || isNGSMG16S(riscd_input))\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_kraken.tsv\"), emit: report  \n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_kraken.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.tsv,*.txt}'\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      options = isIlluminaPaired(reads) ? ' --paired ' : ''\n      options += t1.getName().endsWith(\".gz\") ? \" --gzip-compressed \" : \"\"\n      \"\"\"\n        kraken --db ${db_kraken} --threads 8 --fastq-input ${options} ${t1} ${t2} > ${base}_kraken.txt 2> ${base}_kraken.log\n        kraken-report --db ${db_kraken} ${base}_kraken.txt > ${base}_kraken.tsv\n        ${isFullOutput()} || rm *.txt\n      \"\"\"    \n}\n\nprocess braken {\n    container \"quay.io/biocontainers/bracken:1.0.0--1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro -v ${db_bracken}:${db_bracken}:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    input:\n      path(kraken_report)\n    output:\n      path '*'\n      tuple val(riscd), path(\"${base}_bracken_genus.tsv\"), emit: genus_report\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*_import_taxa.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_bracken.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.tsv'\n    script:\n      md = parseMetadataFromFileName(kraken_report.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        est_abundance.py -i ${kraken_report} -k ${db_bracken} -o ${base}_bracken_genus.tsv -l G -t 10 > ${base}_bracken_genus.log\n        est_abundance.py -i ${kraken_report} -k ${db_bracken} -o ${base}_bracken_species.tsv -l S -t 10 > ${base}_bracken_species.log\n        /scripts/create_import.py -ds ${md.ds} -o ${base}_import_taxa.csv -g ${base}_bracken_genus.tsv -sp ${base}_bracken_species.tsv -k ${kraken_report}\n      \"\"\"    \n}\n\nworkflow step_3TX_class__kraken {\n    take: reads\n    main:\n      report = kraken(reads).report \n      braken(report)\n     emit:\n       genus_report = braken.out.genus_report\n}\n\nworkflow {\n  step_3TX_class__kraken(getSingleInput())\n}\n"}
{"file_name": "step_1PP_trimming__fastp.nf", "file_path": "/steps/step_1PP_trimming__fastp.nf", "language": "nextflow", "id": "step_1PP_trimming__fastp", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;taskTime } from '../functions/common.nf'\ninclude { getInput;isIonTorrent;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd;extractKey } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_trimming'\ndef METHOD = 'fastp' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess fastp {\n    container \"ghcr.io/genpat-it/fastp:0.23.1--e4ac3df4c5\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    time { taskTime( 10.m, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)    \n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      tuple val(riscd), path('*.fastq.gz'), emit: trimmed\n      path '{*.sh,*.log}', hidden: true\n       afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n       publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.gz'\n       publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.json,*.html}'\n       publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n       publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n          fastp --in1 ${r1} --out1 ${base}_R1.fastq.gz --in2 ${r2} --out2 ${base}_R2.fastq.gz \\\n          --unpaired1 ${base}_unpaired.fastq.gz --unpaired2 ${base}_unpaired.fastq.gz \\\n          --json ${base}_summary.json --html ${base}_summary.html --thread 8        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n          fastp --in1 ${r1} --out1 ${base}_R1.fastq.gz  \\\n          --json ${base}_summary.json --html ${base}_summary.html --thread 8        \n        \"\"\"        \n      }\n}\n\nprocess sample_reads_check {\n    container \"quay.io/biocontainers/biopython:1.78\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 14.GB, task.attempt ) }\n    time { taskTime( 15.m, task.attempt ) }    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(_), path(reads)\n      tuple val(_), path(trimmed)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*SRC_raw.*[kg]'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*SRC_raw.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*SRC_treads.*[kg]'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '{*SRC_treads.csv,*_readsCheck.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_readscheck.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      (t1,t2,u) = (trimmed instanceof java.util.Collection) ? trimmed : [trimmed, null, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n        /scripts/SampleReadsCheck.py -n $base -R1 $r1 -R2 $r2 -T1 $t1 -T2 $t2 -U $u > ${base}_SRC_raw.log\n        cat ${base}_SRC_raw.log >\t${base}_SRC_treads.log;\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        /scripts/SampleReadsCheck_ionTorrent.py -n $base -R1 $r1 -T1 $t1 > ${base}_SRC_raw.log\n        \"\"\"\n      }      \n}\n\nprocess fastqc {\n    container 'biocontainers/fastqc:v0.11.5_cv4'\n    memory { taskMemory( 400.MB, task.attempt ) }\n    time { taskTime( 5.m, task.attempt ) }    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '{*.zip,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_fastqc.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads[0].getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      fastqc $reads > \"${base}_fastqc.log\" 2>&1\n      \"\"\"\n}\n\nworkflow step_1PP_trimming__fastp {\n    take: \n      rawreads\n    main:\n      trimmed = fastp(rawreads).trimmed\n\n      fastqc(trimmed)\n      readsCheckInput = rawreads.cross(trimmed) { extractKey(it) }.multiMap { \n        rawreads: it[0]\n        trimmed: it[1]\n      }      \n      sample_reads_check(readsCheckInput.rawreads, readsCheckInput.trimmed)\n\n    emit:\n      trimmed      \n}\n\nworkflow {\n    step_1PP_trimming__fastp(getInput())\n}"}
{"file_name": "step_1PP_filtering__minimap2.nf", "file_path": "/steps/step_1PP_filtering__minimap2.nf", "language": "nextflow", "id": "step_1PP_filtering__minimap2", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;stepInputs;extractKey } from '../functions/common.nf'\ninclude { getSingleInput;isCompatibleWithSeqType;getReference;getRisCd} from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_filtering'\ndef METHOD = 'minimap2' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess minimap2 {\n    container \"quay.io/biocontainers/minimap2:2.26--he4a0461_1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    cpus { [16, params.max_cpus as int].min() }\n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(referencePath)\n    output:\n      path '*'\n      tuple path(\"${base}.sam\"), val(reference), path(referencePath), emit: sam\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference, seq_type: 'nanopore'])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base_ref}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      base_ref = \"${base}_${reference}\"       \n      \"\"\"\n      minimap2 -ax map-ont ${referencePath} ${reads} -t ${task.cpus} -o ${base}.sam\n      \"\"\"\n}\n\nprocess samtools {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 18.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }\n    input:\n      tuple path(sam), val(reference), path(referencePath)\n    output:\n      tuple val(riscd), path('*.fastq.gz'), emit: filtered\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"*.gz\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*_sorted.bam*,*.vcf}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_samtools.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      base_ref = \"${base}_${reference}\"\n      tmpName = \"${base}_tmp\"\n      riscd = getRisCd(md, ex, STEP, METHOD)   \n      \"\"\"\n\t\t    samtools view -bS ${sam} -@ ${task.cpus} -o ${tmpName}.bam \n        samtools view -b -F 4 ${tmpName}.bam  -@ ${task.cpus} -o ${tmpName}_map.bam  2>> ${base_ref}_samtools.log\n        samtools sort ${tmpName}_map.bam -o ${tmpName}.sorted.bam -@ ${task.cpus} 2>> ${base_ref}_samtools.log \n        samtools bam2fq ${tmpName}.sorted.bam | gzip > ${base_ref}.fastq.gz 2>> ${base_ref}_samtools.log\n        samtools index ${tmpName}.sorted.bam 2>> ${base_ref}_samtools.log\n\t    \"\"\"\n}\n\nworkflow step_1PP_filtering__minimap2 {\n    take: \n      reads\n      reference\n    main:\n      minimap2(reads, reference).sam | samtools\n    emit:\n      samtools.out.filtered  \n}\n\nworkflow {\n    getSingleInput().cross(getReference('fa')) { extractKey(it) }\n      .multiMap { \n          reads: it[0]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n    step_1PP_filtering__minimap2(input.reads, input.refs)\n}"}
{"file_name": "step_2MG_denovo__metaspades.nf", "file_path": "/steps/step_2MG_denovo__metaspades.nf", "language": "nextflow", "id": "step_2MG_denovo__metaspades", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2MG_denovo'\ndef METHOD = 'metaspades' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess metaspades {\n    container 'quay.io/biocontainers/spades:3.11.1--py27_zlib1.2.8_0'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 24.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }       \n    when:\n      isCompatibleWithSeqType(reads, 'illumina_paired', task.process)    \n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_spades_scaffolds.fasta\"), emit: scaffolds\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_spades.cfg\" }\n    script:\n      (t1,t2) = reads\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      spades.py --meta -k 21,33,55,77 -t ${task.cpus}  -o spades -1 ${t1} -2 ${t2} > ${base}_spades.log 2>&1 \n\t\t\tmv spades/scaffolds.fasta ${base}_spades_scaffolds.fasta\n      mv spades/contigs.fasta ${base}_spades_contigs.fasta\n      \"\"\"\n}\n\nprocess assembly_filter {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      path(scaffolds)\n    output:\n      tuple val(riscd), path(\"${base}_spades_scaffolds_L200.fasta\"), emit: fasta\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_assemblyfilter.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      /scripts/AssemblyFilter.py -n ${base} -f ${scaffolds} -l 200 -c 0 ;\n      \"\"\"\n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple val(_), path(l200)\n    output:\n      path '*_quast.*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_quast.cfg\" }\n    script:\n      md = parseMetadataFromFileName(l200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${l200} > ${base}_quast.log ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}_quast.csv ; \n      \"\"\"\n}\n\nworkflow step_2MG_denovo__metaspades{\n    take: data\n    main:\n      metaspades(data)\n      assembly_filter(metaspades.out.scaffolds).fasta | quast\n    emit:\n      assembled = assembly_filter.out.fasta\n}\n\nworkflow {\n    step_2MG_denovo__metaspades(getSingleInput())\n}\n\n"}
{"file_name": "step_1PP_generated__fasta2fastq.nf", "file_path": "/steps/step_1PP_generated__fasta2fastq.nf", "language": "nextflow", "id": "step_1PP_generated__fasta2fastq", "content": "nextflow.enable.dsl=2\n\ninclude { stepInputs;parseMetadataFromFileName;executionMetadata;flattenPath } from '../functions/common.nf'\ninclude { getSingleInput } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_generated'\ndef METHOD = 'fasta2fastq' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess fasta2fastq {\n    container \"quay.io/biocontainers/biopython:1.78\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(assembly)\n    output:\n      path '**'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'results/*.gz', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_fasta2fastq\"\n      \"\"\"\n        (mkdir input && mkdir results && cd input && ln -s ../${assembly} ${base}.fasta) \n        /scripts/fasta2fastq.py --fasta input --fastq results       \n        gzip results/*.fastq\n      \"\"\"\n}\n\nworkflow step_1PP_generated__fasta2fastq {\n    take: \n      reads\n    main:\n      fasta2fastq(reads)\n}\n\nworkflow {\n    step_1PP_generated__fasta2fastq(getSingleInput())\n}"}
{"file_name": "step_2AS_denovo__plasmidspades.nf", "file_path": "/steps/step_2AS_denovo__plasmidspades.nf", "language": "nextflow", "id": "step_2AS_denovo__plasmidspades", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'plasmidspades' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess plasmid_spades {\n    container 'quay.io/biocontainers/spades:3.11.1--py27_zlib1.2.8_0'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 24.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }       \n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_scaffolds.fasta\"), emit: scaffolds\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n        spades.py --plasmid --only-assembler --careful -k 21,33,55,77 -t ${task.cpus}  -o spades -1 ${t1} -2 ${t2}\n        mv spades/scaffolds.fasta ${base}_scaffolds.fasta ;\n        mv spades/contigs.fasta ${base}_contigs.fasta ;\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n          spades.py --plasmid --iontorrent --careful -o spades -s $t1 -t ${task.cpus}  \n          mv spades/scaffolds.fasta ${base}_scaffolds.fasta ;\n          mv spades/contigs.fasta ${base}_contigs.fasta ;\n        \"\"\"        \n      }          \n}\n\nprocess assembly_filter {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      path(scaffolds)\n    output:\n      tuple val(riscd), path(\"${base}_${METHOD}_scaffolds_L200.fasta\"), emit: fasta\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_assemblyfilter.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_assemblyfilter.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      /scripts/AssemblyFilterPlasmids.py \\\n        -o ${base}_${METHOD}_scaffolds_L200.fasta -oc ${base}_${METHOD}_scaffolds_L200.check \\\n        -f ${scaffolds} -l 200 -c 0\n      \"\"\"\n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple val(_), path(l200)\n    output:\n      path '*_quast.*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}_quast.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_quast.cfg\" }\n    script:\n      md = parseMetadataFromFileName(l200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${l200}\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}_quast.csv\n      \"\"\"\n}\n\nworkflow step_2AS_denovo__plasmidspades {\n    take: data\n    main:\n      plasmid_spades(data)\n      assembly_filter(plasmid_spades.out.scaffolds).fasta | quast\n    emit:\n      assembled = assembly_filter.out.fasta\n}\n\nworkflow {\n    step_2AS_denovo__plasmidspades(getSingleInput())\n}\n\n"}
{"file_name": "step_4TY_flaA__flaA.nf", "file_path": "/steps/step_4TY_flaA__flaA.nf", "language": "nextflow", "id": "step_4TY_flaA__flaA", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;isSpeciesSupported } from '../functions/common.nf'\ninclude { getInput;param } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_flaA'\ndef METHOD = 'flaA' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef MLST_SCHEMA_NAME = 'flaA'\n\ndef GENUS_ALLOWED = [\n  'campylobacter'\n]\n\nprocess mlst_flaa {\n    container \"ghcr.io/genpat-it/mlst-w-db:2.23.0--60b8b2e3dd_231219.124455\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }       \n    when:\n      isSpeciesSupported(genus_species, GENUS_ALLOWED, scaffolds200, task.process)\n    input:\n      tuple val(riscd_input), path(scaffolds200)\n      val genus_species\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.csv,*.tsv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_mlst_flaa.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n        mlst --threads ${task.cpus} --scheme ${MLST_SCHEMA_NAME} ${scaffolds200} > ${base}_mbn_flaA.tsv 2> ${base}_mbn_flaA.log\n        /scripts/process-mlst-result.py ${base}_mbn_flaA.tsv ${base}_mbn_flaa_cc.csv /NGStools/mlst/db/pubmlst\n      \"\"\"\n}\n\nworkflow step_4TY_flaA__flaA {\n    take: \n      assembly\n      genus_species\n    main:\n      mlst_flaa(assembly, genus_species)\n}\n\nworkflow {\n    step_4TY_flaA__flaA(getInput(), param('genus_species'))\n}"}
{"file_name": "step_2AS_denovo__spades.nf", "file_path": "/steps/step_2AS_denovo__spades.nf", "language": "nextflow", "id": "step_2AS_denovo__spades", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'spades' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess denovo {\n    container 'quay.io/biocontainers/spades:3.11.1--py27_zlib1.2.8_0'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 24.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }       \n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path(\"${base}_spades_scaffolds.fasta\"), emit: scaffolds\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_spades.cfg\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      if (isIlluminaPaired(reads)) { \n        \"\"\"\n        spades.py --only-assembler --careful -k 21,33,55,77 -t ${task.cpus}  -o spades -1 ${t1} -2 ${t2} > ${base}_spades.log 2>&1 ;\n        mv spades/scaffolds.fasta ${base}_spades_scaffolds.fasta ;\n        mv spades/contigs.fasta ${base}_spades_contigs.fasta ;\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n          spades.py --iontorrent --careful -o spades -s $t1 -t ${task.cpus}  > ${base}_spades.log 2>&1\n          mv spades/scaffolds.fasta ${base}_spades_scaffolds.fasta ;\n          mv spades/contigs.fasta ${base}_spades_contigs.fasta ;\n        \"\"\"        \n      }          \n}\n\nprocess assembly_filter {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 3.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      path(scaffolds)\n    output:\n      tuple val(riscd), path(\"${base}_${METHOD}_scaffolds_L200.fasta\"), emit: fasta\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_assemblyfilter.cfg\" }\n    script:\n      md = parseMetadataFromFileName(scaffolds.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      /scripts/AssemblyFilter.py -n ${base} -f ${scaffolds} -l 200 -c 0 ;\n      \"\"\"\n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple val(_), path(l200)\n    output:\n      path '*_quast.*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_quast.cfg\" }\n    script:\n      md = parseMetadataFromFileName(l200.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${l200} > ${base}_quast.log ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}_quast.csv ; \n      \"\"\"\n}\n\nworkflow step_2AS_denovo__spades {\n    take: data\n    main:\n      denovo(data)\n      assembly_filter(denovo.out.scaffolds).fasta | quast\n    emit:\n      assembled = assembly_filter.out.fasta\n}\n\nworkflow {\n    step_2AS_denovo__spades(getSingleInput())\n}\n\n"}
{"file_name": "step_4TY_lineage__westnile.nf", "file_path": "/steps/step_4TY_lineage__westnile.nf", "language": "nextflow", "id": "step_4TY_lineage__westnile", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;_getSingleReference } from '../functions/parameters.nf'\ninclude { stepInputs;csv2map;getRisCd;parseRISCD } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_lineage'\ndef METHOD = 'westnile' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nSTEP_ASSETS_DIR = \"${params.assets_dir}/${ENTRYPOINT}\"\n\ndef HCOV_THRESHOLD = params.step_4TY_lineage__westnile___threshold\n\ndef WESTNILE_LINEAGE_REFERENCES_PATH = \"${STEP_ASSETS_DIR}/westnile_lineage_references.json\"\n\nWESTNILE_LINEAGE_REFERENCES = new groovy.json.JsonSlurper().parseText(file(WESTNILE_LINEAGE_REFERENCES_PATH).text)\n\n\ndef getReferenceForLineage(lineageFile) {\n   try {        \n      def lineage = csv2map(lineageFile, \",\").lineage\n      assert lineage\n      def refData = WESTNILE_LINEAGE_REFERENCES.find { it.lineage == lineage }\n      assert refData\n      return [ refData.ref_riscd, refData.ref_code, \"${STEP_ASSETS_DIR}/${refData.ref_path}\" ]\n   } catch(Throwable t) {\n       exit 1, \"unexpected exception: ${t.asString()}\"\n   }   \n}\n\ndef isValidLineage(lineageResult) {\n   try {        \n      def lineage = csv2map(lineageResult[1], \",\").lineage   \n      def md = parseMetadataFromFileName(lineageResult[1].getName())\n      def notAssigned =  lineage == 'NA'\n      def notDetermined = lineage == 'ND'\n      if (notAssigned) {\n        log.warn \"[${md.cmp}] Lineage not assigned\"\n      } else if (notDetermined) {\n        log.warn \"[${md.cmp}] Lineage not determined\"\n      }\n      return !notAssigned && !notDetermined\n   } catch(Throwable t) {\n       exit 1, \"unexpected exception: ${t.asString()}\"\n   }   \n}\n \ndef getWNVReferences() {\n   try {        \n      references = Channel.fromList(WESTNILE_LINEAGE_REFERENCES.collect { [ it.ref_riscd, it.ref_code, \"${STEP_ASSETS_DIR}/${it.ref_path}\" ] } )\n   } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n   }   \n}\n\nprocess snippy {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    maxForks 4\n    when:\n      reference_path && reference_path.exists() && !reference_path.empty()\n    input:\n      tuple val(riscd_input), path(reads), val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path \"**/${base_ref}*\"\n      path '*'\n      tuple val(riscd), val(reference), path(\"snippy/${base_ref}.bam\"),  emit: bam  \n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${METHOD}_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      if (r2) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --R1 ${r1} --R2 ${r2} --outdir snippy --prefix ${base_ref} --quiet --tmpdir tmp  &> ${base_ref}.log\n        \n        \"\"\"\n      } else {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --ctgs ${r1} --outdir snippy --prefix ${base_ref} --quiet --tmpdir tmp  &> ${base_ref}.log\n        \"\"\"\n      }\n}\n\nprocess samtools_depth {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 500.MB, task.attempt ) }\n    input:\n      tuple val(riscd_input), val(reference), path(bam)\n    output:\n      path '*'\n      tuple val(riscd), path(\"${base_ref}.coverage\"), emit: coverage\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      lineage = WESTNILE_LINEAGE_REFERENCES.find { it.ref_code == reference }.lineage\n      assert lineage\n      base_ref = \"${base}_${METHOD}_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)\n      \"\"\"\n      samtools depth -a ${bam} | awk '{ if (\\$3!=0) c++;s+=\\$3}{h++} END { if (c!=0) print s/c; else print 0;if (h!=0) print c/h; else print 0 }' > ${base_ref}.coverage\n\t    echo \"${reference}\" >> ${base_ref}.coverage\n\t    echo \"${lineage}\" >> ${base_ref}.coverage\n      \"\"\"\n}\n\nprocess lineage_selection {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    input:\n      tuple val(riscd_input), path(covfile)\n    output:\n      path '*'\n      tuple val(riscd), path(\"${base}.csv\"), emit: lineage, optional: true\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: 'errors.log', saveAs: { \"${base}.err\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.csv'\n    script:\n      coverage_file = (covfile instanceof java.util.Collection) ? covfile.flatten()[0] : covfile\n      md = parseMetadataFromFileName(coverage_file.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}_lineage\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        for f in  ${covfile}; do cat \\${f}  | tr '\\n' ',' | cut -d ',' -f 2,3,4 ; done | sort -nr > ${base}_summary.csv\n        /scripts/select_lineage.py --file_list ${covfile} --threshold ${HCOV_THRESHOLD} --output ${base}.csv &> ${base}.log\n\t    \"\"\"\n}\n\nworkflow step_4TY_lineage__westnile {\n    take: \n      reads\n    main:\n      references = getWNVReferences()\n      comb = reads.combine(references)\n      bam = snippy(comb).bam\n      coverages = samtools_depth(bam).coverage.groupTuple()\n      lineage = lineage_selection(coverages).lineage \n    emit:\n      lineage = lineage.filter { isValidLineage(it) }\n}\n\nworkflow {  \n    step_4TY_lineage__westnile(getSingleInput())\n}"}
{"file_name": "step_4AN_AMR__resfinder.nf", "file_path": "/steps/step_4AN_AMR__resfinder.nf", "language": "nextflow", "id": "step_4AN_AMR__resfinder", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;param;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4AN_AMR'\ndef METHOD = 'resfinder'\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess resfinder {\n    container \"ghcr.io/genpat-it/resfinder:4.1.5--858219071c\"\n    containerOptions = \"-u 0:0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      val genus_species\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.txt,*.fsa,*_format_*.json}', saveAs: { f -> \"${base}_${f}\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, '']\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      species = genus_species.replace('_', ' ').toLowerCase()\n      \"\"\"\n        run_resfinder.py -o ./ -s \"${species}\" -l 0.6 -t 0.9 --acquired --db_path_res /resfinder/db_resfinder -ifq ${r1} ${r2} \n      \"\"\"      \n}\n\nworkflow step_4AN_AMR__resfinder {\n    take: \n      reads\n      genus_species\n    main:\n      resfinder(reads, genus_species);\n}\n\nworkflow {\n    step_4AN_AMR__resfinder(getSingleInput(),param('genus_species'))\n}\n"}
{"file_name": "step_4TY_plasmid__mobsuite.nf", "file_path": "/steps/step_4TY_plasmid__mobsuite.nf", "language": "nextflow", "id": "step_4TY_plasmid__mobsuite", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;param;isCompatibleWithSeqType } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_plasmid'\ndef METHOD = 'mobsuite'\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess mobsuite {\n    container \"ghcr.io/genpat-it/mobsuite:3.1.4--c2c728b1a2\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    cpus { [32, params.max_cpus as int].min() }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(assembly)\n    output:\n      path '**'\n      tuple val(riscd), path(\"output/*.fasta\"), emit: plasmids, optional: true\n      path '{*.sh,*.log}', hidden: true \n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'output/*', saveAs: { f -> \"${base}_${flattenPath(f)}\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      /usr/local/bin/_entrypoint.sh mob_recon -s ${base} -o output -i ${assembly} -n ${task.cpus} --force --unicycler_contigs --run_overhang\n      \"\"\"      \n}\n\nworkflow step_4TY_plasmid__mobsuite {\n    take: \n      reads\n    main:\n      plasmids = mobsuite(reads).plasmids\n    emit:\n      plasmids = plasmids\n}\n\nworkflow {\n    step_4TY_plasmid__mobsuite(getSingleInput())\n}\n"}
{"file_name": "step_2AS_mapping__medaka.nf", "file_path": "/steps/step_2AS_mapping__medaka.nf", "language": "nextflow", "id": "step_2AS_mapping__medaka", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;extractKey;taskMemory;stepInputs;getRisCd;extractDsRef } from '../functions/common.nf'\ninclude { getSingleInput;getReference;isCompatibleWithSeqType } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_mapping'\ndef METHOD = 'medaka' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess medaka {\n    container \"quay.io/biocontainers/medaka:1.8.0--py38hdaa7744_0\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    // memory { taskMemory( 8.GB, task.attempt ) }\n    cpus { [32, params.max_cpus as int].min() }\n    when:\n      reference_path && reference_path.exists() && !reference_path.empty() && (isCompatibleWithSeqType(reads, ['nanopore'], task.process))\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path '*_input.json'\n      tuple val(riscd), path(\"${base_ref}.fasta\"), emit: consensus    \n      tuple path(\"${base_ref}.bam\"), val(reference), emit: bam    \n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"*.fasta\"    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"*.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base_ref}.log\" }    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${METHOD}_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        #trap \"find -name \"*.sam\" -delete\" EXIT\n        cp ${reference_path} _${reference_path}\n        medaka_consensus \\\n            -t ${task.cpus} \\\n            -i ${reads} \\\n            -d _${reference_path} \\\n            -o ./\n        mv consensus.fasta ${base_ref}.fasta\n        mv calls_to_draft.bam ${base_ref}.bam        \n      \"\"\"      \n}\n\n\nprocess coverage_minmax {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      path '*.csv'\n      path '*.sh', hidden: true\n      tuple path(\"${base_ref}_samtools_depth.txt\"), val(reference), emit: coverage_depth  \n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage_minmax.csv\"), emit: coverage_extra\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_minmax.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools view -F 4 -c ${bam} > samtools_view.txt\n      samtools depth ${bam} > ${base_ref}_samtools_depth.txt\n      /scripts/coverage_minmax.py ${md.cmp} ${md.ds} samtools_view.txt ${base_ref}_samtools_depth.txt ${base_ref}_import_coverage_minmax.csv\n\t    \"\"\"\n}\n\nprocess samtools_depth {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      tuple path(\"${base_ref}.coverage\"), val(reference), emit: coverage\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools_depth.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools depth -a ${bam} | awk '{ if (\\$3!=0) c++;s+=\\$3}{h++} END { if (c!=0) print s/c; else print 0;if (h!=0) print c/h; else print 0 }' > ${base_ref}.coverage\n\t    \"\"\"\n}\n\nprocess coverage_check {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple path(coverage), path(consensus), val(reference)\n      val(context)\n    output:\n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage.csv\"), emit: coverage_basic\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.csv,*.check}'      \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check.cfg\" }\n    script:\n      //TODO fix output folder\n      md = parseMetadataFromFileName(consensus.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${context}_${reference}\"\n      coverages= coverage.toRealPath().toFile().readLines()\n      \"\"\"\n      /scripts/coverage.py ${coverage} ${consensus} ${reference} ${md.cmp} ${md.ds.substring(2)} ${base_ref}.check ${base_ref}_import_coverage.csv \n\t    \"\"\"\n}\n\nprocess coverage_check_merge {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  input:\n    tuple val(key), val(reference), path(covMinMax), path(covBasic)\n    val(method)\n  output:\n    tuple val(md.ds), path(\"${base_ref}_import_coverage_merged.csv\"), emit: coverage_merged\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check_merge.cfg\" }\n  script:\n    md = parseMetadataFromFileName(covMinMax.getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    base_ref = \"${base}_${method}_${reference}\"\n    \"\"\"\n    paste -d, ${covMinMax} ${covBasic} | cut -d, -f1,2,3,4,5,8,9,10,11,12,13 > ${base_ref}_import_coverage_merged.csv\n    \"\"\"  \n}\n\nprocess coverage_plot {\n    container \"quay.io/biocontainers/matplotlib:3.1.2\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      tuple path(coverage_depth), val(reference)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.png'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_plot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(coverage_depth.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      \"\"\"\n      /scripts/coverage_plot.py ${coverage_depth} ${base_ref}_coverage_plot.png\n\t    \"\"\"\n}\n\nworkflow step_2AS_mapping__medaka {\n  take: \n    reads\n    reference \n  main:\n    medaka(reads, reference)\n\n    coverage_minmax(medaka.out.bam, METHOD)\n    coverage_minmax.out.coverage_depth | coverage_plot\n    \n    coverage = samtools_depth(medaka.out.bam, METHOD).coverage\n    coverage.cross(medaka.out.consensus) { extractDsRef(it) }.map { \n        return [ it[0][0], it[1][1], it[0][1] ]\n    }.set { coverageRefAndConsensus }\n    coverageBasic = coverage_check(coverageRefAndConsensus, METHOD).coverage_basic\n\n    crossedChecks = coverage_minmax.out.coverage_extra.cross(coverageBasic) { it[0] + \"-\" + it[1] }\n    .map { [ it[0][0], it[0][1], it[0][2], it[1][2] ] }\n    coverage_check_merge(crossedChecks, METHOD)\n\n  emit:\n    consensus = medaka.out.consensus\n}\n\nworkflow {\n    getSingleInput().cross(getReference('fa')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n    step_2AS_mapping__medaka(input.reads, input.refs)\n}"}
{"file_name": "step_1PP_filtering__bowtie.nf", "file_path": "/steps/step_1PP_filtering__bowtie.nf", "language": "nextflow", "id": "step_1PP_filtering__bowtie", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;taskMemory;extractKey } from '../functions/common'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\ninclude { getSingleInput;getReference;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_filtering'\ndef METHOD = 'bowtie' \n\nprocess bowtie2 {\n    container \"ghcr.io/genpat-it/bowtie2:2.1.0--37ad014737\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    when:\n      referencePath && referencePath.exists() && !referencePath.empty() && (isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process))\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(referencePath)\n    output:\n      path '*'\n      tuple path(\"${base_ref}.sam\"), val(reference), path(referencePath), val(type), emit: sam\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\" \n      type = isIlluminaPaired(reads) ? 'paired' : 'single' \n      if (isIlluminaPaired(reads)) {\n        \"\"\"\n        bowtie2-build ${referencePath} ${reference}\n        bowtie2 -p 8 --very-fast -x ${reference} -1 ${t1} -2 ${t2} -S ${base_ref}.sam 2>> ${base_ref}.log\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        bowtie2-build ${referencePath} ${reference}\n        bowtie2 -p 8 --very-fast -x ${reference} -U ${t1} -S ${base_ref}.sam 2>> ${base_ref}.log\n        \"\"\"      \n      }     \n}\n\nprocess samtools {\n    container 'quay.io/biocontainers/samtools:0.1.19--hf89b575_7'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 18.GB, task.attempt ) }\n    input:\n      tuple path(sam), val(reference), path(referencePath), val(type)\n    output:\n      tuple val(riscd), path('*.fastq.gz'), emit: filtered\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"${base}_R*.fastq.gz\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*_sorted.bam*,*.vcf}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      tmpName = \"${base}_tmp\"\n      filteredR1 = \"${base}_R1_bowtie_${reference}.fastq.gz\"\n      filteredR2 = \"${base}_R2_bowtie_${reference}.fastq.gz\"\n      riscd = getRisCd(md, ex, STEP, METHOD)\n      if (type == 'paired') {\n        \"\"\"\n\t\t\t  samtools view -bS ${sam} -@ 16 > ${tmpName}.bam 2>> ${base_ref}_samtools.log\n\t\t    samtools view -b -f 2 ${tmpName}.bam > ${tmpName}_map.bam 2>> ${base_ref}_samtools.log\n\t\t    samtools sort -n ${tmpName}_map.bam ${tmpName}.sorted -@ 16 2>> ${base_ref}_samtools.log\n\t\t    samtools bam2fq ${tmpName}.sorted.bam > ${tmpName}.fastq 2>> ${base_ref}_samtools.log\n\t\t\t  grep '^@.*/1\\$' -A 3 ${tmpName}.fastq | grep -v -- \"^--\\$\" | gzip > ${filteredR1}\n\t\t    grep '^@.*/2\\$' -A 3 ${tmpName}.fastq | grep -v -- \"^--\\$\" | gzip > ${filteredR2}   \n\t\t\t  rm ${tmpName}*.*\n        \"\"\"\n}     else if (type == 'single') {\n        \"\"\"\n\t\t    samtools view -bS ${sam} -@ 16 > ${tmpName}.bam 2>> ${base_ref}_samtools.log\n\t\t    samtools view -b -F 4 ${tmpName}.bam > ${tmpName}_map.bam 2>> ${base_ref}_samtools.log\n\t\t    samtools sort -n ${tmpName}_map.bam ${tmpName}.sorted -@ 16 2>> ${base_ref}_samtools.log\n\t\t    samtools bam2fq ${tmpName}.sorted.bam > ${tmpName}.fastq 2>> ${base_ref}_samtools.log\n        gzip -c ${tmpName}.fastq > ${filteredR1} \n        rm ${tmpName}*.*\n        \"\"\"\n      }      \n}\n\nworkflow step_1PP_filtering__bowtie {\n    take: \n      reads\n      reference \n    main:\n      bowtie2(reads, reference) //[ refId, refPath ]\n      samtools(bowtie2.out.sam)\n    emit:\n      samtools.out.filtered      \n}\n\n\nworkflow {\n    getSingleInput().cross(getReference('fa')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }  \n    step_1PP_filtering__bowtie(input.reads, input.refs)\n}\n\n"}
{"file_name": "step_4TY_cgMLST__chewbbaca.nf", "file_path": "/steps/step_4TY_cgMLST__chewbbaca.nf", "language": "nextflow", "id": "step_4TY_cgMLST__chewbbaca", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;taskTime } from '../functions/common.nf'\ninclude { getInput;param;optionalOrDefault;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\nSPECIES_SCHEMA = [\n  listeria_monocytogenes : ['l_mono_chewie_1748_220623'],\n  escherichia_coli : ['e_coli_chewie_2360_210531'],\n  salmonella_enterica : ['s_enterica_chewie_3255_210531']\n]\n\nSCHEMAS = [\n  l_mono_chewie_1748_220623 : \"/schemas/Listeria_monocytogenes_Pasteur_cgMLST_2022-06-23T18_03_54.613576.zip\",\n  e_coli_chewie_2360_210531 : \"/schemas/Escherichia_coli_INNUENDO_wgMLST_2021-05-31T14_24_05.304225.zip\",\n  s_enterica_chewie_3255_210531 : \"/schemas/Salmonella_enterica_INNUENDO_cgMLST_2021-05-31T20_28_21.350919.zip\"\n]\n\nCHEWBBACA_SINGLE_END_PARAMS = [\n  'l_mono_chewie_1748_220623': ' --minimum-length 144 --st 0.1 --bsr 0.6 ',\n  'e_coli_chewie_2360_210531': ' --minimum-length 0 --st 0.01 --bsr 0.6 --genes-list /schemas/Escherichia_coli_INNUENDO_cgMLST_EFSA_filterlist.txt ',\n  's_enterica_chewie_3255_210531': ' --minimum-length 0 --st 0.01 --bsr 0.6 --genes-list /schemas/Salmonella_enterica_INNUENDO_cgMLST_EFSA_filterlist.txt '\n]\n\nCHEWBBACA_PAIRED_END_PARAMS = [\n  'l_mono_chewie_1748_220623': ' --minimum-length 144 ',\n  'e_coli_chewie_2360_210531': ' --minimum-length 0 --genes-list /schemas/Escherichia_coli_INNUENDO_cgMLST_EFSA_filterlist.txt ',\n  's_enterica_chewie_3255_210531': ' --minimum-length 0 --genes-list /schemas/Salmonella_enterica_INNUENDO_cgMLST_EFSA_filterlist.txt '\n]\n\ndef getExtraParams(schema) {\n  try {   \n    if (isIonTorrent(null)) {\n      if (CHEWBBACA_SINGLE_END_PARAMS.containsKey(schema)) {\n        return CHEWBBACA_SINGLE_END_PARAMS[schema]\n      }\n    } else {\n      if (CHEWBBACA_PAIRED_END_PARAMS.containsKey(schema)) {\n        return CHEWBBACA_PAIRED_END_PARAMS[schema]\n      }\n    }        \n    return ''\n  } catch(Throwable t) {\n      exit 1, \"getExtraParams, unexpected exception: ${t.asString()}\"\n  } \n}\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_cgMLST'\ndef METHOD = 'chewbbaca' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef getSchema(gsp, schema) {\n  try {  \n    def genus_species = gsp ? gsp.toLowerCase() : ''\n    def (genus, species) = genus_species.contains(\"_\") ? genus_species.split('_') : [ genus_species, null ]\n\n    def allowedSchemas = []\n    if (SPECIES_SCHEMA.containsKey(genus_species)) {\n      allowedSchemas =  SPECIES_SCHEMA.get(genus_species) \n    } else if (SPECIES_SCHEMA.containsKey(genus)) {\n      allowedSchemas = SPECIES_SCHEMA.get(genus)\n    }\n    if (!allowedSchemas) {\n      log.debug \"no compatible schemas found for genus_species: ${genus_species}\"\n      return null\n    }\n    if (!schema) {\n      return allowedSchemas[0] //at this stage, use the first allowed schema\n    }\n    if (allowedSchemas.contains(schema)) {\n      return schema\n    }\n    log.warn \"schema ${schema} not compatible with genus_species: ${genus_species}\"\n    return null;      \n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n\nprocess chewbbaca {\n    container \"ghcr.io/genpat-it/chewbbaca-w-chewie-schemas:2.8.5--16b816c96d\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    time { taskTime( 30.m, task.attempt ) }    \n    cpus { [32, params.max_cpus as int].min() }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      getSchema(genus_species, schema)\n    input:\n      tuple val(riscd_input), path(assembly)\n      val genus_species\n      val schema\n    output:\n      path '**'\n      path(\"${base}_results_statistics.tsv\"), emit: stats\n      tuple path(\"${base}_results_alleles.tsv\"), path('schema/'), emit: alleles\n      tuple path(\"${base}_results_alleles.tsv\"), path(\"${base}_new_alleles.txt\"), val(schemaName), emit: alleles_with_new\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [schema:schemaName, chewbbaca: '2.8.5'])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.tsv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"${base}_new_alleles.txt\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: 'listGenes2Call.txt', saveAs: { \"${base}_listGenes2Call.txt\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      schemaName = getSchema(genus_species, schema)\n      schemaPath = SCHEMAS.get(schemaName) \n      newAlleleKey = assembly.getName().replaceAll('_', '-')\n      speciesSpecificParams = getExtraParams(schemaName)\n      \"\"\"\n        #!/bin/bash -euo pipefail\n        unzip ${schemaPath} -d schema > /dev/null\n        chmod -R 777 schema\n        mkdir input && cp ${assembly} input/\n        chewBBACA.py AlleleCall -i input -g schema -o results --cpu ${task.cpus} --force-continue --verbose ${speciesSpecificParams}\n        grep \"${newAlleleKey}\" schema/*.fasta -A1 -h | grep -v \"\\\\-\\\\-\" > ${base}_new_alleles.txt || echo \"no INF alleles found\"\n        mv results/*/results_alleles.tsv ${base}_results_alleles.tsv\n        mv results/*/results_contigsInfo.tsv ${base}_results_contigsInfo.tsv\n        mv results/*/results_statistics.tsv ${base}_results_statistics.tsv        \n      \"\"\"\n}\n\nprocess chewbbaca_check {\n    container \"quay.io/biocontainers/python:3.9\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"    \n    memory { taskMemory( 1.GB, task.attempt ) }\n    time { taskTime( 5.m, task.attempt ) }    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      path(chewbbacaStats)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n      path(\"${base}_import_chewbbaca_check.csv\"), emit: check\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}_chewbbaca_check.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_chewbbaca_check.cfg\" }\n\n    script:\n      md = parseMetadataFromFileName(chewbbacaStats.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      /scripts/chewieCheck.py --stat ${chewbbacaStats} > ${base}_import_chewbbaca_check.csv      \n      \"\"\" \n}\n\nprocess hashing {\n    container \"ghcr.io/genpat-it/hashing:1.0--29180a232f\"    \n    memory { taskMemory( 2.GB, task.attempt ) }\n    time { taskTime( 10.m, task.attempt ) }    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple path(chewbbaca_result), path(schema_path)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '_hashed_results.tsv', saveAs: { \"${base}_chewbbaca_results_crc32.tsv\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_hashing.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_hashing.cfg\" }\n    script:\n      md = parseMetadataFromFileName(chewbbaca_result.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n        mask_matrix.py -i ${chewbbaca_result} -o masked_results.tsv\n        alleleprofile_hasher.py -p masked_results.tsv -d ${schema_path} -o ./_hashed_results.tsv   \n        rm -Rf ${schema_path}/* \n      \"\"\"\n}\n\nworkflow step_4TY_cgMLST__chewbbaca {\n    take: \n      assembly\n      genus_species\n      schema\n    main:\n      //assume channels are already crossed\n      chewbbaca_result = chewbbaca(assembly, genus_species, schema)      \n      hashing(chewbbaca_result.alleles)\n      chewbbaca_check(chewbbaca_result.stats).check\n}\n\nworkflow {\n    step_4TY_cgMLST__chewbbaca(getInput(), param('genus_species'), optionalOrDefault('schema', ''))\n}\n"}
{"file_name": "step_1PP_filtering__krakentools.nf", "file_path": "/steps/step_1PP_filtering__krakentools.nf", "language": "nextflow", "id": "step_1PP_filtering__krakentools", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;extractKey } from '../functions/common'\ninclude { getTrimmedReads;getParamTaxaId;getParamIncludeChildren;getParamIncludeParents;getKrakenResults } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_filtering'\ndef METHOD = 'krakentools' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess krakentools {\n    container 'quay.io/biocontainers/krakentools:1.2--pyh5e36f6f_0'\n    memory { taskMemory( 1.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(krakenResults)\n      tuple val(riscd_input2), path(reads)\n      val(taxaid)\n      val(include_parents)\n      val(include_children)\n    output:\n      path '*'\n      path '{*.sh,*.log,*.err}', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_input2], md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fastq.gz'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.err', saveAs: { \"${base}_krakentools.err\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_krakentools.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_krakentools.cfg\" }\n    script:\n      (t1,t2) = reads\n      (krakenReport,krakenOutput) = krakenResults\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      options = (include_parents ? ' --include-parents ' : '') + (include_children ? ' --include-children ' : '')\n      suffix = \"t${taxaid}\" + (include_parents ? 'p' : '') + (include_children ? 'c' : '')\n      \"\"\"\n        zcat ${krakenOutput} > kraken_output\n        extract_kraken_reads.py --fastq-output -k kraken_output ${options} -r ${krakenReport} -1 ${t1} -2 ${t2} -t ${taxaid} -o ${base}_R1_krakentools_${suffix}.fastq -o2 ${base}_R2_krakentools_${suffix}.fastq\n        gzip *${suffix}.fastq && rm kraken_output\n      \"\"\"\n}\n\n\nworkflow step_1PP_filtering__krakentools {\n    take: \n      kraken\n      trimmed\n      taxaid\n      include_children\n      include_parents\n    main:\n      krakentools(kraken, trimmed, taxaid, include_children, include_parents)\n}\n\nworkflow {\n    getKrakenResults().cross(getTrimmedReads(false)) { extractKey(it) }.multiMap { \n      kraken: it[0]\n      trimmed: it[1]\n    }.set { krakenAndTrimmed }\n    step_1PP_filtering__krakentools(krakenAndTrimmed.kraken, krakenAndTrimmed.trimmed, getParamTaxaId(), getParamIncludeChildren(), getParamIncludeParents())\n}"}
{"file_name": "step_2AS_mapping__snippy.nf", "file_path": "/steps/step_2AS_mapping__snippy.nf", "language": "nextflow", "id": "step_2AS_mapping__snippy", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata; extractKey;taskMemory;stepInputs } from '../functions/common.nf'\ninclude { getSingleInput;getReference;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_mapping'\ndef METHOD = 'snippy' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess snippy {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    maxForks 4\n    when:\n      reference_path && reference_path.exists() && !reference_path.empty() && (isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process))\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path \"**/${base_ref}*\"\n      path '*_input.json'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference, ref_format:ref_format])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{**/*.tab,**/*.fa,**/*.bam*,**/*.bed,**/*.csv,**/*.vcf*,**/*.gff,**/*.html,**/*.txt}', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"*.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base_ref}.log\" }    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_snippy_${reference}\"\n      ref_format = (reference_path.getName() ==~ /.+\\.f(n)?(a)?(sta)?$/) ? 'fasta' : 'gb'\n      is_fasta = r1.getName() ==~ /.+\\.fa(sta)?$/\n      if (is_fasta) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --ctgs ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"     \n      } else if (isIlluminaPaired(reads)) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --R1 ${r1} --R2 ${r2} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --se ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"      \n      }\n}\n\nworkflow step_2AS_mapping__snippy {\n  take: \n    reads\n    reference \n  main:\n    snippy(reads, reference)\n}\n\nworkflow {\n    getSingleInput().cross(getReference('any')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n    step_2AS_mapping__snippy(input.reads, input.refs)\n}"}
{"file_name": "step_0SQ_rawreads__fastq.nf", "file_path": "/steps/step_0SQ_rawreads__fastq.nf", "language": "nextflow", "id": "step_0SQ_rawreads__fastq", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common'\ninclude { getInput;isCompatibleWithSeqType;isIlluminaPaired } from '../functions/parameters.nf'\ninclude { stepInputs;parseRISCD } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '0SQ_rawreads'\ndef METHOD = 'fastq' // XXX will be adjusted according to input RISCD\ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess fastqc {\n    container 'biocontainers/fastqc:v0.11.5_cv4'\n    memory { taskMemory( 1.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      isCompatibleWithSeqType(reads, ['illumina_paired','ion'], null)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md_input, [dt: md_input.dt], md_input.acc, md_input.met, [seq_type:seq_type])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/result\", pattern: '{*.zip,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '*.log', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_${ex.dt}_fastqc.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      md_input = parseRISCD(riscd_input)       \n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      seq_type = isIlluminaPaired(reads) ? 'illumina_paired' : 'ion'\n      \"\"\"\n      fastqc $reads &> \"${base}_fastqc.log\" \n      \"\"\"\n}\n\nprocess nanoplot {\n    container 'quay.io/biocontainers/nanoplot:1.41.3--pyhdfd78af_0'\n    memory { taskMemory( 4.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], null)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      tuple val(riscd_input), path('*.txt'), emit: stats\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md_input, [dt: md_input.dt], md_input.acc, md_input.met, [seq_type:seq_type])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/result\", pattern: '{*.txt,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}_${ex.dt}_nanoplot.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_${ex.dt}_nanoplot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      md_input = parseRISCD(riscd_input)       \n      seq_type = 'nanopore'\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      NanoPlot -t ${task.cpus} --fastq ${reads} --tsv_stats --no_static -o . -p ${base}_\n      \"\"\"\n}\n\nprocess nanopore_reads_check {\n    container \"quay.io/biocontainers/biopython:1.78\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    input:\n      tuple val(riscd_input), path(stats)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/result\", pattern: '{*_readsCheck.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}_${ex.dt}_readscheck.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md_input.ds}-${md_input.dt}_${md_input.met}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}_${ex.dt}_readscheck.log\" }\n    script:\n      md = parseMetadataFromFileName(stats.getName())\n      md_input = parseRISCD(riscd_input)       \n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      /scripts/SampleReadsCheck_nanopore.py -n $base -s $stats\n      \"\"\"\n}\n\nworkflow step_0SQ_rawreads__fastq {\n    take: data\n    main:\n      fastqc(data)\n      nanoplot(data).stats | nanopore_reads_check\n}\n\nworkflow {\n  step_0SQ_rawreads__fastq(getInput())\n}"}
{"file_name": "step_2AS_mapping__minimap2.nf", "file_path": "/steps/step_2AS_mapping__minimap2.nf", "language": "nextflow", "id": "step_2AS_mapping__minimap2", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName; executionMetadata;extractKey;taskMemory;stepInputs;getRisCd;extractDsRef } from '../functions/common.nf'\ninclude { getSingleInput;getReference;isCompatibleWithSeqType } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_mapping'\ndef METHOD = 'minimap2' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess minimap2 {\n    container \"quay.io/biocontainers/minimap2:2.26--he4a0461_1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    // memory { taskMemory( 10.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }    \n    when:\n      referencePath && referencePath.exists() && !referencePath.empty() && (isCompatibleWithSeqType(reads, ['nanopore'], task.process))\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(referencePath)\n    output:\n      path '*'\n      tuple path(\"${base_ref}.sam\"), val(reference), path(referencePath), emit: sam\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD,  [reference:reference, seq_type: 'nanopore'])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      base_ref = \"${base}_${reference.replace(\"_\", \"\")}\"      \n      \"\"\"\n      minimap2 -ax map-ont ${referencePath} ${reads} -t ${task.cpus} -o ${base_ref}.sam\n      \"\"\"\n}\n\nprocess samtools {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    //memory { taskMemory( 6.GB, task.attempt ) }\n      input:\n      tuple path(sam), val(reference), path(referencePath)\n    output:\n      tuple path(\"${base_ref}_sorted.bam\"), val(reference), emit: bam\n      tuple path(\"${base_ref}.fq\"), val(reference), emit: fq\n      path '*'\n      path '*.sh', hidden: true\n  \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*_sorted.bam*,*.vcf}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_minimap2_${reference}\"\n      \"\"\"\n      samtools view -bS -o ${base_ref}.bam ${sam} 2>> ${base_ref}.log\n      samtools sort -@ 8 ${base_ref}.bam ${base_ref}_sorted 2>> ${base_ref}.log\n      samtools index ${base_ref}_sorted.bam 2>> ${base_ref}.log\n\n    \tsamtools mpileup -uf ${referencePath} ${base_ref}_sorted.bam > ${base_ref}.bcf 2>> ${base_ref}.log\n\t    bcftools view -cg ${base_ref}.bcf > ${base_ref}.var.flt.vcf 2>> ${base_ref}.log\n  \t  vcfutils.pl vcf2fq ${base_ref}.var.flt.vcf > ${base_ref}.fq 2>> ${base_ref}.log\n\t    \"\"\"\n}\n\nprocess seqio {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    input:\n      tuple path(fq), val(reference)\n    output:\n      tuple val(riscd), path(\"${base_ref}.fasta\"), emit: consensus    \n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta*'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_seqio.cfg\" }\n    script:\n      md = parseMetadataFromFileName(fq.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_minimap2_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      #!/usr/bin/env python3\n\n      import os\n      from Bio import SeqIO\n\n      try:\n          if os.path.getsize(\"${fq}\") > 0:\n              SeqIO.convert(\"${fq}\", 'fastq', \"${base_ref}.fasta\", 'fasta')              \n          else:\n              print(\"WARNING no reads map on reference: ${reference}\")\n      except:\n          print(\"Error: not found fastq file after mapping on: ${reference}\")\n\t    \"\"\"  \n}\n\n\nprocess coverage_minmax {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      path '*.csv'\n      path '*.sh', hidden: true\n      tuple path(\"${base_ref}_samtools_depth.txt\"), val(reference), emit: coverage_depth  \n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage_minmax.csv\"), emit: coverage_extra\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_minmax.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools view -F 4 -c ${bam} > samtools_view.txt\n      samtools depth ${bam} > ${base_ref}_samtools_depth.txt\n      /scripts/coverage_minmax.py ${md.cmp} ${md.ds} samtools_view.txt ${base_ref}_samtools_depth.txt ${base_ref}_import_coverage_minmax.csv\n\t    \"\"\"\n}\n\nprocess samtools_depth {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      tuple path(\"${base_ref}.coverage\"), val(reference), emit: coverage\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools_depth.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools depth -a ${bam} | awk '{ if (\\$3!=0) c++;s+=\\$3}{h++} END { if (c!=0) print s/c; else print 0;if (h!=0) print c/h; else print 0 }' > ${base_ref}.coverage\n\t    \"\"\"\n}\n\n\nprocess coverage_check {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple path(coverage), path(consensus), val(reference)\n      val(context)\n    output:\n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage.csv\"), emit: coverage_basic\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.csv,*.check}'      \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check.cfg\" }\n    script:\n      //TODO fix output folder\n      md = parseMetadataFromFileName(consensus.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${context}_${reference}\"\n      coverages= coverage.toRealPath().toFile().readLines()\n      \"\"\"\n      /scripts/coverage.py ${coverage} ${consensus} ${reference} ${md.cmp} ${md.ds.substring(2)} ${base_ref}.check ${base_ref}_import_coverage.csv \n\t    \"\"\"\n}\n\nprocess coverage_check_merge {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  input:\n    tuple val(key), val(reference), path(covMinMax), path(covBasic)\n    val(method)\n  output:\n    tuple val(md.ds), path(\"${base_ref}_import_coverage_merged.csv\"), emit: coverage_merged\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check_merge.cfg\" }\n  script:\n    md = parseMetadataFromFileName(covMinMax.getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    base_ref = \"${base}_${method}_${reference}\"\n    \"\"\"\n    paste -d, ${covMinMax} ${covBasic} | cut -d, -f1,2,3,4,5,8,9,10,11,12,13 > ${base_ref}_import_coverage_merged.csv\n    \"\"\"  \n}\n\nprocess coverage_plot {\n    container \"quay.io/biocontainers/matplotlib:3.1.2\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      tuple path(coverage_depth), val(reference)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.png'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_plot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(coverage_depth.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_minimap2_${reference}\"\n      \"\"\"\n      /scripts/coverage_plot.py ${coverage_depth} ${base_ref}_coverage_plot.png\n\t    \"\"\"\n}\n\n\nworkflow step_2AS_mapping__minimap2 {\n  take: \n    reads\n    reference \n  main:\n    minimap2(reads, reference)\n    samtools(minimap2.out.sam)\n    consensus = seqio(samtools.out.fq).consensus\n\n    coverage_minmax(samtools.out.bam, METHOD)\n    coverage_minmax.out.coverage_depth | coverage_plot\n    \n    coverage = samtools_depth(samtools.out.bam, METHOD).coverage\n    coverage.cross(consensus) { extractDsRef(it) }.map { \n        return [ it[0][0], it[1][1], it[0][1] ]\n    }.set { coverageRefAndConsensus }\n    coverageBasic = coverage_check(coverageRefAndConsensus, METHOD).coverage_basic\n\n    crossedChecks = coverage_minmax.out.coverage_extra.cross(coverageBasic) { it[0] + \"-\" + it[1] }\n    .map { [ it[0][0], it[0][1], it[0][2], it[1][2] ] }\n    coverage_check_merge(crossedChecks, METHOD)\n\n  emit:\n    consensus = consensus\n}\n\n\nworkflow {\n    getSingleInput().cross(getReference('fa')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n    step_2AS_mapping__minimap2(input.reads, input.refs)\n}\n"}
{"file_name": "step_1PP_hostdepl__minimap2.nf", "file_path": "/steps/step_1PP_hostdepl__minimap2.nf", "language": "nextflow", "id": "step_1PP_hostdepl__minimap2", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;stepInputs;extractKey } from '../functions/common.nf'\ninclude { getSingleInput;isCompatibleWithSeqType;getHostReference;getRisCd } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_hostdepl'\ndef METHOD = 'minimap2' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess minimap2 {\n    container \"quay.io/biocontainers/minimap2:2.26--he4a0461_1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    // memory { taskMemory( 10.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }\n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(_), val(host_code), path(host)\n    output:\n      path '*'\n      tuple path(\"${base_ref}.sam\"), val(host_code), emit: sam\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD,  [reference:host_code, seq_type: 'nanopore'])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      base_ref = \"${base}_${host_code.replace(\"_\", \"\")}\"      \n      \"\"\"\n      minimap2 -ax map-ont ${host} ${reads} -t ${task.cpus} -o ${base_ref}.sam\n      \"\"\"\n}\n\nprocess samtools {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    // memory { taskMemory( 18.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }\n    input:\n      tuple path(sam), val(host_code)\n    output:\n      tuple val(riscd), path('*.fastq.gz'), emit: depleted\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: \"*.gz\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}_samtools.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      base_ref = \"${base}_${host_code.replace(\"_\", \"\")}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)                 \n      \"\"\"\n        trap \"rm '${base}_um.bam'\" EXIT\n\t\t    samtools view -bS ${sam} -@ ${task.cpus} -o ${base}.bam \n        samtools view -b -f 4 ${base}.bam  -@ ${task.cpus} -o ${base}_um.bam  2>> ${base_ref}_samtools.log \n        samtools sort -n ${base}_um.bam -o ${base}.sorted -@ ${task.cpus} 2>> ${base_ref}_samtools.log \n        samtools bam2fq ${base}.sorted  | gzip > ${base_ref}.fastq.gz 2>> ${base_ref}_samtools.log\n\t    \"\"\"\n}\n\nworkflow step_1PP_hostdepl__minimap2 {\n    take: \n      reads\n      host\n    main:\n      minimap2(reads, host).sam | samtools\n    emit:\n      samtools.out.depleted \n}\n\nworkflow {\n    getSingleInput().cross(getHostReference()) { extractKey(it) }\n      .multiMap { \n          reads: it[0]\n          host:  it[1]\n      }.set { input }\n  \n    step_1PP_hostdepl__minimap2(input.reads, input.host)\n}"}
{"file_name": "step_2AS_mapping__ivar.nf", "file_path": "/steps/step_2AS_mapping__ivar.nf", "language": "nextflow", "id": "step_2AS_mapping__ivar", "content": "nextflow.enable.dsl=2\n\ninclude { extractDsRef;getEmpty;flattenPath; parseMetadataFromFileName; executionMetadata; extractKey;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;getReferences;getReferenceCodes;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent;isSegmentedMapping } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_mapping'\ndef METHOD = 'ivar' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef getExDt(reference, ex) {\n    try {        \n      reflist = getReferenceCodes()\n      if (reflist.size() < 2) {\n        return ex.dt\n      } \n      if (!reference) {\n        log.warn \"${reference} should not be empty\"\n        return ex.dt\n      }      \n      if (!reflist.contains(reference)) {\n        log.warn \"${reference} not found in ${reflist}\"\n        return ex.dt\n      }\n      return ex.dt + reflist.indexOf(reference)\n    } catch(Throwable t) {\n        log.warn \"getExDt: unexpected exception: ${t.asString()}\"\n        return ex.dt\n    }\n}\n\ndef canBeAggregated(actual) {\n    try {        \n      expected = getReferenceCodes()\n      if (expected.size() < 2) {\n          return false\n      }\n      if (actual.size() != expected.size()) {\n        log.warn \"expected ${expected.size()} references, got: ${actual.size()}\"\n        return false\n      } \n      if (actual.any {!expected.contains(it)}) {\n        log.warn \"there is some unexpected value in ${actual} (values should be: ${expected})\"\n        return false\n      }\n      return true\n    } catch(Throwable t) {\n        log.warn \"canBeAggregated: unexpected exception: ${t.asString()}\"\n        return false\n    }\n}\n\nprocess snippy {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    maxForks 4\n    when:\n      reference_path && reference_path.exists() && !reference_path.empty() && isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path \"**/${base_ref}*\"\n      tuple path(\"snippy/${base_ref}.bam\"), val(reference), emit: bam  \n      path '*.sh', hidden: true\n      path '*_input.json'\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, [dt: ex_dt], STEP, METHOD, [reference:reference])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/result\", pattern: '{**/*.tab,**/*.fa,**/*.bam*,**/*.bed,**/*.csv,**/*.vcf*,**/*.gff,**/*.html,**/*.txt}', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '**/*.log', saveAs: { filename -> flattenPath(filename) }     \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_snippy.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: \"*.json\"\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_vdsnippy_${reference}\"\n      is_fasta = r1.getName() ==~ /.+\\.fa(sta)?$/\n      if (is_fasta) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --ctgs ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"     \n      } else if (isIlluminaPaired(reads)) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --R1 ${r1} --R2 ${r2} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        trap \"find -name \"*.sam\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --se ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        NUMVAR=\\$((`cat snippy/*.tab | wc -l`-1))\n        echo -e \"Sample\\tRef\\tNumVar\" >> ${base_ref}.check\n        echo -e \"${md.cmp}\\t${reference}\\t\\$NUMVAR\" >> ${base_ref}.check\n        \"\"\"      \n      }\n}\n\nprocess samtools_pileup {\n    container \"quay.io/biocontainers/samtools:1.10--h2e538c0_3\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple path(bam), val(reference)\n    output:\n      tuple path(\"${base_ref}.pileup\"), val(reference), emit: pileup    \n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools_pileup.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_${METHOD}_${reference}\"\n      \"\"\"\n        #input should be DS10561267-DT200702_2020.TE.89540.1.2_vdsnippy_NC045512.bam\n        samtools mpileup -d 1000 -A -Q 0 ${bam} > ${base_ref}.pileup\n      \"\"\"\n}\n\nprocess ivar {\n    container \"quay.io/biocontainers/ivar:1.3--h089eab3_1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      tuple path(pileup), val(reference)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n      tuple val(riscd), val(reference), path(\"${base_ref}.fasta\"), emit: consensus    \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_${METHOD}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(pileup.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_${METHOD}_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        # Note : samtools mpileup output must be piped into ivar consensus\n        cat ${pileup} | ivar consensus -p ${base_ref} -q ${params.step_2AS_mapping__ivar___q} -m 1 > ${base_ref}.log\n        mv ${base_ref}.fa ${base_ref}.fasta\n      \"\"\"\n}\n\nprocess samtools_depth {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      tuple path(\"${base_ref}.coverage\"), val(reference), emit: coverage\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools_depth.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools depth -a ${bam} | awk '{ if (\\$3!=0) c++;s+=\\$3}{h++} END { if (c!=0) print s/c; else print 0;if (h!=0) print c/h; else print 0 }' > ${base_ref}.coverage\n\t    \"\"\"\n}\n\nprocess coverage_minmax {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      path '*.csv'\n      path '*.sh', hidden: true\n      tuple path(\"${base_ref}_samtools_depth.txt\"), val(reference), emit: coverage_depth  \n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage_minmax.csv\"), emit: coverage_extra\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '{*.csv,*.txt}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_minmax.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools view -F 4 -c ${bam} > samtools_view.txt\n      samtools depth ${bam} > ${base_ref}_samtools_depth.txt\n      /scripts/coverage_minmax.py ${md.cmp} ${md.ds} samtools_view.txt ${base_ref}_samtools_depth.txt ${base_ref}_import_coverage_minmax.csv\n\t    \"\"\"\n}\n\nprocess coverage_check {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple path(coverage), path(consensus), val(reference)\n      val(context)\n    output:\n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage.csv\"), emit: coverage_basic\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '{*.csv,*.check}'      \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check.cfg\" }\n    script:\n      //TODO fix output folder\n      md = parseMetadataFromFileName(consensus.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_${context}_${reference}\"\n      coverages= coverage.toRealPath().toFile().readLines()\n      \"\"\"\n      /scripts/coverage.py ${coverage} ${consensus} ${reference} ${md.cmp} ${md.ds.substring(2)} ${base_ref}.check ${base_ref}_import_coverage.csv \n\t    \"\"\"\n}\n\nprocess coverage_check_group {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  when:\n    (files instanceof java.util.Collection) && files.size() > 1  \n  input:\n    tuple val(key), path(files)\n    val(method)\n  output:\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${res_folder}/meta\", pattern: '.command.sh', saveAs: { \"${base}_${method}_coverage_check.cfg\" }\n  publishDir mode: 'rellink', \"${params.outdir}/${res_folder}/result\", pattern: '*.csv'\n  script:\n    def coverage_file = files.flatten()[0]\n    md = parseMetadataFromFileName(coverage_file.getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    res_folder = isSegmentedMapping() ? \"${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}\" : \"aggregate\"\n    referenceOrdered = getReferenceCodes()\n    files.sort{ c1, c2 ->\n      def ref1 = c1.getName().replaceAll(\".+_${method}_\", \"\") -~ /_import.+/ \n      def ref2 = c2.getName().replaceAll(\".+_${method}_\", \"\") -~ /_import.+/ \n      referenceOrdered.indexOf(ref1) <=> referenceOrdered.indexOf(ref2) \n    }\n    \"\"\"\n    cat ${files} > ${base}_${method}_coverage_full.csv.tmp\n    head -n 1 ${base}_${method}_coverage_full.csv.tmp > ${base}_${method}_coverage_full.csv\n    grep -E \"^[A-Z]\" -v ${base}_${method}_coverage_full.csv.tmp >> ${base}_${method}_coverage_full.csv\n    \"\"\"  \n}\n\nprocess coverage_check_merge {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  input:\n    tuple val(key), val(reference), path(covMinMax), path(covBasic)\n    val(method)\n  output:\n    tuple val(md.ds), path(\"${base_ref}_import_coverage_merged.csv\"), emit: coverage_merged\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '*.csv'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check_merge.cfg\" }\n  script:\n    md = parseMetadataFromFileName(covMinMax.getName())\n    ex_dt = getExDt(reference, ex)\n    base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n    base_ref = \"${base}_${method}_${reference}\"\n    \"\"\"\n    paste -d, ${covMinMax} ${covBasic} | cut -d, -f1,2,3,4,5,8,9,10,11,12,13 > ${base_ref}_import_coverage_merged.csv\n    \"\"\"  \n}\n\nprocess coverage_plot {\n    container \"quay.io/biocontainers/matplotlib:3.1.2\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      tuple path(coverage_depth), val(reference)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/result\", pattern: '*.png'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex_dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_plot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(coverage_depth.getName())\n      ex_dt = getExDt(reference, ex)\n      base = \"${md.ds}-${ex_dt}_${md.cmp}\"\n      base_ref = \"${base}_ivar_${reference}\"\n      \"\"\"\n      /scripts/coverage_plot.py ${coverage_depth} ${base_ref}_coverage_plot.png\n\t    \"\"\"\n}\n\nprocess aggregate {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  when:\n    isSegmentedMapping() && canBeAggregated(references)\n  input:\n    tuple val(riscd_input), val(references), path(consensus)\n  output:\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_${METHOD}_aggregate.cfg\" }\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: \"*.json\"\n  afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [references:referenceOrdered])}' > ${base}_input.json\"\n  script:\n    md = parseMetadataFromFileName(consensus[0].getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    sorted = []\n    referenceOrdered = getReferenceCodes()\n    references.eachWithIndex{ ref, index ->\n        sorted[referenceOrdered.indexOf(ref)] = consensus[index]\n    }\n    \"\"\"\n      for f in ${sorted.join(\" \")} ; do \n        if [ \"0\" -ne \"`grep -cve '^\\\\s*\\$' -e \">\" \\${f}`\" ] ;\n            then\n                cat \"\\${f}\" | awk 1 >> ${base}_${METHOD}_aggregate.fasta\n        fi\n      done\n    \"\"\"  \n}\n\nworkflow step_2AS_mapping__ivar {\n  take: \n    reads\n    reference \n  main:\n    bam = snippy(reads, reference).bam\n    pileup = samtools_pileup(bam).pileup\n    consensus = ivar(pileup).consensus\n\n    coverage_minmax(bam, 'vdsnippy')\n    coverage_minmax.out.coverage_depth | coverage_plot\n\n    coverage = samtools_depth(bam, 'vdsnippy').coverage\n\n    coverage.cross(consensus) { extractDsRef(it) }.map { \n        return [ it[0][0], it[1][2], it[0][1] ]\n    }.set { coverageRefAndConsensus }\n    coverageBasic = coverage_check(coverageRefAndConsensus, 'ivar').coverage_basic\n\n    crossedChecks = coverage_minmax.out.coverage_extra.cross(coverageBasic) { it[0] + \"-\" + it[1] }\n    .map { [ it[0][0], it[0][1], it[0][2], it[1][2] ] }\n\n    coverage_check_group(coverage_check_merge(crossedChecks, 'vdsnippy').coverage_merged | groupTuple, 'vdsnippy')\n    \n    reads.cross(consensus | groupTuple) {{ extractKey(it) }}\n      .map { [\n         it[0][0], // riscd\n         it[1][1], // reference codes\n         it[1][2]  // consensus file\n      ]}.set { consensus_to_aggregate}\n\n    aggregate(consensus_to_aggregate)\n  \n  emit:\n    consensus = consensus.map { it[0,2] }\n    coverage_depth = coverage_minmax.out.coverage_depth\n}\n\nworkflow {\n    getSingleInput().cross(getReferences('any')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n\n    step_2AS_mapping__ivar(input.reads, input.refs)\n}"}
{"file_name": "step_1PP_downsampling__bbnorm.nf", "file_path": "/steps/step_1PP_downsampling__bbnorm.nf", "language": "nextflow", "id": "step_1PP_downsampling__bbnorm", "content": "nextflow.enable.dsl=2\n\ninclude { stepInputs;parseMetadataFromFileName;executionMetadata } from '../functions/common.nf'\ninclude { getSingleInput;param;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '1PP_downsampling'\ndef METHOD = 'bbnorm' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess bbnorm {\n    container \"quay.io/biocontainers/bbmap:39.01--h5c4e2a8_0\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    when:\n      isCompatibleWithSeqType(reads, 'illumina_paired', task.process)        \n    input:\n      tuple val(riscd_input), path(reads)\n      val(k)\n      val(target)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, [k:k, target:target])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fastq.gz'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.hist,*_input.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = reads\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_bbnorm_k${k}_t${target}\"\n      // bbnorm has trouble calculating the max heap size inside a container\n      javaHeapLimit = ((params.max_memory as nextflow.util.MemoryUnit).getMega() * 0.85) as int\n      \"\"\"\n        bbnorm.sh \\\n          in=${r1} \\\n          in2=${r2} \\\n          out=${base}_R1.fastq.gz \\\n          out2=${base}_R2.fastq.gz \\\n          hist=${base}.hist \\\n          k=${k} \\\n          target=${target} \\\n          -Xmx${javaHeapLimit}m\n      \"\"\"\n}\n\nworkflow step_1PP_downsampling__bbnorm {\n    take: \n      reads\n      k\n      target\n    main:\n      bbnorm(reads, k, target)\n}\n\nworkflow {\n    step_1PP_downsampling__bbnorm(getSingleInput(), param('k'), param('target'))\n}"}
{"file_name": "step_2AS_filtering__seqio.nf", "file_path": "/steps/step_2AS_filtering__seqio.nf", "language": "nextflow", "id": "step_2AS_filtering__seqio", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\ninclude { param } from '../functions/parameters.nf'\n\nFILTERABLE_REFERENCES_PATH = param('step_2AS_filtering__seqio')\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'spades' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef isReferenceFilterable(refCode, _path) {\n    try {\n      def referencePath = (_path instanceof java.util.Collection) ? _path.flatten()[0] : _path\n      if (referencePath.empty) {\n        //it means that no reference is provided\n        log.warn \"no reference provided - skipping scaffold filtering\"\n        return false\n      }\n      println \"${referencePath.toRealPath().toString()} vs ${FILTERABLE_REFERENCES_PATH}\"\n      //we can filter a reference only if present in abricate virus DB\n      //assuming: reference path in  \"referencesDir\" -> then it is filterable\n      def filterable = referencePath.toRealPath().toString().contains(FILTERABLE_REFERENCES_PATH)\n      if (!filterable) {\n        log.warn \"a reference can be filtered only if present in our abricate virus DB - skipping scaffold filtering for '${refCode}'\"\n      }\n      return filterable\n    } catch(Throwable t) {\n        log.error \"error while executing 'isReferenceFilterable' on '${referencePath}' (${t.toString()})\"\n        return false\n    } \n}\n\nprocess filter {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    when:\n       isReferenceFilterable(reference, referencePath)\n    input:\n      tuple val(riscd_input), path(calls)\n      tuple val(riscd_input2),  path(l200)\n      tuple val(riscd_ref), val(reference), path(referencePath)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_input2, riscd_ref], md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_filter.cfg\" }\n    script:\n      md = parseMetadataFromFileName(calls.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      output = l200?.getName().replace(\".fasta\", \"_${reference.replace('_', '')}.fasta\")\n      \"\"\"\n      /scripts/cleanDenovo.py ${l200} ${calls} ${reference} ${output}\n      \"\"\"\n}\n\nworkflow step_2AS_filtering__seqio {\n    take: \n      calls\n      assembly\n      reference\n    main:\n      filter(calls, assembly, reference)\n}\n"}
{"file_name": "step_2AS_mapping__bowtie.nf", "file_path": "/steps/step_2AS_mapping__bowtie.nf", "language": "nextflow", "id": "step_2AS_mapping__bowtie", "content": "nextflow.enable.dsl=2\n\ninclude { extractDsRef;parseMetadataFromFileName;executionMetadata;extractKey;taskMemory;getEmpty } from '../functions/common.nf'\ninclude { getSingleInput;getReference;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_mapping'\ndef METHOD = 'bowtie' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess bowtie2 {\n    container \"ghcr.io/genpat-it/bowtie2:2.1.0--37ad014737\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    when:\n      referencePath && referencePath.exists() && !referencePath.empty() && (isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process))\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(referencePath)\n    output:\n      path '*'\n      tuple path(\"${base_ref}.sam\"), val(reference), path(referencePath), emit: sam\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs([riscd_input,riscd_ref], md, ex, STEP, METHOD, [reference:reference])}' > ${base_ref}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      if (isIlluminaPaired(reads)) {\n        \"\"\"\n        bowtie2-build ${referencePath} ${reference}\n        bowtie2 -p 8 --very-fast -x ${reference} -1 ${t1} -2 ${t2} -S ${base_ref}.sam 2>> ${base_ref}.log\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        bowtie2-build ${referencePath} ${reference}\n        bowtie2 -p 8 --very-fast -x ${reference} -U ${t1} -S ${base_ref}.sam 2>> ${base_ref}.log\n        \"\"\"      \n      }      \n}\n\nprocess samtools {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 6.GB, task.attempt ) }\n        input:\n      tuple path(sam), val(reference), path(referencePath)\n    output:\n      tuple path(\"${base_ref}_sorted.bam\"), val(reference), emit: bam\n      tuple path(\"${base_ref}.fq\"), val(reference), emit: fq\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*_sorted.bam*,*.vcf}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools.cfg\" }\n    script:\n      md = parseMetadataFromFileName(sam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      \"\"\"\n      samtools view -bS -o ${base_ref}.bam ${sam} 2>> ${base_ref}.log\n      samtools sort -@ 8 ${base_ref}.bam ${base_ref}_sorted 2>> ${base_ref}.log\n      samtools index ${base_ref}_sorted.bam 2>> ${base_ref}.log\n\n    \tsamtools mpileup -uf ${referencePath} ${base_ref}_sorted.bam > ${base_ref}.bcf 2>> ${base_ref}.log\n\t    bcftools view -cg ${base_ref}.bcf > ${base_ref}.var.flt.vcf 2>> ${base_ref}.log\n  \t  vcfutils.pl vcf2fq ${base_ref}.var.flt.vcf > ${base_ref}.fq 2>> ${base_ref}.log\n\t    \"\"\"\n}\n\nprocess seqio {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    input:\n      tuple path(fq), val(reference)\n    output:\n      tuple val(riscd), path(\"${base_ref}.fasta\"), emit: consensus    \n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta*'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_seqio.cfg\" }\n    script:\n      md = parseMetadataFromFileName(fq.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n      #!/usr/bin/env python3\n\n      import os\n      from Bio import SeqIO\n\n      try:\n          if os.path.getsize(\"${fq}\") > 0:\n              SeqIO.convert(\"${fq}\", 'fastq', \"${base_ref}.fasta\", 'fasta')              \n          else:\n              print(\"WARNING no reads map on reference: ${reference}\")\n      except:\n          print(\"Error: not found fastq file after mapping on: ${reference}\")\n\t    \"\"\"  \n}\n\nprocess samtools_depth {\n    container 'quay.io/biocontainers/samtools:1.10--h9402c20_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      tuple path(\"${base_ref}.coverage\"), val(reference), emit: coverage\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_samtools_depth.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools depth -a ${bam} | awk '{ if (\\$3!=0) c++;s+=\\$3}{h++} END { if (c!=0) print s/c; else print 0;if (h!=0) print c/h; else print 0 }' > ${base_ref}.coverage\n\t    \"\"\"\n}\n\nprocess coverage_minmax {\n    container \"ghcr.io/genpat-it/samtools:0.1.19--f3869562fe\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n        input:\n      tuple path(bam), val(reference)\n      val(method)\n    output:\n      path '*.csv'\n      path '*.sh', hidden: true\n      tuple path(\"${base_ref}_samtools_depth.txt\"), val(reference), emit: coverage_depth  \n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage_minmax.csv\"), emit: coverage_extra\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_minmax.cfg\" }\n    script:\n      md = parseMetadataFromFileName(bam.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${method}_${reference}\"\n      \"\"\"\n      samtools view -F 4 -c ${bam} > samtools_view.txt\n      samtools depth ${bam} > ${base_ref}_samtools_depth.txt\n      /scripts/coverage_minmax.py ${md.cmp} ${md.ds} samtools_view.txt ${base_ref}_samtools_depth.txt ${base_ref}_import_coverage_minmax.csv\n\t    \"\"\"\n}\n\nprocess coverage_check {\n    container \"ghcr.io/genpat-it/python3:3.10.1--29cf21c1f1\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple path(coverage), path(consensus), val(reference)\n      val(context)\n    output:\n      tuple val(md.ds), val(reference), path(\"${base_ref}_import_coverage.csv\"), emit: coverage_basic\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.csv,*.check}'      \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check.cfg\" }\n    script:\n      //TODO fix output folder\n      md = parseMetadataFromFileName(consensus.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_${context}_${reference}\"\n      coverages= coverage.toRealPath().toFile().readLines()\n      \"\"\"\n      /scripts/coverage.py ${coverage} ${consensus} ${reference} ${md.cmp} ${md.ds.substring(2)} ${base_ref}.check ${base_ref}_import_coverage.csv \n\t    \"\"\"\n}\n\nprocess coverage_check_group {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  input:\n    tuple val(key), path(files)\n    val(method)\n  output:\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}_${method}_coverage_check.cfg\" }\n  script:\n    def coverage_file = (files instanceof java.util.Collection) ? files.flatten()[0] : files\n    md = parseMetadataFromFileName(coverage_file.getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    \"\"\"\n    cat ${files} | sort -ur > ${base}_${method}_import_coverage_full.csv\n    \"\"\"  \n}\n\nprocess coverage_check_merge {\n  container \"ubuntu:20.04\"\n  tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n  memory { taskMemory( 200.MB, task.attempt ) }\n  input:\n    tuple val(key), val(reference), path(covMinMax), path(covBasic)\n    val(method)\n  output:\n    tuple val(md.ds), path(\"${base_ref}_import_coverage_merged.csv\"), emit: coverage_merged\n    path '*'\n    path '*.sh', hidden: true\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.csv'\n  publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_check_merge.cfg\" }\n  script:\n    md = parseMetadataFromFileName(covMinMax.getName())\n    base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n    base_ref = \"${base}_${method}_${reference}\"\n    \"\"\"\n    paste -d, ${covMinMax} ${covBasic} | cut -d, -f1,2,3,4,5,8,9,10,11,12,13 > ${base_ref}_import_coverage_merged.csv\n    \"\"\"  \n}\n\nprocess coverage_plot {\n    container \"quay.io/biocontainers/matplotlib:3.1.2\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 1.GB, task.attempt ) }\n    input:\n      tuple path(coverage_depth), val(reference)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.png'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}_coverage_plot.cfg\" }\n    script:\n      md = parseMetadataFromFileName(coverage_depth.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_bowtie_${reference}\"\n      \"\"\"\n      /scripts/coverage_plot.py ${coverage_depth} ${base_ref}_coverage_plot.png\n\t    \"\"\"\n}\n\nworkflow step_2AS_mapping__bowtie {\n  take: \n    reads\n    reference \n  main:\n    bowtie2(reads, reference) //[ refId, refPath ]\n    samtools(bowtie2.out.sam)\n    consensus = seqio(samtools.out.fq).consensus\n\n    coverage_minmax(samtools.out.bam, 'bowtie')\n    coverage_minmax.out.coverage_depth | coverage_plot\n    \n    coverage = samtools_depth(samtools.out.bam, 'bowtie').coverage\n    coverage.cross(consensus) { extractDsRef(it) }.map { \n        return [ it[0][0], it[1][1], it[0][1] ]\n    }.set { coverageRefAndConsensus }\n    coverageBasic = coverage_check(coverageRefAndConsensus, 'bowtie').coverage_basic\n\n    crossedChecks = coverage_minmax.out.coverage_extra.cross(coverageBasic) { it[0] + \"-\" + it[1] }\n    .map { [ it[0][0], it[0][1], it[0][2], it[1][2] ] }\n    coverage_check_group(coverage_check_merge(crossedChecks, 'bowtie').coverage_merged | groupTuple, 'bowtie')\n  emit:\n    consensus\n  }\n\nworkflow {\n    getSingleInput().cross(getReference('fa')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }  \n    step_2AS_mapping__bowtie(input.reads, input.refs)\n}"}
{"file_name": "step_4TY_MLST__mlst.nf", "file_path": "/steps/step_4TY_MLST__mlst.nf", "language": "nextflow", "id": "step_4TY_MLST__mlst", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;param } from '../functions/parameters.nf'\ninclude { stepInputs } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '4TY_MLST'\ndef METHOD = 'mlst' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess mlst {\n    container \"ghcr.io/genpat-it/mlst-w-db:2.23.0--60b8b2e3dd_231219.124455\"\n    containerOptions = \"-v ${workflow.projectDir}/scripts/${ENTRYPOINT}:/scripts:ro\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }\n    input:\n      tuple val(riscd_input), path(assembly)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*.csv,*.tsv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '{*.log,*.json}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      excluded_schemas = param('step_4TY_MLST__mlst___excluded_schemas')\n      \"\"\"\n        mlst --threads ${task.cpus}  ${assembly} --exclude '${excluded_schemas}' > ${base}.tsv 2> ${base}.log\n        /scripts/process-mlst-result.py ${base}.tsv ${base}_cc.csv /NGStools/mlst/db/pubmlst\n      \"\"\"\n}\n\nworkflow step_4TY_MLST__mlst {\n    take: \n      assembly\n    main:\n      mlst(assembly)\n}\n\nworkflow {\n  step_4TY_MLST__mlst(getInput())\n}"}
{"file_name": "step_2AS_denovo__flye.nf", "file_path": "/steps/step_2AS_denovo__flye.nf", "language": "nextflow", "id": "step_2AS_denovo__flye", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent;optional } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'flye' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess flye {\n    container \"quay.io/biocontainers/flye:2.9.2--py310h2b6aa90_2\"    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 32.GB, task.attempt ) }\n    cpus { [16, params.max_cpus as int].min() }   \n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '**'\n      tuple val(riscd), path(\"${base}.fasta\"), emit: assembly\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: 'assembly_info.txt', saveAs: { \"${base}_assembly_info.txt\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      g = \"${optional('step_2AS_denovo__flye__genome_size')}\".replaceAll(/[\\s\\n\\t\\r\"'$\\{]/, \"\")\n      g_par = g ? \" --genome-size ${g}\" : ''      \n      m_par = optional('step_2AS_denovo__flye__meta') ? \" --meta \" : ''          \n      \"\"\"\n        flye \\\n        --nano-hq \\\n        ${reads} \\\n        --out-dir .\\\n        --threads ${task.cpus} ${g_par} ${m_par}\n        mv assembly.fasta ${base}.fasta\n      \"\"\"        \n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    input:\n      tuple val(_), path(assembly)\n    output:\n      path '*_quast.*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_quast\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${assembly} ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}.csv ; \n      \"\"\"\n}\n\n\nworkflow step_2AS_denovo__flye {\n    take: reads\n    main:\n      contigs = flye(reads).assembly\n      quast(contigs)\n    emit:\n      assembly = contigs\n}\n\nworkflow {\n    step_2AS_denovo__flye(getSingleInput())\n}"}
{"file_name": "step_2AS_denovo__shovill.nf", "file_path": "/steps/step_2AS_denovo__shovill.nf", "language": "nextflow", "id": "step_2AS_denovo__shovill", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory;taskTime } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent } from '../functions/parameters.nf'\ninclude { stepInputs;getRisCd } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '2AS_denovo'\ndef METHOD = 'shovill' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\nprocess shovill {\n    container \"ghcr.io/genpat-it/shovill:1.1.0--d84470570e\"    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 16.GB, task.attempt ) }\n    time { taskTime( 45.m, task.attempt ) }    \n    cpus { [8, params.max_cpus as int].min() }   \n    when:\n      isCompatibleWithSeqType(reads, ['illumina_paired'], null)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '**'\n      tuple val(riscd), path(\"${base}.fasta\"), emit: assembly\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        shovill --outdir out --minlen 200 --cpus ${task.cpus} --ram ${task.memory.toGiga()} --R1 ${t1} --R2 ${t2}\n        mv out/contigs.fa ${base}.fasta\n      \"\"\"\n}\n\nprocess shovill_se {\n    container \"ghcr.io/genpat-it/shovill-se:1.1.1--ba51ea69e5\"    \n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 16.GB, task.attempt ) }\n    time { taskTime( 45.m, task.attempt ) }    \n    cpus { [8, params.max_cpus as int].min() }   \n    when:\n      isCompatibleWithSeqType(reads, ['ion'], null)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '**'\n      tuple val(riscd), path(\"${base}.fasta\"), emit: assembly\n      path '*.sh', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '*.fasta'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    script:\n      (t1,t2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(t1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_${METHOD}\"\n      riscd = getRisCd(md, ex, STEP, METHOD)      \n      \"\"\"\n        shovill-se --outdir out --minlen 200 --cpus ${task.cpus} --ram ${task.memory.toGiga()} --se ${t1} --opts '--sc --iontorrent' --kmers '31,33,55'\n        mv out/contigs.fa ${base}.fasta\n      \"\"\"        \n}\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }\n    time { taskTime( 5.m, task.attempt ) }    \n    input:\n      tuple val(_), path(assembly)\n    output:\n      path '*_quast.*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: '*.csv'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_quast\"\n      \"\"\"\n      quast -m 200 --fast -o quast ${assembly} > ${base}.log ;\n\t\t\tcut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}.csv ; \n      \"\"\"\n}\n\nprocess checkm {\n    container 'quay.io/biocontainers/checkm-genome:1.1.3--py_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 48.GB, task.attempt ) }\n    time { taskTime( 15.m, task.attempt ) }    \n    cpus { [16, params.max_cpus as int].min() }\n    input:\n      tuple val(_), path(assembly)\n    output:\n      path '{out/**,*.tab}'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: 'results.tab', saveAs: { \"${base}_results.tab\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/result\", pattern: 'out/files/storage/bin_stats.analyze.tsv', saveAs: { \"${base}_bin_stats.analyze.tsv\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: 'out/files/checkm.log', saveAs: { \"${base}_hmm.log\" }         \n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '*.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/qc/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      md = parseMetadataFromFileName(assembly.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_checkm\"\n      \"\"\"\n        mkdir in && mkdir -p out/files && cp ${assembly} in/\n        checkm lineage_wf -x fasta -t ${task.cpus} --reduced_tree --tab_table ./in ./out/files | tee results.tab.tmp\n        grep \"^Bin\" -A1 results.tab.tmp > results.tab || true && rm results.tab.tmp\n      \"\"\"\n}\n\nworkflow step_2AS_denovo__shovill {\n    take: rawreads\n    main:\n      contigs_from_pe = shovill(rawreads).assembly\n      contigs_from_se = shovill_se(rawreads).assembly\n      contigs = contigs_from_pe.mix(contigs_from_se)\n      quast(contigs)          \n      if (!params.skip_checkm) {\n        checkm(contigs)\n      }\n    emit:\n      assembly = contigs\n}\n\nworkflow {\n    step_2AS_denovo__shovill(getSingleInput())\n}\n"}
{"file_name": "step_3TX_class__confindr.nf", "file_path": "/steps/step_3TX_class__confindr.nf", "language": "nextflow", "id": "step_3TX_class__confindr", "content": "nextflow.enable.dsl=2\n\ninclude { stepInputs;parseMetadataFromFileName;executionMetadata;isSpeciesSupported;taskMemory } from '../functions/common.nf'\ninclude { getSingleInput;param;isIlluminaPaired;isCompatibleWithSeqType } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef STEP = '3TX_class'\ndef METHOD = 'confindr' \ndef ENTRYPOINT = \"step_${STEP}__${METHOD}\"\n\ndef GENUS_ALLOWED = [\n  'escherichia',\n  'salmonella',\n  'listeria' \n]\n\nprocess confindr {\n    container \"ghcr.io/genpat-it/confindr:0.7.4--8fe19dd246\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 24.GB, task.attempt ) }\n    cpus  16\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process) && isSpeciesSupported(genus_species, GENUS_ALLOWED, reads, task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      val(genus_species)\n    output:\n      path '*'\n      path '*_report.csv'\n      path '{*.sh,*.log}', hidden: true\n    afterScript \"echo '${stepInputs(riscd_input, md, ex, STEP, METHOD, null)}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: 'confindr_report.csv', saveAs: { \"${base}_report.csv\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/result\", pattern: '{*_rmlst.csv,*_contamination.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '*_input.json'\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: 'confindr_log.txt', saveAs: { \"${base}_log.txt\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/${md.anno}/${md.cmp}/${STEP}/${md.ds}-${ex.dt}_${METHOD}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}_confindr\"     \n      \"\"\"\n        confindr.py \\\n          -i ./ \\\n          -o . \\\n          -d /db \\\n          -t ${task.cpus} \\\n          --cross_details\n      \"\"\"   \n}\n\nworkflow step_3TX_class__confindr {\n    take: \n      reads\n      genus_species\n    main:\n      confindr(reads, genus_species)\n}\n\nworkflow {\n    step_3TX_class__confindr(getSingleInput(),param('genus_species'))\n}"}
{"file_name": "module_reportree.nf", "file_path": "/modules/module_reportree.nf", "language": "nextflow", "id": "module_reportree", "content": "nextflow.enable.dsl=2\n\ninclude { getVCFs;param;optionalOrDefault } from '../functions/parameters.nf'\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\n\nif (getReportreeInputType() == 'alleles') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alleles\"\n  include { getAlleles as inputFn } from \"../multi/multi_clustering__reportree_alleles\"\n} else if (getReportreeInputType() == 'alignment') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alignment\"\n  include { getInput as inputFn } from '../functions/parameters.nf'\n} else {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_vcf\"\n  include { getVCFs as inputFn } from '../functions/parameters.nf'\n}\n\ndef getReportreeInputType() {\n    def res = param('multi_clustering__reportree__input')\n    if (!(res in ['alleles', 'vcf', 'alignment'])) {\n        exit 2, \"params (multi_clustering__reportree__input) not valid\"    \n    } \n    return res\n}\n\nworkflow {    \n    multi_clustering__reportree(inputFn(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}"}
{"file_name": "module_reads_processing.nf", "file_path": "/modules/module_reads_processing.nf", "language": "nextflow", "id": "module_reads_processing", "content": "nextflow.enable.dsl=2\n\ninclude { step_0SQ_rawreads__fastq } from '../steps/step_0SQ_rawreads__fastq'\ninclude { step_1PP_trimming__trimmomatic } from '../steps/step_1PP_trimming__trimmomatic'\ninclude { step_1PP_trimming__fastp } from '../steps/step_1PP_trimming__fastp'\ninclude { step_3TX_class__kraken } from '../steps/step_3TX_class__kraken'\ninclude { extractKey } from '../functions/common.nf'\ninclude { getInput;hasFastqData;hasEnoughFastqData;isIlluminaPaired;isIonTorrent;isNanopore; } from '../functions/parameters.nf'\ninclude { isBacterium } from '../functions/sampletypes.nf'\n\nworkflow module_reads_processing {\n    take: \n      rawReads\n    main:\n        rawReads.branch {\n            with_data: hasFastqData(it[1])\n            no_reads: true\n        }\n        .set { rawreads_branched }\n        step_0SQ_rawreads__fastq(rawreads_branched.with_data)        \n\n        rawreads_branched.with_data.branch {\n            illumina: isIlluminaPaired(it[1])\n            ion: isIonTorrent(it[1])\n            nanopore: isNanopore(it[1])\n            other: true // won't be processed\n        }\n        .set { trimming_by_seqtype }\n\n        trimming_by_seqtype.illumina.branch {\n            bacteria: isBacterium(it)\n            other: true \n        }\n        .set { trimming_illumina }\n\n        // trimmomatic\n        trimmed_by_trimmomatic = step_1PP_trimming__trimmomatic(trimming_illumina.other).trimmed\n\n        // fastp\n        trimmed_by_fastp = step_1PP_trimming__fastp(trimming_by_seqtype.ion.mix(trimming_illumina.bacteria)).trimmed\n\n        trimmed_by_trimmomatic.mix(trimmed_by_fastp).branch {\n            with_data: hasEnoughFastqData(it[1])\n            insufficient_number_of_reads: true\n        }\n        .set { trimmed_branched }\n        step_3TX_class__kraken(trimmed_branched.with_data)\n    emit:\n        no_reads = rawreads_branched.no_reads\n        trimmed_with_data = trimmed_branched.with_data\n        insufficient_number_of_reads = trimmed_branched.insufficient_number_of_reads\n}\n\nworkflow {\n    module_reads_processing(getInput())\n}\n"}
{"file_name": "module_vdraft_light.nf", "file_path": "/modules/module_vdraft_light.nf", "language": "nextflow", "id": "module_vdraft_light", "content": "nextflow.enable.dsl=2\n\ninclude { step_1PP_hostdepl__bowtie } from '../steps/step_1PP_hostdepl__bowtie'\ninclude { step_2AS_mapping__bowtie } from '../steps/step_2AS_mapping__bowtie'\ninclude { extractKey;getEmpty } from '../functions/common.nf'\ninclude { getSingleInput;getHostOptional;getReference } from '../functions/parameters.nf'\n\nworkflow module_vdraft_light {\n    take: \n        trimmedReads\n        host\n        reference\n    main:\n\n        trimmedReads.cross(host) { extractKey(it) }\n            .map { [ it[0][0], it[0][1], it[1][1] ] } //riscd, reads, host\n            .branch {\n                with_host: it[1][1]\n                without_host: true\n            }\n        .set { branchedTrimmed }\n\n        depleted = step_1PP_hostdepl__bowtie(branchedTrimmed.with_host)\n\n        branchedTrimmed.without_host\n            .mix(depleted)\n            .map { it[0,1] }\n            .set { trimmedOrDepleted }\n\n        trimmedOrDepleted.cross(reference) { extractKey(it) }\n        .multiMap { \n            reads: it[0] // riscd, reads\n            refs:  it[1][1..3] // riscd, code, path\n        }.set { readsAndReferences }\n\n        step_2AS_mapping__bowtie(readsAndReferences.reads, readsAndReferences.refs)\n    }\n\nworkflow {\n    module_vdraft_light(getSingleInput(), getHostOptional(), getReference('fa'))\n}"}
{"file_name": "module_scaffolds_filtering.nf", "file_path": "/modules/module_scaffolds_filtering.nf", "language": "nextflow", "id": "module_scaffolds_filtering", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_filtering__seqio } from '../steps/step_2AS_filtering__seqio'\ninclude { step_3TX_species__vdabricate } from '../steps/step_3TX_species__vdabricate'\ninclude { extractKey } from '../functions/common.nf'\ninclude { getInput;getReference;getDS } from '../functions/parameters.nf'\n\nworkflow module_scaffolds_filtering {    \n    take: \n        assembled\n        reference \n        abricatedatabase \n    main:\n        assembled.cross(abricatedatabase) { extractKey(it) }\n            .map { \n                [ it[0][0], it[0][1], it[1][1] ] //scaffolds, db\n            }.set { scaffoldsAndDatabase }\n        calls = step_3TX_species__vdabricate(scaffoldsAndDatabase)\n        calls\n            .cross(assembled) { extractKey(it) } // [ [riscd, calls], [riscd, assembly] ]\n            .cross(reference) { extractKey(it) } // [ [ [riscd, calls], [riscd, assembly] ], [ key, riscd, refid refpath ] ]\n            .multiMap { \n                calls: it[0][0]\n                assembly: it[0][1]\n                reference: it[1][1..3]\n            }.set { filt }\n        step_2AS_filtering__seqio(filt.calls, filt.assembly, filt.reference)\n}\n\nworkflow {\n    module_scaffolds_filtering(getInput(), getReference('fa'), Channel.of([ getDS(), 'viruses_TREF' ]))\n}"}
{"file_name": "module_draft_genome.nf", "file_path": "/modules/module_draft_genome.nf", "language": "nextflow", "id": "module_draft_genome", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_mapping__bowtie } from '../steps/step_2AS_mapping__bowtie'\ninclude { step_2AS_mapping__ivar } from '../steps/step_2AS_mapping__ivar'\ninclude { step_4AN_genes__prokka } from '../steps/step_4AN_genes__prokka'\ninclude { step_4TY_lineage__pangolin } from '../steps/step_4TY_lineage__pangolin'\ninclude { extractKey; getEmpty } from '../functions/common.nf'\ninclude { getSingleInput;getReferenceOptional;getReference } from '../functions/parameters.nf'\n\ndef PROKKA_KINGDOM = 'Viruses'\n\nworkflow module_draft_genome {\n    take: \n        reads\n        reference \n        referenceGB\n    main:\n        reads.cross(reference) { extractKey(it) }.multiMap { \n            reads: it[0] // riscd, reads\n            refs:  it[1][1..3] // riscd, code, path\n        }.set { readsAndReferences }\n\n        step_2AS_mapping__bowtie(readsAndReferences.reads, readsAndReferences.refs)\n\n        consensus = step_2AS_mapping__ivar(readsAndReferences.reads, readsAndReferences.refs).consensus\n\n        consensus.cross(referenceGB) { extractKey(it) }.map { \n            [ it[0][0], it[0][1], PROKKA_KINGDOM, it[1][1], it[1][2], it[1][3] ] // riscd assembly kingdom riscd_ref refid refpath]\n\n        }.set { consensusKingdomReference }\n\n        step_4AN_genes__prokka(consensusKingdomReference)\n    }\n\nworkflow  {\n    module_draft_genome(getSingleInput(), getReference('fa'), getReferenceOptional('gb'))\n}"}
{"file_name": "module_plasmids.nf", "file_path": "/modules/module_plasmids.nf", "language": "nextflow", "id": "module_plasmids", "content": "nextflow.enable.dsl=2\n\ninclude { step_4TY_plasmid__mobsuite } from '../steps/step_4TY_plasmid__mobsuite'\ninclude { isSpeciesSupported;taskMemory;flattenPath;parseMetadataFromFileName;executionMetadata } from '../functions/common.nf'\ninclude { getSingleInput;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent;param } from '../functions/parameters.nf'\n\nPOINFINDER_GENUS_ALLOWED = [\n  'campylobacter',\n  'enterococcus_faecalis',\n  'enterococcus_faecium',\n  'escherichia_coli',\n  'helicobacter_pylori',\n  'klebsiella',\n  'mycobacterium_tuberculosis',\n  'neisseria_gonorrhoeae',\n  'plasmodium_falciparum',\n  'salmonella',\n  'staphylococcus_aureus'\n]\n\ndef ex = executionMetadata()\n\ndef getPointfinderParam(gsp) {\n try {  \n    def genus_species = gsp ? gsp.toLowerCase() : ''\n    def (genus, species) = genus_species.contains(\"_\") ? genus_species.split('_') : [ genus_species, null ]\n    if (POINFINDER_GENUS_ALLOWED.contains(genus_species)) {\n        // genus_species allowed\n        return genus_species;\n    }       \n    if (POINFINDER_GENUS_ALLOWED.contains(genus)) {\n        // ALL genus allowed\n        return genus\n    }\n    return ''\n  } catch(Throwable t) {\n      exit 1, \"unexpected exception: ${t.asString()}\"\n  } \n}\n\nprocess staramr {\n    container \"ghcr.io/genpat-it/staramr:0.9.1--8fe6b5a239\"\n    containerOptions = \"--user root\"\n    tag \"${riscd_input}/${plasmid_name}\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      tuple val(riscd_input), path(plasmid_file)\n      val(genus_species)\n    output:\n      path '**'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/result\", pattern: 'result/{*.tsv,*.xlsx}', saveAs: { filename -> \"${base}_${flattenPath(filename)}\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/result\", pattern: 'result/hits/*', saveAs: { filename -> \"hits/${base}_hits_${flattenPath(filename) -~ /_DS.+/}.fasta\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/meta\", pattern: 'result/*.txt', saveAs: { filename -> \"${base}_${flattenPath(filename)}\" }  \n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/meta\", pattern: '*.json'\n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/plasmids/${plasmid_name}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      plasmid_name = plasmid_file.getName() -~ /\\.fa.*$/\n      base = \"${plasmid_name}_staramr\"\n      pointfinder_db = getPointfinderParam(genus_species)\n      extra = pointfinder_db ? \" --pointfinder-organism ${pointfinder_db}\" : ''\n      \"\"\"\n        sed 's/^>.*/>/g' ${plasmid_file} | awk '{for(i=1;i<=NF;i++){if(\\$i~/^>/){\\$i=\">contig\"++count}}} 1' > ${base}.fasta\n        staramr search ${extra} -o result ${base}.fasta\n      \"\"\"\n}\n\nworkflow module_plasmids {\n    take: \n        assembly\n        genus_species\n    main:\n        plasmids = step_4TY_plasmid__mobsuite(assembly).plasmids\n\n        plasmids.multiMap { it ->\n            riscd: it[0]\n            plasmids: it[1]\n        }.set { branched }\n\n        input = branched.riscd.combine(branched.plasmids.flatten())\n        staramr(input, genus_species)\n}\n\nworkflow {\n    module_plasmids(getSingleInput(), param('genus_species'))\n}"}
{"file_name": "module_filtered_denovo.nf", "file_path": "/modules/module_filtered_denovo.nf", "language": "nextflow", "id": "module_filtered_denovo", "content": "nextflow.enable.dsl=2\n\ninclude { step_1PP_filtering__bowtie } from '../steps/step_1PP_filtering__bowtie'\ninclude { step_2AS_denovo__spades } from '../steps/step_2AS_denovo__spades'\ninclude { extractKey } from '../functions/common.nf'\ninclude { getSingleInput;getReference } from '../functions/parameters.nf'\n\nworkflow module_filtered_denovo {\n    take: \n        reads\n        reference \n    main:\n        reads.cross(reference) { extractKey(it) }.multiMap { \n            reads: it[0] // riscd, reads\n            refs:  it[1][1..3] // riscd, code, path\n        }.set { readsAndReferences }\n\n        filtered = step_1PP_filtering__bowtie(readsAndReferences.reads, readsAndReferences.refs)\n        assembled = step_2AS_denovo__spades(filtered)\n    emit:\n        assembled\n        filtered\n}\n\nworkflow {\n    module_filtered_denovo(getSingleInput(), getReference('fa'))\n}"}
{"file_name": "module_denovo.nf", "file_path": "/modules/module_denovo.nf", "language": "nextflow", "id": "module_denovo", "content": "nextflow.enable.dsl=2\n\ninclude { step_1PP_hostdepl__bowtie } from '../steps/step_1PP_hostdepl__bowtie'\ninclude { step_2AS_denovo__spades } from '../steps/step_2AS_denovo__spades'\ninclude { extractKey; getEmpty } from '../functions/common.nf'\ninclude { getSingleInput;getHost } from '../functions/parameters.nf'\n\nworkflow module_denovo {\n    take: \n        trimmedReads\n        host \n    main:\n        trimmedReads.cross(host) { extractKey(it) }\n            .map { [ it[0][0], it[0][1], it[1][1] ] } //riscd, reads, host\n            .branch {\n                with_host: it[1][1]\n                without_host: true\n            }\n        .set { branchedTrimmed }\n\n        depleted = step_1PP_hostdepl__bowtie(branchedTrimmed.with_host)\n\n        branchedTrimmed.without_host\n            .mix(depleted)\n            .map { it[0,1] }\n            .set{ denovoInput }\n        assembled = step_2AS_denovo__spades(denovoInput)\n    emit:\n        assembled\n        depleted\n}\n\nworkflow {\n    module_denovo(getSingleInput(), getHost())\n}"}
{"file_name": "module_panaroo.nf", "file_path": "/modules/module_panaroo.nf", "language": "nextflow", "id": "module_panaroo", "content": "nextflow.enable.dsl=2\n\ninclude { getInput;param;optional } from '../functions/parameters.nf'\ninclude { flattenPath } from '../functions/common.nf'\n\nprocess panaroo {\n    container \"quay.io/biocontainers/panaroo:1.3.3--pyhdfd78af_0\"\n    input:\n      path gffs\n    output:\n      path '**'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"panaroo.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"panaroo.log\" }\n    script:\n      \"\"\"          \n         panaroo -i *.gff \\\n            -o results \\\n            --clean-mode ${param('multi_pangenome__panaroo__clean_mode')} \\\n            --remove-invalid-genes \\\n            --threshold ${param('multi_pangenome__panaroo__threshold')} \\\n            --family_threshold ${param('multi_pangenome__panaroo__family_threshold')} \\\n            --len_dif_percent ${param('multi_pangenome__panaroo__len_dif_percent')} \\\n            -t ${param('multi_pangenome__panaroo__threads')} \\\n            --alignment core \\\n            --aligner mafft \\\n            --merge_paralogs  \\\n            ${optional('multi_pangenome__panaroo__extra')}\n      \"\"\"\n}\n\nworkflow multi_pangenome__panaroo {\n    take: \n        input\n    main:      \n        input\n            .map { it[1] }\n            .collect()\n            .set { gffs }   \n        panaroo(gffs)\n}\n\nworkflow {\n    multi_pangenome__panaroo(getInput())\n}"}
{"file_name": "module_qc_quast.nf", "file_path": "/modules/module_qc_quast.nf", "language": "nextflow", "id": "module_qc_quast", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;param  } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\nprocess quast {\n    container 'quay.io/biocontainers/quast:4.4--boost1.61_1'\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 200.MB, task.attempt ) }   \n    maxForks 10\n    input:\n      tuple val(_), path(assembly)\n    output:\n      path '*_quast.tsv', emit: tsv\n      path '{*.sh,*.log}', hidden: true \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '*.tsv'   \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n        md = parseMetadataFromFileName(assembly.getName())\n        base = \"${md.ds}-${ex.dt}_${md.cmp}_quast\"\n        \"\"\"\n        quast -m 200 --fast -o quast ${assembly}\n        cut -f1,14,15,16,17,18,19,20,21 quast/transposed_report.tsv > ${base}.tsv \n        \"\"\"\n}\n\nprocess summary {\n    container 'ubuntu:20.04'\n    memory { taskMemory( 200.MB, task.attempt ) }\n    when:\n      (quast_results instanceof java.util.Collection) && quast_results.size() > 1      \n    input:\n      path(quast_results)\n    output:\n      path '*.tsv'\n      path '{*.sh,*.log}', hidden: true \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '*.tsv'   \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n        base = \"summary\"\n        \"\"\"\n        cat ${quast_results} | sort -u > ${base}.tsv\n        \"\"\"\n}\n\nworkflow module_qc_quast {\n    take: \n      input\n    main:\n      quast(input).tsv.collect() | summary\n}\n\nworkflow {\n    tsv = quast(getInput()).tsv.collect()\n    summary(tsv)\n}\n\n"}
{"file_name": "module_wgs_bacteria.nf", "file_path": "/modules/module_wgs_bacteria.nf", "language": "nextflow", "id": "module_wgs_bacteria", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_denovo__shovill } from '../steps/step_2AS_denovo__shovill'\ninclude { getSingleInput } from '../functions/parameters.nf'\n\nworkflow module_wgs_bacteria {\n    take: \n      trimmedReads\n    main:\n        step_2AS_denovo__shovill(trimmedReads)\n    emit:\n        step_2AS_denovo__shovill.out\n}\n\nworkflow {\n    module_wgs_bacteria(getSingleInput())\n}"}
{"file_name": "module_qc_nanoplot.nf", "file_path": "/modules/module_qc_nanoplot.nf", "language": "nextflow", "id": "module_qc_nanoplot", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;param;isCompatibleWithSeqType;isIlluminaPaired   } from '../functions/parameters.nf'\ninclude { stepInputs;parseRISCD } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\nprocess module_qc_nanoplot {\n    container 'quay.io/biocontainers/nanoplot:1.41.3--pyhdfd78af_0'\n    memory { taskMemory( 2.GB, task.attempt ) }\n    cpus { [8, params.max_cpus as int].min() }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    maxForks 10\n    when:\n      isCompatibleWithSeqType(reads, ['nanopore'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true \n    afterScript \"echo '${stepInputs(riscd_input, md2, [dt: md2.dt], md2.acc, md2.met, [seq_type:'nanopore'])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.txt,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n\n    script:\n      md = parseMetadataFromFileName(reads.getName())\n      md2 = parseRISCD(riscd_input)       \n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      \"\"\"\n      NanoPlot -t ${task.cpus} --fastq ${reads} --tsv_stats --no_static -o . -p ${base}_\n      \"\"\"\n}\n\nworkflow {\n    module_qc_nanoplot(getInput())\n}\n\n"}
{"file_name": "module_enterotoxin_saureus_finder.nf", "file_path": "/modules/module_enterotoxin_saureus_finder.nf", "language": "nextflow", "id": "module_enterotoxin_saureus_finder", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_denovo__unicycler } from '../steps/step_2AS_denovo__unicycler'\ninclude { step_4AN_AMR__blast } from '../steps/step_4AN_AMR__blast'\ninclude { getSingleInput;getGenusSpeciesOptional } from '../functions/parameters.nf'\ninclude { extractKey } from '../functions/common.nf'\n\nworkflow module_enterotoxin_saureus_finder {\n    take: \n        trimmed\n        genus_species\n    main:\n        assembly = step_2AS_denovo__unicycler(trimmed)\n\n        assembly.cross(genus_species) { extractKey(it) }\n            .multiMap { \n                assembly: it[0]\n                species: it[1]\n            }.set { assemblyAndSpecies }\n        step_4AN_AMR__blast(assemblyAndSpecies.assembly, assemblyAndSpecies.species)\n}\n\nworkflow {\n    module_enterotoxin_saureus_finder(getSingleInput(), getGenusSpeciesOptional())\n}\n"}
{"file_name": "module_segmented.nf", "file_path": "/modules/module_segmented.nf", "language": "nextflow", "id": "module_segmented", "content": "nextflow.enable.dsl=2\n\ninclude { extractKey } from '../functions/common.nf'\ninclude { step_2AS_mapping__ivar } from '../steps/step_2AS_mapping__ivar'\ninclude { getSingleInput;getReferences } from '../functions/parameters.nf'\n\nworkflow module_segmented {\n    take: \n        reads\n        reference \n    main:\n        step_2AS_mapping__ivar(reads, reference)\n    emit:\n        step_2AS_mapping__ivar.out.consensus\n}\n\nworkflow {\n    getSingleInput().cross(getReferences('any')) { extractKey(it) }\n      .multiMap { \n          reads: it[0] // riscd, R[]\n          refs:  it[1][1..3] // riscd, code, path\n      }.set { input }\n\n    module_segmented(input.reads, input.refs)\n}"}
{"file_name": "module_ksnp3.nf", "file_path": "/modules/module_ksnp3.nf", "language": "nextflow", "id": "module_ksnp3", "content": "nextflow.enable.dsl=2\n\ninclude { getInput;param } from '../functions/parameters.nf'\ninclude { taskMemory;flattenPath } from '../functions/common.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nprocess ksnp3 {\n    container \"ghcr.io/genpat-it/ksnp3:3.0--addd2c2d0e\"\n    input:\n      path(assembly)\n      val(kmers_size)\n      val(analysis_type)\n    output:\n      path(\"results/${outfile}\"), emit: fasta\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}\", pattern: \"*results/${outfile}\", saveAs: { \"matrix.fasta\" }   \n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*results/*.fasta', saveAs: { filename -> flattenPath(filename) }   \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"ksnp3.cfg\" }\n    script:\n      extra_params = (analysis_type == 'core' ? '-core' : '')\n      outfile = (analysis_type == 'core' ? 'core_SNPs_matrix.fasta' : 'SNPs_all_matrix.fasta')\n      \"\"\"\n        trap \"rm -Rf results/TemporaryFilesToDelete\" EXIT\n        for FILE in ${assembly}; do \\\n          CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/' | sed 's/\\\\./-/g'`; \\\n          ln -s \\$FILE \\${CMP}.fasta ; \\\n          echo -e \"`pwd`/\\${CMP}.fasta\\t\\$CMP\" >> input.tsv ; \\\n        done        \n        kSNP3 -in input.tsv -k ${kmers_size} -NJ ${extra_params} -outdir results > ksnp3.log\n      \"\"\"\n}\n\nprocess iqtree {\n    container \"quay.io/biocontainers/iqtree:1.6.12--he513fc3_0\"\n    input:\n      path(fasta)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n      path(\"*.treefile\"), emit: nwk\n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*.treefile', saveAs: { \"matrix.nwk\" } \n    publishDir mode: 'copy', \"${params.outdir}\", pattern: '*.treefile', saveAs: { filename -> filename.replace(\".treefile\", \".nwk\") } \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"iqtree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"iqtree.cfg\" }\n    script:\n      \"\"\"\n        iqtree -nt AUTO -s ${fasta}      \n        sed -i 's/-/./g' *.treefile\n      \"\"\"\n}\n\nprocess grapetree {\n    container \"quay.io/biocontainers/grapetree:2.1--pyh3252c3a_0\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"grapetree.cfg\" }\n    script:\n      \"\"\"\n        grapetree -p ${matrix} > matrix.nwk\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__ksnp3 {\n    take: \n        input\n        kmers_size\n        analysis_type\n        metadata\n        geodata\n    main:\n        input\n          .map { it[1] }\n          .collect()\n          .set { inputSet }\n        matrix = ksnp3(inputSet, kmers_size, analysis_type).fasta\n        iqtree(matrix)\n        grapetree(matrix)\n        augur(iqtree.out.nwk, metadata, geodata)\n}\n\nworkflow {\n    multi_clustering__ksnp3(getInput(), param('kmers_size'), param('analysis_type'), param('metadata'), param('geodata'));\n}\n"}
{"file_name": "module_vcf2mst.nf", "file_path": "/modules/module_vcf2mst.nf", "language": "nextflow", "id": "module_vcf2mst", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory } from '../functions/common.nf'\ninclude { getVCFs } from '../functions/parameters.nf'\n\nprocess vcf2mst {\n    container \"ghcr.io/genpat-it/vcf2mst:0.0.1--d587d682e9\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(vcf_files)\n    output:\n      path '*'\n      path 'HDmatrix.tsv', emit: matrix\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'tree.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"vcf2mst.cfg\" }\n    script:\n      \"\"\"\n        mkdir vcf_files \n        for FILE in ${vcf_files}; do CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([[:digit:]]+\\\\.[[:alnum:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+\\\\.[[:digit:]]+).+/\\\\1/'`; cp \\$FILE vcf_files/\\${CMP} ; done\n        find vcf_files -mindepth 1 > vcf_list            \n        vcf2mst.pl vcf_list tree.nwk vcf > vcf2mst.log\n        cp /tmp/hamming_distance_matrix.tsv HDmatrix.tsv\n      \"\"\"\n}\n\nprocess dists {\n    container \"quay.io/biocontainers/cgmlst-dists:0.4.0--hec16e2b_2\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(matrix)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"cgmlst-dists.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cgmlst-dists.cfg\" }\n    script:\n      \"\"\"\n        cgmlst-dists -c ${matrix} > vcf2mst_dists_matrix.csv\n      \"\"\"\n}\n\n\nworkflow multi_clustering__vcf2mst {\n    take: \n        input\n    main:\n        matrix=vcf2mst(input).matrix\n        dists(matrix)\n}\n\nworkflow {\n    getVCFs()\n    .map { it[1] }\n    .collect()\n    .set { inputSet }\n    multi_clustering__vcf2mst(inputSet)\n}\n"}
{"file_name": "module_covid_emergency.nf", "file_path": "/modules/module_covid_emergency.nf", "language": "nextflow", "id": "module_covid_emergency", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_mapping__ivar } from '../steps/step_2AS_mapping__ivar'\ninclude { step_4TY_lineage__pangolin } from '../steps/step_4TY_lineage__pangolin'\ninclude { extractKey } from '../functions/common.nf'\ninclude { getSingleInput } from '../functions/parameters.nf'\n\ndef referenceCode = 'NC_045512.2'\ndef referencePath = \"${params.assets_dir}/module_covid_emergency/NC_045512.fasta\"\ndef referenceRiscd = '220308-020220308005121273-2AS_import-external'\n\nworkflow module_covid_emergency {\n    take: \n        trimmed\n    main:\n        trimmed.multiMap {\n            trimmed: it\n            reference: [ referenceRiscd, referenceCode, file(referencePath) ]\n        }.set { trAndRef }\n        consensus = step_2AS_mapping__ivar(trAndRef.trimmed, trAndRef.reference).consensus\n        step_4TY_lineage__pangolin(consensus)\n    }\n\nworkflow {\n    module_covid_emergency(getSingleInput())\n}"}
{"file_name": "module_cfsan.nf", "file_path": "/modules/module_cfsan.nf", "language": "nextflow", "id": "module_cfsan", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory;flattenPath } from '../functions/common.nf'\ninclude { getReferenceUnkeyed;getResult;getInput;param } from '../functions/parameters.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nIMAGES = [\n  '2.2.1': 'staphb/cfsan-snp-pipeline:2.2.1',\n  '2.0.2': 'cfsanbiostatistics/snp-pipeline@sha256:448787923371ade95217982814db25efb1e01287a8180b523d76a9f093f97d01'\n]\n\ndef DOCKER_IMAGE = IMAGES[param('multi_clustering__cfsan__version')] ?: (exit 2, \"params (multi_clustering__cfsan__version) not valid\");\n\nprocess cfsan_snp_pipeline {\n    container DOCKER_IMAGE\n    containerOptions = \"-v ${workflow.projectDir}/scripts/multi_clustering__cfsan:/scripts:ro\"\n    input:\n      path(samples)\n      tuple val(_), val(reference), path(refPath)\n    output:\n      path '**'\n      path 'results/snpma.fasta', emit: snpma\n      path '{*.sh,*.log}', hidden: true\n    stageInMode 'symlink'  \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*.tsv', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'results/*.vcf', saveAs: { filename -> flattenPath(filename) }      \n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'results/*.fasta', saveAs: { filename -> flattenPath(filename) }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cfsan_snp_pipeline.cfg\" }\n    script:\n      version = param('multi_clustering__cfsan__version')\n      \"\"\"\n        trap \"find  \\\\( -name '*.bam' -o -name '*.sam' -o -name '*.pileup' \\\\) -delete\" EXIT\n        for FILE in ${samples}; do CMP=`echo \\$FILE | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`; mkdir -p samples/\\${CMP} && mv \\$FILE samples/\\${CMP}/ ; done\n        cfsan_snp_pipeline run -c /scripts/snppipeline_${version}.conf -m soft -o results --samples_dir samples ${refPath} >> cfsan_snp_pipeline.log\n      \"\"\"\n}\n\nprocess iqtree {\n    container \"quay.io/biocontainers/iqtree:1.6.12--he513fc3_0\"\n    stageInMode 'copy'\n    input:\n      path(snpma)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n      path(\"snpma.fasta.treefile\"), emit: nwk\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'snpma.fasta.treefile', saveAs: { \"snpma.fasta.nwk\" } \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"iqtree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"iqtree.cfg\" }\n    script:\n      \"\"\"\n        iqtree -s ${snpma} -nt AUTO\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__cfsan {\n    take: \n        input\n        reference\n        metadata\n        geodata\n    main:\n        snpma = cfsan_snp_pipeline(input, reference).snpma\n        nwk = iqtree(snpma).nwk\n        augur(nwk, metadata, geodata)\n}\n\nworkflow {\n    reads = getInput()\n        .map { it[1] }\n        .toSortedList( { a, b -> a[0] <=> b[0] } )\n        .flatten()     \n        .collect()  \n    multi_clustering__cfsan(reads, getReferenceUnkeyed('fa'), param('metadata'), param('geodata'));\n}"}
{"file_name": "module_snippycore.nf", "file_path": "/modules/module_snippycore.nf", "language": "nextflow", "id": "module_snippycore", "content": "nextflow.enable.dsl=2\n\ninclude { flattenPath; parseMetadataFromFileName; executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;getReferenceUnkeyed;getInputFolders;isIlluminaPaired;isCompatibleWithSeqType;isIonTorrent  } from '../functions/parameters.nf'\n\ndef ex = executionMetadata()\n\ndef ENTRYPOINT = \"multi_alignment__snippycore\"\n\nprocess snippy {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    memory { taskMemory( 8.GB, task.attempt ) }\n    when:\n      isCompatibleWithSeqType(reads, ['ion','illumina_paired'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n      tuple val(riscd_ref), val(reference), path(reference_path)\n    output:\n      path 'snippy', emit: results\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base_ref}.log\" }    \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base_ref}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      base_ref = \"${base}_snippy_${reference}\"\n      is_fasta = r1.getName() ==~ /.+\\.fa(sta)?$/\n      if (is_fasta) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --ctgs ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"     \n      } else if (isIlluminaPaired(reads)) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --R1 ${r1} --R2 ${r2} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"\n      } else if (isIonTorrent(reads)) {\n        \"\"\"\n        trap \"find -name \"*.?am\" -delete ; rm -Rf tmp\" EXIT\n        mkdir tmp\n        snippy --reference ${reference_path} --se ${r1} --outdir snippy --prefix ${base_ref} --quiet  --tmpdir tmp  &> ${base_ref}-cli.log\n        \"\"\"      \n      }      \n}\n\n\nprocess snippy_core {\n    container \"ghcr.io/genpat-it/snippy:4.5.1--7be4a1c45a\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path vcf_files, stageAs: 'data?'\n      tuple val(ref_riscd), val(ref_code), path(ref_file)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{core*}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"snippy_core.cfg\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log', saveAs: { \"snippy_core.log\" }\n    script:\n      // XXX renaming folders getting sample name from the first vcf file inside\n      \"\"\"          \n        #!/bin/bash -euo pipefail\n        mkdir inputs && cd inputs && for dir in ${vcf_files}; do ln -s ../\\${dir} `ls ../\\${dir}/*.vcf | head -n 1 | sed -E 's/.+DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`; done && cd ..\n        snippy-core --ref ${ref_file} --prefix core --inprefix snps inputs/*\n      \"\"\"\n}\n\nworkflow multi_alignment__snippycore {\n    take: \n        reads      \n        reference\n    main:\n        reads.combine(reference)\n                .multiMap { \n                    reads: it[0..1] // riscd, R[]\n                    reference:  it[2..4] // riscd, code, path\n                }.set { input }\n        folders = snippy(input.reads,input.reference).results\n        snippy_core(folders.collect(), reference)\n}\n\nworkflow multi_alignment__snippycore_vcf {\n    take: \n        input\n        reference\n    main:\n        input\n            .map { it[1] }\n            .collect()\n            .set { vcfs }       \n        snippy_core(vcfs, reference)\n}\n\nworkflow {\n    // 1PP_* => snippy + snippycore\n    multi_alignment__snippycore(getInput().filter( ~/^.*\\/1PP_.*/ ), getReferenceUnkeyed('gb'))\n    // 2AS_* => snippycore only\n    multi_alignment__snippycore_vcf(getInputFolders().filter( ~/^.*\\/2AS_.*/ ), getReferenceUnkeyed('gb'))\n}"}
{"file_name": "module_westnile.nf", "file_path": "/modules/module_westnile.nf", "language": "nextflow", "id": "module_westnile", "content": "nextflow.enable.dsl=2\n\n// include { module_denovo } from '../modules/module_denovo'\n// include { module_scaffolds_filtering } from '../modules/module_scaffolds_filtering'\ninclude { step_4TY_lineage__westnile;getReferenceForLineage } from '../steps/step_4TY_lineage__westnile'\ninclude { step_2AS_mapping__ivar } from '../steps/step_2AS_mapping__ivar'\n\ninclude { extractKey } from '../functions/common.nf'\ninclude { getSingleInput } from '../functions/parameters.nf'\n\nworkflow module_westnile {\n    take: \n        reads        \n    main:\n        lineage = step_4TY_lineage__westnile(reads)\n\n        reads.cross(lineage) { extractKey(it) }.multiMap { \n            reads: it[0]\n            reference: getReferenceForLineage(it[1][1])\n        }.set { readsAndRef }\n        \n        step_2AS_mapping__ivar(readsAndRef.reads, readsAndRef.reference).consensus\n}\n\nworkflow {\n    module_westnile(getSingleInput())\n}\n"}
{"file_name": "module_vdraft.nf", "file_path": "/modules/module_vdraft.nf", "language": "nextflow", "id": "module_vdraft", "content": "nextflow.enable.dsl=2\n\ninclude { module_denovo } from '../modules/module_denovo'\ninclude { module_scaffolds_filtering } from '../modules/module_scaffolds_filtering'\ninclude { module_draft_genome } from '../modules/module_draft_genome'\ninclude { extractKey } from '../functions/common.nf'\ninclude { getSingleInput;getHost;getReference;getReferenceOptional;getDS } from '../functions/parameters.nf'\n\nworkflow module_vdraft {\n    take: \n        reads\n        host\n        reference\n        referenceGB\n        abricateDatabase\n    main:\n        denovoOut = module_denovo(reads, host);\n\n        denovoOut.assembled\n            .cross(reference) { extractKey(it) }\n            .cross(abricateDatabase) { extractKey(it) }.multiMap { \n                assembly: it[0][0][0..1]\n                reference: it[0][1]\n                abricateDatabase: it[1]\n            }.set { cARA }\n        module_scaffolds_filtering(cARA.assembly, cARA.reference, cARA.abricateDatabase)\n        \n        denovoOut.depleted\n            .cross(reference) { extractKey(it) }\n            .cross(referenceGB) { extractKey(it) }\n            .multiMap {\n                depleted: it[0][0][0..1]\n                reference: it[0][1]\n                referenceGB: it[1]\n            }\n            .set { cDR }\n        module_draft_genome(cDR.depleted, cDR.reference, cDR.referenceGB)\n    }\n\nworkflow {\n    module_vdraft(getSingleInput(), getHost(), getReference('fa'), getReferenceOptional('gb'), Channel.of([ getDS(), 'viruses_TREF' ]))\n}\n"}
{"file_name": "module_grapetree.nf", "file_path": "/modules/module_grapetree.nf", "language": "nextflow", "id": "module_grapetree", "content": "nextflow.enable.dsl=2\n\ninclude { parseRISCD;taskMemory } from '../functions/common.nf'\ninclude { _getAlleles;param } from '../functions/parameters.nf'\n\ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')\ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')\n\nprocess extract_cgMLST {\n    container \"ghcr.io/genpat-it/chewbbaca-w-chewie-schemas:2.8.5--16b816c96d\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(alleles)\n    output:\n      path '**'\n      path 'cgMLST.tsv', emit: cgMLST\n      path '*.sh', hidden: true\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'mdata_stats.tsv', saveAs: { \"missing_loci.tsv\" }\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: 'cgMLST.tsv', saveAs: { \"cgMLST.tsv\" }\n    publishDir mode: 'copy', \"${params.outdir}/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"extract_cgMLST.cfg\" }\n    script:\n      \"\"\"\n        for file in ${alleles} ; do awk 'FNR==1{print \"\"}1' \\${file} | sed 's/,/\\t/g' | sed -E \"s/^[^SF][^ai]\\\\S+/\\${file}/\" | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'; done | sort -ru  > results_alleles_all.tsv\n        chewie ExtractCgMLST -i results_alleles_all.tsv -o . > extract_cgMLST.log\n      \"\"\"\n}\n\nprocess dists {\n    container \"quay.io/biocontainers/cgmlst-dists:0.4.0--hec16e2b_2\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(cgMLST)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.csv}'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"cgmlst-dists.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"cgmlst-dists.cfg\" }\n    script:\n      \"\"\"\n        cgmlst-dists -c ${cgMLST} > cgMLST_dists_matrix.csv\n      \"\"\"\n}\n\nprocess grapetree {\n    container \"quay.io/biocontainers/grapetree:2.1--pyh3252c3a_0\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(cgMLST)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n      path(\"cgMLST_NJ.nwk\"), emit: nwk_nj\n    publishDir mode: 'copy', \"${params.outdir}/result\", pattern: '*.nwk'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"grapetree.cfg\" }\n    script:\n      \"\"\"\n        grapetree -p ${cgMLST} > cgMLST.nwk\n        grapetree --method RapidNJ -p ${cgMLST} > cgMLST_NJ.nwk\n      \"\"\"\n}\n\nprocess augur {\n    container \"quay.io/biocontainers/augur:22.0.0--pyhdfd78af_0\"\n    memory { taskMemory( 4.GB, task.attempt ) }\n    input:\n      path(nwk)\n      path(metadata)\n      path(geodata)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json'\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '*.log', saveAs: { \"augur.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"augur.cfg\" }\n    script:\n      \"\"\"\n        cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' \\\n           | sed 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv\n        METADATA_LIST=\\$(head -n 1 augur_metadata.tsv | tr \\$'\\t' ' ')\n        augur refine --tree ${nwk} --output-tree tree_tt.nwk --output-node-data refine.node.json --metadata augur_metadata.tsv\n        augur export v2 --tree tree_tt.nwk --node-data refine.node.json --output auspice.json \\\n          --color-by-metadata \\${METADATA_LIST} \\\n          --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n          --metadata augur_metadata.tsv \\\n          --lat-longs ${geodata}\n      \"\"\"\n}\n\nworkflow multi_clustering__grapetree {\n    take: \n        input\n        metadata\n        geodata\n    main:\n        cgMLST = extract_cgMLST(input.collect()).cgMLST\n        dists(cgMLST)\n        nwk_nj = grapetree(cgMLST).nwk_nj\n        augur(nwk_nj, metadata, geodata)\n}\n\nworkflow {\n    multi_clustering__grapetree(getInput(), param('metadata'), param('geodata'));\n}\n\ndef getInput() {\n    if (!params.containsKey('input')) {\n      exit 2, \"missing required param: input\";\n    }\n    def schema = params.containsKey('schema') ? params.schema : null\n    assert params.input instanceof ArrayList\n    params.input.inject(Channel.empty()) {\n        res, val -> res.mix(_getAlleles(val.cmp, val.riscd, schema))\n    }        \n}\n\n"}
{"file_name": "module_qc_fastqc.nf", "file_path": "/modules/module_qc_fastqc.nf", "language": "nextflow", "id": "module_qc_fastqc", "content": "nextflow.enable.dsl=2\n\ninclude { parseMetadataFromFileName;executionMetadata;taskMemory } from '../functions/common.nf'\ninclude { getInput;param;isCompatibleWithSeqType;isIlluminaPaired   } from '../functions/parameters.nf'\ninclude { stepInputs;parseRISCD } from '../functions/common.nf'\n\ndef ex = executionMetadata()\n\nprocess module_qc_fastqc {\n    container 'biocontainers/fastqc:v0.11.5_cv4'\n    memory { taskMemory( 1.GB, task.attempt ) }\n    tag \"${md?.cmp}/${md?.ds}/${md?.dt}\"\n    maxForks 10\n    when:\n      isCompatibleWithSeqType(reads, ['illumina_paired','ion'], task.process)\n    input:\n      tuple val(riscd_input), path(reads)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true \n    afterScript \"echo '${stepInputs(riscd_input, md2, [dt: md2.dt], md2.acc, md2.met, [seq_type:seq_type])}' > ${base}_input.json\"\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: '{*.zip,*.html}', saveAs: { filename -> filename.replaceFirst(\"-DT\\\\d+_\", \"-${ex.dt}_\") }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"${base}.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"${base}.cfg\" }\n    script:\n      (r1,r2) = (reads instanceof java.util.Collection) ? reads : [reads, null]\n      md = parseMetadataFromFileName(r1.getName())\n      md2 = parseRISCD(riscd_input)   \n      base = \"${md.ds}-${ex.dt}_${md.cmp}\"\n      seq_type = isIlluminaPaired(reads) ? 'illumina_paired' : 'ion'\n      \"\"\"\n      fastqc $reads &> \"${base}_fastqc.log\" \n      \"\"\"\n}\n\nworkflow {\n    module_qc_fastqc(getInput())\n}\n\n"}
{"file_name": "module_augur.nf", "file_path": "/modules/module_augur.nf", "language": "nextflow", "id": "module_augur", "content": "nextflow.enable.dsl=2\n\ninclude { taskMemory } from '../functions/common.nf'\ninclude { getInput;getReferenceUnkeyed;param;optional } from '../functions/parameters.nf'\n\ndef SUMMARY_DATE_ALIASES = param('multi_clustering__reportree__summary_date_aliases')  \ndef SAMPLE_COLUMN = param('multi_clustering__reportree__summary_sample_column')  \ndef GEO_RESOLUTION_COLUMNS = param('multi_clustering__reportree__summary_geo_column')  \ndef TRAITS_COLUMNS = param('multi_clustering__augur__traits_columns') \n\nprocess align {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 5.GB, task.attempt ) }\n    input:\n      path(sequences)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'alignment.fasta', emit: alignment\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'alignment.fasta' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"align.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"align.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__align_extra')  \n      \"\"\"\n        for f in ${sequences}; do \n          cmp=`echo -n \\$f | sed -E 's/DS[[:digit:]]+-DT[[:digit:]]+_([^_]+)_[[:graph:]]+/\\\\1/'`\n          awk \"NR==1 {\\\\\\$0=\\\\\">\\$cmp\\\\\"}1\" \\$f > seq_\\$f\n        done     \n        augur align \\\n        --nthreads auto \\\n        --sequences seq_* \\\n        --reference-sequence ${ref_path} \\\n        --remove-reference \\\n        --output alignment.fasta \\\n        --fill-gaps ${extra}  \n      \"\"\"\n}\n\nprocess tree {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    input:\n      path(alignment)\n    output:\n      path '*'\n      path 'tree_raw.nwk', emit: tree_raw\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'tree_raw.nwk' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"tree.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"tree.cfg\" }\n    script:\n      method = param('multi_clustering__augur__tree_method')  \n      extra = optional('multi_clustering__augur__tree_extra')  \n      \"\"\"\n        augur tree \\\n          --alignment ${alignment} \\\n          --output tree_raw.nwk \\\n          --method ${method} \\\n          --nthreads auto ${extra}\n      \"\"\"\n}\n\nprocess refine {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(alignment)\n      path(metadata)\n    output:\n      path '*'\n      path 'tree.nwk', emit: tree\n      path 'branch_lengths.json', emit: branch_lengths\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'tree.nwk' \n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'branch_lengths.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"refine.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"refine.cfg\" }\n    script:\n      coalescent = param('multi_clustering__augur__refine_coalescent')  \n      extra = optional('multi_clustering__augur__refine_extra')  \n      \"\"\"\n         augur refine \\\n            --tree ${tree} \\\n            --alignment ${alignment} \\\n            --metadata ${metadata} \\\n            --output-tree tree.nwk \\\n            --output-node-data branch_lengths.json \\\n            --timetree \\\n            --coalescent ${coalescent} \\\n            --date-confidence ${extra}\n      \"\"\"\n}\n\nprocess ancestral {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(alignment)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'nt_muts.json', emit: nt_muts\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'nt_muts.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"ancestral.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"ancestral.cfg\" }\n    script:\n      inference = param('multi_clustering__augur__ancestral_inference')  \n      extra = optional('multi_clustering__augur__ancestral_extra')      \n      \"\"\"\n        augur ancestral \\\n            --tree ${tree} \\\n            --alignment ${alignment} \\\n            --root-sequence ${ref_path} \\\n            --output-node-data nt_muts.json \\\n            --inference ${inference} ${extra}\n      \"\"\"\n}\n\nprocess translate {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(nt_muts)\n      tuple val(_), val(ref_code), path(ref_path)\n    output:\n      path '*'\n      path 'aa_muts.json', emit: aa_muts\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'aa_muts.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"translate.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"translate.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__translate_extra')      \n      \"\"\"\n        augur translate \\\n            --tree ${tree} \\\n            --ancestral-sequences ${nt_muts} \\\n            --reference-sequence ${ref_path} \\\n            --output aa_muts.json ${extra}\n      \"\"\"\n}\n\nprocess traits {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(metadata)\n    output:\n      path '*'\n      path 'traits.json', emit: traits\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'traits.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"traits.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"traits.cfg\" }\n    script:\n      columns = TRAITS_COLUMNS   \n      extra = optional('multi_clustering__augur__traits_extra')      \n      \"\"\"\n        augur traits \\\n            --tree ${tree} \\\n            --metadata ${metadata} \\\n            --output-node-data traits.json \\\n            --columns ${columns} \\\n            --confidence ${extra}\n      \"\"\"\n}\n\nprocess prepare_metadata {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(metadata)\n    output:\n      path '*'\n      path 'augur_metadata.tsv', emit: metadata\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'augur_metadata.tsv' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"prepare_metadata.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"prepare_metadata.cfg\" }\n    script:\n      \"\"\"\n         cat ${metadata} | sed 's/${SAMPLE_COLUMN}/name/i' | sed -E 's/${SUMMARY_DATE_ALIASES}/date/i' > augur_metadata.tsv \n      \"\"\"\n}\n\nprocess export {\n    container \"quay.io/biocontainers/augur:23.1.1--pyhdfd78af_1\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    input:\n      path(tree)\n      path(metadata)\n      path(branch_lengths)\n      path(traits)\n      path(nt_muts)\n      path(aa_muts)\n      path(lat_longs)\n    output:\n      path '*'\n      path 'auspice.json', emit: auspice\n    publishDir mode: 'rellink', \"${params.outdir}/result\", pattern: 'auspice.json' \n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.log', saveAs: { \"export.log\" }\n    publishDir mode: 'rellink', \"${params.outdir}/meta\", pattern: '.command.sh', saveAs: { \"export.cfg\" }\n    script:\n      extra = optional('multi_clustering__augur__export_extra')      \n      \"\"\"\n        METADATA_LIST=\\$(head -n 1 ${metadata} | tr \\$'\\t' ' ')\n        augur export v2 \\\n            --tree ${tree} \\\n            --metadata ${metadata} \\\n            --node-data ${branch_lengths} ${traits} ${nt_muts} ${aa_muts} \\\n            --lat-longs ${lat_longs} \\\n            --color-by-metadata \\${METADATA_LIST} \\\n            --geo-resolutions ${GEO_RESOLUTION_COLUMNS} \\\n            --output auspice.json ${extra}\n      \"\"\"\n}\n\nworkflow multi_clustering__augur {\n    take: \n      reference\n      raw_metadata\n      geodata\n      ref2\n    main:  \n      metadata = prepare_metadata(raw_metadata).metadata\n      fastas = getInput().flatMap { it[1] }.collect()\n      alignment = align(fastas, reference).alignment\n      tree_raw = tree(alignment).tree_raw\n      tree = refine(tree_raw, alignment, metadata).tree\n      nt_muts = ancestral(tree, alignment, reference).nt_muts\n      aa_muts = translate(tree, nt_muts, reference).aa_muts\n      traits = traits(tree, metadata).traits\n      export(tree, metadata, refine.out.branch_lengths, traits, nt_muts, aa_muts, geodata)\n  }\n\nworkflow {\n    reference = getReferenceUnkeyed('gb')      \n    multi_clustering__augur(reference, param('metadata'), param('geodata'), reference)\n}\n"}
{"file_name": "module_surveillance.nf", "file_path": "/modules/module_surveillance.nf", "language": "nextflow", "id": "module_surveillance", "content": "nextflow.enable.dsl=2\n\ninclude { getVCFs;param;optionalOrDefault } from '../functions/parameters.nf'\ninclude { taskMemory;getEmpty } from '../functions/common.nf'\n\nif (getReportreeInputType() == 'alleles') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alleles\"\n  include { getAlleles as inputFn } from \"../multi/multi_clustering__reportree_alleles\"\n} else if (getReportreeInputType() == 'alignment') {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_alignment\"\n  include { getInput as inputFn } from '../functions/parameters.nf'\n} else {\n  include { multi_clustering__reportree } from \"../multi/multi_clustering__reportree_vcf\"\n  include { getVCFs as inputFn } from '../functions/parameters.nf'\n}\n\ndef getReportreeInputType() {\n    def res = param('multi_clustering__reportree__input')\n    if (!(res in ['alleles', 'vcf', 'alignment'])) {\n        exit 2, \"params (multi_clustering__reportree__input) not valid\"    \n    } \n    return res\n}\n\nworkflow {    \n    multi_clustering__reportree(inputFn(),  param('metadata'), param('geodata'), optionalOrDefault('multi_clustering__reportree__nomenclature', getEmpty()));\n}"}
{"file_name": "module_ngsmanager.nf", "file_path": "/modules/module_ngsmanager.nf", "language": "nextflow", "id": "module_ngsmanager", "content": "nextflow.enable.dsl=2\n\ninclude { module_reads_processing } from '../modules/module_reads_processing'\ninclude { module_wgs_bacteria } from '../modules/module_wgs_bacteria'\ninclude { module_typing_bacteria } from '../modules/module_typing_bacteria'\ninclude { module_covid_emergency } from '../modules/module_covid_emergency'\ninclude { module_vdraft } from '../modules/module_vdraft'\ninclude { module_westnile } from '../modules/module_westnile'\ninclude { module_qc_quast } from '../modules/module_qc_quast'\ninclude { step_4TY_lineage__pangolin } from '../steps/step_4TY_lineage__pangolin'\n\n\ninclude { getSingleInput;param } from '../functions/parameters.nf'\ninclude { getEmpty;extractKey;isFastqRiscd;isFastaRiscd } from '../functions/common.nf'\ninclude { isNegativeControl;isNegativeControlSarsCov2;isPositiveControlSarsCov2;isVirus;isWNV;isBacterium;isSarsCov2;isAmpliseq } from '../functions/sampletypes.nf'\n\nworkflow ngsmanager_fastq {\n    take: \n      rawReads  \n    main:\n      try {       \n        trimmed = module_reads_processing(rawReads).trimmed_with_data\n\n        trimmed.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        assemblyBacteria = module_wgs_bacteria(branched.bacteria)\n        crossedBacteriaData = branched.bacteria.cross(assemblyBacteria) { extractKey(it) }.multiMap { \n          trimmed: it[0]\n          assembly: it[1]\n        }\n        module_typing_bacteria(crossedBacteriaData.trimmed, crossedBacteriaData.assembly)\n\n        /* sarscov2 stuff here */\n        module_covid_emergency(branched.sarscov2)\n\n        /* viruses stuff here */\n        // no extra modules executed\n        \n        /* wnv stuff here */\n        module_westnile(branched.wnv)\n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }\n     \n}\n\nworkflow ngsmanager_fasta {\n    take: \n      fasta\n    main:\n      try {      \n        module_qc_quast(fasta)\n\n        fasta.branch {\n            bacteria: isBacterium(it)\n            sarscov2: isSarsCov2(it) || isNegativeControlSarsCov2(it) || isPositiveControlSarsCov2(it)\n            negative_control: isNegativeControl(it)\n            viruses: isVirus(it)\n            wnv: isWNV(it)\n            ampliseq: isAmpliseq(it)\n            other: true\n        }\n        .set { branched }\n\n        /* bacteria stuff here */\n        module_typing_bacteria(Channel.empty(), branched.bacteria)\n\n        /* sarscov2 stuff here */\n        step_4TY_lineage__pangolin(branched.sarscov2)       \n\n        /* wnv stuff here */\n        module_westnile(branched.wnv)        \n      } catch (t) {\n          exit 1, \"unexpected exception: ${t.asString()}\"\n      }     \n}\n\nworkflow pipeline_ngsmanager {\n  if (isFastqRiscd(param('riscd'))){\n    ngsmanager_fastq(getSingleInput())\n  } else if (isFastaRiscd(param('riscd'))){\n    ngsmanager_fasta(getSingleInput())\n  } else {\n    exit 2, \"unexpected riscd provided: ${param('riscd')}\"\n  }\n}\n\nworkflow {\n  pipeline_ngsmanager()\n}"}
{"file_name": "module_typing_bacteria.nf", "file_path": "/modules/module_typing_bacteria.nf", "language": "nextflow", "id": "module_typing_bacteria", "content": "nextflow.enable.dsl=2\n\ninclude { step_2AS_mapping__bowtie } from '../steps/step_2AS_mapping__bowtie'\ninclude { step_3TX_species__kmerfinder;getBacterialReferencePath } from '../steps/step_3TX_species__kmerfinder'\ninclude { step_4AN_genes__prokka } from '../steps/step_4AN_genes__prokka'\ninclude { step_4AN_AMR__abricate } from '../steps/step_4AN_AMR__abricate'\ninclude { step_4AN_AMR__staramr } from '../steps/step_4AN_AMR__staramr'\ninclude { step_4TY_cgMLST__chewbbaca } from '../steps/step_4TY_cgMLST__chewbbaca'\ninclude { step_4TY_MLST__mlst } from '../steps/step_4TY_MLST__mlst'\ninclude { step_4TY_flaA__flaA } from '../steps/step_4TY_flaA__flaA'\ninclude { csv2map; extractKey; getEmpty } from '../functions/common.nf'\ninclude { getTrimmedReads;getAssembly } from '../functions/parameters.nf'\n\nworkflow module_typing_bacteria {\n    take: \n      trimmed\n      assembly\n    main:\n      assigned_species = step_3TX_species__kmerfinder(assembly).assigned_species\n      \n      if (!params.skip_bestref_mapping) {\n        trimmed.cross(assigned_species) { extractKey(it) }.multiMap { \n          trimmed: it[0]\n          species: it[1][1]\n          referencePath: it[1][2]\n        }.set { trimAndAndSpecies }\n        step_2AS_mapping__bowtie(trimAndAndSpecies.trimmed, trimAndAndSpecies.referencePath)\n      } \n\n      step_4AN_AMR__abricate(assembly)\n\n      step_4AN_genes__prokka(assembly.map{ [ it[0], it[1], 'Bacteria', '-', '-', getEmpty() ] })\n\n      assembly.cross(assigned_species) { extractKey(it) }.multiMap { \n        assembly: it[0]\n        species: it[1][1]\n      }.set { assemblyAndSpecies }\n\n      step_4AN_AMR__staramr(assemblyAndSpecies.assembly, assemblyAndSpecies.species)\n      step_4TY_MLST__mlst(assemblyAndSpecies.assembly)\n      step_4TY_flaA__flaA(assemblyAndSpecies.assembly, assemblyAndSpecies.species)\n      step_4TY_cgMLST__chewbbaca(assemblyAndSpecies.assembly, assemblyAndSpecies.species, '')\n    emit:\n        genus_species = assigned_species\n}\n\nworkflow {\n    module_typing_bacteria(getTrimmedReads(true), getAssembly())\n}"}
{"file_name": "bowtie2.nf", "file_path": "/etc/bowtie2.nf", "language": "nextflow", "id": "bowtie2", "content": "nextflow.enable.dsl=2\n\ninclude { logHeader } from '../functions/common.nf'\ninclude { param } from '../functions/parameters.nf'\n\nlog.info logHeader('NGSMANAGER')\n\nprocess bowtie2_index {\n    container \"ghcr.io/genpat-it/bowtie2:2.1.0--37ad014737\"\n    input:\n      val(ref)\n      path(fasta)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/result\", pattern: '*.bt*'    \n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/meta\", pattern: '*.log'\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/meta\", pattern: '.command.sh', saveAs: { \"${ref}.cfg\" }\n    script:\n      base = \"bowtie2_${ref}\"\n      \"\"\"\n      bowtie2-build ${fasta} ${ref} &> ${base}.log\n      \"\"\"\n}\n\nworkflow build_index {\n  bowtie2_index(param('reference'), param('reference_path'))\n}"}
{"file_name": "ncbi_download.nf", "file_path": "/etc/ncbi_download.nf", "language": "nextflow", "id": "ncbi_download", "content": "nextflow.enable.dsl=2\n\ninclude { logHeader;taskMemory;taskTime } from '../functions/common.nf'\ninclude { getNCBICodes } from '../functions/parameters.nf'\n\nlog.info logHeader('NGSMANAGER')\n\ndef REF_FORMATS = ['fasta', 'gb']\n\ndef ASSEMBLY_FASTA_SUFFIX='_genomic.fna.gz'\ndef ASSEMBLY_GB_SUFFIX='_genomic.gbff.gz'\ndef ASSEMBLY_MAX_DOWNLOAD_SIZE= params.ncbi_max_download_assembly\ndef SRA_MAX_DOWNLOAD_SIZE= params.ncbi_max_download_sra\n\nprocess ncbi_assembly_refseq {\n    container \"ncbi/edirect:12.5\"\n    tag \"${ref}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    time { taskTime( 10.m, task.attempt ) }        \n    maxForks 3\n    when:\n      ref ==~ /^GCF.+$/ \n    input:\n      val(ref)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/result\", pattern: '{*.fasta}'    \n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/meta\", pattern: '{*.json,*.xml,*.txt,*.log}'\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/fasta/meta\", pattern: '.command.sh', saveAs: { \"${ref}.cfg\" }\n    script:\n      assert ref ==~ /^GCF_[\\w\\.]+$/\n      \"\"\"\n        esearch -db assembly -query \"${ref} AND latest_refseq [PROP]\" -sort Accession | esummary > search_refseq_summary       \n        DOCID=`xtract -input search_refseq_summary -pattern DocumentSummary -position last -first Id | awk 'NF'`\n        VERSION=`xtract -input search_refseq_summary -pattern DocumentSummary -position last -first Synonym/RefSeq | awk 'NF'`\n        if  [[ ! -z \"\\${DOCID}\" ]] && [[ ! -z \"\\${VERSION}\" ]] ;\n        then\n          ANAME=`xtract -input search_refseq_summary -pattern DocumentSummary -position last -first AssemblyName | awk 'NF'`\n          echo -e \"\\${ANAME}\\n\\${VERSION}\" > \\${VERSION}.txt          \n          efetch -db assembly -id \\${DOCID} -format docsum > \\${VERSION}.xml\n          efetch -db assembly -id \\${DOCID} -format docsum -mode json > \\${VERSION}.json\n          FTP_PATH=`xtract -input \\${VERSION}.xml -pattern DocumentSummary -position last -first FtpPath_RefSeq | awk 'NF'`\n          FTP_BASENAME=`echo \\${FTP_PATH} | sed 's/.*\\\\///g'`\n          echo \"\\${FTP_PATH}/\\${FTP_BASENAME}${ASSEMBLY_FASTA_SUFFIX}\" > \\${VERSION}.log\n          curl --max-filesize ${ASSEMBLY_MAX_DOWNLOAD_SIZE} \\${FTP_PATH}/\\${FTP_BASENAME}${ASSEMBLY_FASTA_SUFFIX} | gunzip -c > \\${VERSION}.fasta\n        else \n          (>&2 echo \"${ref}: not found or not the latest version\"; exit 2) \n        fi      \n      \"\"\"\n}\n\nprocess ncbi_assembly_genbank {\n    container \"ncbi/edirect:12.5\"\n    tag \"${ref}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    time { taskTime( 10.m, task.attempt ) }         \n    maxForks 3\n    when:\n      ref ==~ /^GCA.+$/     \n    input:\n      val(ref)\n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/gb/result\", pattern: '{*.gb}'      \n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/gb/meta\", pattern: '{*.json,*.xml,*.txt,*.log}'\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/gb/meta\", pattern: '.command.sh', saveAs: { \"${ref}.cfg\" }\n    script:\n      assert ref ==~ /^GCA_[\\w\\.]+$/\n      \"\"\"\n        esearch -db assembly -query \"${ref} AND latest_genbank [PROP]\" -sort Accession | esummary > search_genbank_summary       \n        DOCID=`xtract -input search_genbank_summary -pattern DocumentSummary -position last -first Id | awk 'NF'`\n        VERSION=`xtract -input search_genbank_summary -pattern DocumentSummary -position last -first Synonym/Genbank | awk 'NF'`\n        if  [[ ! -z \"\\${DOCID}\" ]] && [[ ! -z \"\\${VERSION}\" ]] ;\n        then\n          ANAME=`xtract -input search_genbank_summary -pattern DocumentSummary -position last -first AssemblyName | awk 'NF'`\n          echo -e \"\\${ANAME}\\n\\${VERSION}\" > \\${VERSION}.txt            \n          efetch -db assembly -id \\${DOCID} -format docsum > \\${VERSION}.xml\n          efetch -db assembly -id \\${DOCID} -format docsum -mode json > \\${VERSION}.json\n          FTP_PATH=`xtract -input \\${VERSION}.xml -pattern DocumentSummary -position last -first FtpPath_GenBank | awk 'NF'`\n          FTP_BASENAME=`echo \\${FTP_PATH} | sed 's/.*\\\\///g'`\n          echo \"\\${FTP_PATH}/\\${FTP_BASENAME}${ASSEMBLY_GB_SUFFIX}\" > \\${VERSION}.log\n          curl --max-filesize ${ASSEMBLY_MAX_DOWNLOAD_SIZE} \\${FTP_PATH}/\\${FTP_BASENAME}${ASSEMBLY_GB_SUFFIX} | gunzip -c > \\${VERSION}.gb\n        else \n          (>&2 echo \"${ref}: not found or not the latest version\"; exit 2) \n        fi    \n      \"\"\"\n}\n\nprocess ncbi_sra {\n    container \"quay.io/biocontainers/sra-tools:3.0.10--h9f5acd7_0\"\n    tag \"${code}\"\n    memory { taskMemory( 2.GB, task.attempt ) }\n    time { taskTime( 20.m, task.attempt ) }         \n    maxForks 3\n    input:\n      val(code)\n    output:\n      path '*'\n      path '{*.sh,*.log}', hidden: true\n    publishDir  mode: 'rellink', \"${params.outdir}/${code}/fastq/result\", pattern: '*.gz'\n    publishDir  mode: 'rellink', \"${params.outdir}/${code}/fastq/meta\", pattern: '{*.json,*.txt}'\n    publishDir  mode: 'rellink', \"${params.outdir}/${code}/fastq/meta\", pattern: '.command.log', saveAs: { \"${code}.log\" }\n    publishDir  mode: 'rellink', \"${params.outdir}/${code}/fastq/meta\", pattern: '.command.sh', saveAs: { \"${code}.cfg\" }\n    script:\n      \"\"\"\n        prefetch \"${code}\" -H 1 -X ${SRA_MAX_DOWNLOAD_SIZE} |& tee .size_check.tmp\n        [ \"`grep -c 'is larger than maximum allowed' .size_check.tmp`\" == \"0\" ]\n        vdb-dump \"${code}\" --info -f json >  \"${code}.json\"\n        fasterq-dump -L info --skip-technical --split-files \"${code}\"      \n        for f in *.fastq; do gzip \\$f; done \n        echo \"${code}\\n${code}\" >  \"${code}.txt\"\n      \"\"\"\n}\n\n\nprocess ncbi_nuccore {\n    container \"ncbi/edirect:12.5\"\n    tag \"${ref}-${format}\"\n    memory { taskMemory( 250.MB, task.attempt ) }\n    time { taskTime( 5.m, task.attempt ) }         \n    maxForks 3\n    input:\n      val(ref)\n      each format \n    output:\n      path '*'\n      path '*.sh', hidden: true\n    publishDir  mode: 'rellink', \"${params.outdir}/${ref}/${format}/result\", pattern: '{*.fa*,*.gb}'\n    publishDir  mode: 'rellink', \"${params.outdir}/${ref}/${format}/meta\", pattern: '{*.json,*.txt}'\n    publishDir mode: 'rellink', \"${params.outdir}/${ref}/${format}/meta\", pattern: '.command.sh', saveAs: { \"${ref}.cfg\" }\n    script:\n      efetch_format = format == 'gb' ? 'gbwithparts' : format\n      \"\"\"\n        efetch -db nuccore -id ${ref} -format docsum -mode json > ${ref}.json\n        efetch -db nuccore -id ${ref} -format url > ${ref}.txt\n        efetch -db nuccore -id ${ref} -format acc >> ${ref}.txt\n        [[ `cat ${ref}.txt | wc -l` -eq '2' ]] || (>&2 echo \"Accession Number or URL not retrieved!\"; exit 1)    \n        efetch -db nuccore -id ${ref} -format ${efetch_format} > ${ref}.${format} && grep '\\\\S' ${ref}.${format}\n      \"\"\"\n}\n\nworkflow {\n  getNCBICodes().branch {\n      gcf: it ==~ /(?i)^GCF.+/\n      gca: it ==~ /(?i)^GCA.+/\n      sra: it ==~ /(?i)^SRR.+|(?i)^ERR.+|(?i)^DRR.+/\n      nuccore: true\n  }.set { branched }\n  ncbi_assembly_refseq(branched.gcf)\n  ncbi_assembly_genbank(branched.gca)\n  ncbi_nuccore(branched.nuccore, REF_FORMATS)\n  ncbi_sra(branched.sra)\n}"}
